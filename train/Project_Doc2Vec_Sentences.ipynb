{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project_Separate_Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "import math\n",
    "from IPython.display import clear_output, display\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import string\n",
    "import heapq\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "data = pd.read_pickle(\"./final_data.pkl\")\n",
    "articles = pd.read_pickle('./final_articles.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_claim</th>\n",
       "      <th>cleaned_claim</th>\n",
       "      <th>stemmed_claims</th>\n",
       "      <th>stemmed_stopword_claims</th>\n",
       "      <th>tokenized_claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>date</th>\n",
       "      <th>cont_days</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>label</th>\n",
       "      <th>article_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A line from George Orwell's novel 1984 predict...</td>\n",
       "      <td>a line from george orwells novel 1984 predicts...</td>\n",
       "      <td>a line from georg orwel novel 1984 predict the...</td>\n",
       "      <td>line georg orwel novel 1984 predict power smar...</td>\n",
       "      <td>[line, georg, orwel, novel, 1984, predict, pow...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>11520</td>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maine legislature candidate Leslie Gibson insu...</td>\n",
       "      <td>maine legislature candidate leslie gibson insu...</td>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>[main, legislatur, candid, lesli, gibson, insu...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>11763</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A 17-year-old girl named Alyssa Carson is bein...</td>\n",
       "      <td>a 17yearold girl named alyssa carson is being ...</td>\n",
       "      <td>a 17yearold girl name alyssa carson is be trai...</td>\n",
       "      <td>17yearold girl name alyssa carson train nasa b...</td>\n",
       "      <td>[17yearold, girl, name, alyssa, carson, train,...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2018-07-18</td>\n",
       "      <td>11886</td>\n",
       "      <td>2018</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In 1988 author Roald Dahl penned an open lette...</td>\n",
       "      <td>in 1988 author roald dahl penned an open lette...</td>\n",
       "      <td>in 1988 author roald dahl pen an open letter u...</td>\n",
       "      <td>1988 author roald dahl pen open letter urg par...</td>\n",
       "      <td>[1988, author, roald, dahl, pen, open, letter,...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>12087</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When it comes to fighting terrorism, \"Another ...</td>\n",
       "      <td>when it comes to fighting terrorism another th...</td>\n",
       "      <td>when it come to fight terror anoth thing we kn...</td>\n",
       "      <td>come fight terror anoth thing know doe work ba...</td>\n",
       "      <td>[come, fight, terror, anoth, thing, know, doe,...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>2016-03-22</td>\n",
       "      <td>11038</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           raw_claim  \\\n",
       "0  A line from George Orwell's novel 1984 predict...   \n",
       "1  Maine legislature candidate Leslie Gibson insu...   \n",
       "2  A 17-year-old girl named Alyssa Carson is bein...   \n",
       "3  In 1988 author Roald Dahl penned an open lette...   \n",
       "4  When it comes to fighting terrorism, \"Another ...   \n",
       "\n",
       "                                       cleaned_claim  \\\n",
       "0  a line from george orwells novel 1984 predicts...   \n",
       "1  maine legislature candidate leslie gibson insu...   \n",
       "2  a 17yearold girl named alyssa carson is being ...   \n",
       "3  in 1988 author roald dahl penned an open lette...   \n",
       "4  when it comes to fighting terrorism another th...   \n",
       "\n",
       "                                      stemmed_claims  \\\n",
       "0  a line from georg orwel novel 1984 predict the...   \n",
       "1  main legislatur candid lesli gibson insult par...   \n",
       "2  a 17yearold girl name alyssa carson is be trai...   \n",
       "3  in 1988 author roald dahl pen an open letter u...   \n",
       "4  when it come to fight terror anoth thing we kn...   \n",
       "\n",
       "                             stemmed_stopword_claims  \\\n",
       "0  line georg orwel novel 1984 predict power smar...   \n",
       "1  main legislatur candid lesli gibson insult par...   \n",
       "2  17yearold girl name alyssa carson train nasa b...   \n",
       "3  1988 author roald dahl pen open letter urg par...   \n",
       "4  come fight terror anoth thing know doe work ba...   \n",
       "\n",
       "                                     tokenized_claim         claimant  \\\n",
       "0  [line, georg, orwel, novel, 1984, predict, pow...          Unknown   \n",
       "1  [main, legislatur, candid, lesli, gibson, insu...          Unknown   \n",
       "2  [17yearold, girl, name, alyssa, carson, train,...          Unknown   \n",
       "3  [1988, author, roald, dahl, pen, open, letter,...          Unknown   \n",
       "4  [come, fight, terror, anoth, thing, know, doe,...  Hillary Clinton   \n",
       "\n",
       "        date  cont_days  year  month  label  \\\n",
       "0 2017-07-17      11520  2017      7      0   \n",
       "1 2018-03-17      11763  2018      3      2   \n",
       "2 2018-07-18      11886  2018      7      1   \n",
       "3 2019-02-04      12087  2019      2      2   \n",
       "4 2016-03-22      11038  2016      3      2   \n",
       "\n",
       "                                article_array  \n",
       "0            [122094, 122580, 130685, 134765]  \n",
       "1                    [106868, 127320, 128060]  \n",
       "2                    [132130, 132132, 149722]  \n",
       "3                    [123254, 123418, 127464]  \n",
       "4  [41099, 89899, 72543, 82644, 95344, 88361]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_articles</th>\n",
       "      <th>cleaned_articles</th>\n",
       "      <th>stemmed_articles</th>\n",
       "      <th>stemmed_stopwords_articles</th>\n",
       "      <th>tokenized_articles</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "      <th>tokenized_cleaned_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125385</th>\n",
       "      <td>Dr. Ben Carson: Welfare Benefactor?\\nAn initia...</td>\n",
       "      <td>dr ben carson welfare benefactor an initially ...</td>\n",
       "      <td>dr ben carson welfar benefactor an initi unlik...</td>\n",
       "      <td>dr ben carson welfar benefactor initi unlik cl...</td>\n",
       "      <td>[dr, ben, carson, welfar, benefactor, initi, u...</td>\n",
       "      <td>[dr ben carson welfar benefactor, initi unlik ...</td>\n",
       "      <td>[[dr, ben, carson, welfar, benefactor], [initi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32238</th>\n",
       "      <td>The World Factbook — Central Intelligence Agen...</td>\n",
       "      <td>the world factbook central intelligence agency...</td>\n",
       "      <td>the world factbook central intellig agenc the ...</td>\n",
       "      <td>world factbook central intellig agenc unit sta...</td>\n",
       "      <td>[world, factbook, central, intellig, agenc, un...</td>\n",
       "      <td>[world factbook central intellig agenc unit st...</td>\n",
       "      <td>[[world, factbook, central, intellig, agenc, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16051</th>\n",
       "      <td>1014 texaseducationagencypftexas\\n\\nEmails, La...</td>\n",
       "      <td>1014 texaseducationagencypftexas emails lauren...</td>\n",
       "      <td>1014 texaseducationagencypftexa email lauren c...</td>\n",
       "      <td>1014 texaseducationagencypftexa email lauren c...</td>\n",
       "      <td>[1014, texaseducationagencypftexa, email, laur...</td>\n",
       "      <td>[1014 texaseducationagencypftexa email lauren ...</td>\n",
       "      <td>[[1014, texaseducationagencypftexa, email, lau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118633</th>\n",
       "      <td>Clinton camp delays Weather Channel ad buy aft...</td>\n",
       "      <td>clinton camp delays weather channel ad buy aft...</td>\n",
       "      <td>clinton camp delay weather channel ad buy afte...</td>\n",
       "      <td>clinton camp delay weather channel ad buy back...</td>\n",
       "      <td>[clinton, camp, delay, weather, channel, ad, b...</td>\n",
       "      <td>[clinton camp delay weather channel ad buy bac...</td>\n",
       "      <td>[[clinton, camp, delay, weather, channel, ad, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117945</th>\n",
       "      <td>Living with kangaroos\\nKangaroos are appealing...</td>\n",
       "      <td>living with kangaroos kangaroos are appealing ...</td>\n",
       "      <td>live with kangaroo kangaroo are appeal wild an...</td>\n",
       "      <td>live kangaroo kangaroo appeal wild power nativ...</td>\n",
       "      <td>[live, kangaroo, kangaroo, appeal, wild, power...</td>\n",
       "      <td>[live kangaroo kangaroo appeal wild power nati...</td>\n",
       "      <td>[[live, kangaroo, kangaroo, appeal, wild, powe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             raw_articles  \\\n",
       "125385  Dr. Ben Carson: Welfare Benefactor?\\nAn initia...   \n",
       "32238   The World Factbook — Central Intelligence Agen...   \n",
       "16051   1014 texaseducationagencypftexas\\n\\nEmails, La...   \n",
       "118633  Clinton camp delays Weather Channel ad buy aft...   \n",
       "117945  Living with kangaroos\\nKangaroos are appealing...   \n",
       "\n",
       "                                         cleaned_articles  \\\n",
       "125385  dr ben carson welfare benefactor an initially ...   \n",
       "32238   the world factbook central intelligence agency...   \n",
       "16051   1014 texaseducationagencypftexas emails lauren...   \n",
       "118633  clinton camp delays weather channel ad buy aft...   \n",
       "117945  living with kangaroos kangaroos are appealing ...   \n",
       "\n",
       "                                         stemmed_articles  \\\n",
       "125385  dr ben carson welfar benefactor an initi unlik...   \n",
       "32238   the world factbook central intellig agenc the ...   \n",
       "16051   1014 texaseducationagencypftexa email lauren c...   \n",
       "118633  clinton camp delay weather channel ad buy afte...   \n",
       "117945  live with kangaroo kangaroo are appeal wild an...   \n",
       "\n",
       "                               stemmed_stopwords_articles  \\\n",
       "125385  dr ben carson welfar benefactor initi unlik cl...   \n",
       "32238   world factbook central intellig agenc unit sta...   \n",
       "16051   1014 texaseducationagencypftexa email lauren c...   \n",
       "118633  clinton camp delay weather channel ad buy back...   \n",
       "117945  live kangaroo kangaroo appeal wild power nativ...   \n",
       "\n",
       "                                       tokenized_articles  \\\n",
       "125385  [dr, ben, carson, welfar, benefactor, initi, u...   \n",
       "32238   [world, factbook, central, intellig, agenc, un...   \n",
       "16051   [1014, texaseducationagencypftexa, email, laur...   \n",
       "118633  [clinton, camp, delay, weather, channel, ad, b...   \n",
       "117945  [live, kangaroo, kangaroo, appeal, wild, power...   \n",
       "\n",
       "                                         cleaned_sentence  \\\n",
       "125385  [dr ben carson welfar benefactor, initi unlik ...   \n",
       "32238   [world factbook central intellig agenc unit st...   \n",
       "16051   [1014 texaseducationagencypftexa email lauren ...   \n",
       "118633  [clinton camp delay weather channel ad buy bac...   \n",
       "117945  [live kangaroo kangaroo appeal wild power nati...   \n",
       "\n",
       "                               tokenized_cleaned_sentence  \n",
       "125385  [[dr, ben, carson, welfar, benefactor], [initi...  \n",
       "32238   [[world, factbook, central, intellig, agenc, u...  \n",
       "16051   [[1014, texaseducationagencypftexa, email, lau...  \n",
       "118633  [[clinton, camp, delay, weather, channel, ad, ...  \n",
       "117945  [[live, kangaroo, kangaroo, appeal, wild, powe...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from scipy import spatial\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# 2 minutes to run\n",
    "# # create a label for every sentence based on article ID (eg. 12345-1, 12345-2, 12345-3 ... etc.)\n",
    "\n",
    "# full_sentences_ID = []\n",
    "# for i in range(articles.shape[0]):\n",
    "#     sentence_ID_list = []\n",
    "#     sentence_number = 0\n",
    "#     sentences = articles.tokenized_cleaned_sentence.loc[articles.index[i]]\n",
    "#     for u in range(len(sentences)):\n",
    "#         sentence_ID = str(articles.index[i][0]) +  '-' + str(sentence_number)\n",
    "#         sentence_number += 1\n",
    "#         sentence_ID_list.append(sentence_ID)\n",
    "#     full_sentences_ID.append(sentence_ID_list)\n",
    "    \n",
    "#     # print progress\n",
    "#     progress = round((i/articles.shape[0])*100,2)\n",
    "#     clear_output(wait=True)\n",
    "#     print(\"progress: \" + str(progress) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine the tokenized sentences with their ID\n",
    "# articles_tok_sent = articles.tokenized_cleaned_sentence.to_frame()\n",
    "# articles_tok_sent['sentence_ID'] = full_sentences_ID\n",
    "# articles_tok_sent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # combine the sentence list and index list ~ takes 1.5 hours to run\n",
    "\n",
    "# article_sent_list = []\n",
    "# article_sent_ID = []\n",
    "\n",
    "# for i in range(articles_tok_sent.shape[0]):\n",
    "#     one_article_sent = articles_tok_sent.tokenized_cleaned_sentence.loc[articles.index[i]]\n",
    "#     article_sent_list = article_sent_list + one_article_sent\n",
    "#     one_article_ID = articles_tok_sent.sentence_ID.loc[articles.index[i]]\n",
    "#     article_sent_ID = article_sent_ID + one_article_ID\n",
    "    \n",
    "#     # print progress\n",
    "#     progress = round((i/articles_tok_sent.shape[0])*100,2)\n",
    "#     clear_output(wait=True)\n",
    "#     print(\"progress: \" + str(progress) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a data frame of the independent sentences and ID\n",
    "# # create copies\n",
    "# l1 = article_sent_list\n",
    "# l2 = article_sent_ID\n",
    "# # make dataframe\n",
    "# article_information = pd.Series(l2).to_frame()\n",
    "# article_information.columns = ['sentence_ID']\n",
    "# article_information['sentences'] = l1\n",
    "# # display it\n",
    "# article_information.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save it to pickle\n",
    "# article_information.to_pickle(\"./article_sentences_ind.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Reading Pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_pkl = pd.read_pickle(\"./article_sentences_ind.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_ID</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125385-0</td>\n",
       "      <td>[dr, ben, carson, welfar, benefactor]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125385-1</td>\n",
       "      <td>[initi, unlik, clearli, formid, contend, 2016,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>125385-2</td>\n",
       "      <td>[unlik, chri, christi, rand, paul, mike, hucka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>125385-3</td>\n",
       "      <td>[carson, becam, somewhat, overnight, sensat, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125385-4</td>\n",
       "      <td>[earliest, version, meme, date, least, decemb,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_ID                                          sentences\n",
       "0    125385-0              [dr, ben, carson, welfar, benefactor]\n",
       "1    125385-1  [initi, unlik, clearli, formid, contend, 2016,...\n",
       "2    125385-2  [unlik, chri, christi, rand, paul, mike, hucka...\n",
       "3    125385-3  [carson, becam, somewhat, overnight, sensat, f...\n",
       "4    125385-4  [earliest, version, meme, date, least, decemb,..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_pkl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the lists from the dataframes\n",
    "sentence_ID_list = sentences_pkl.sentence_ID.tolist()\n",
    "sentence_list = sentences_pkl.sentences.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14694"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average sentence size\n",
    "full_sent_size = []\n",
    "for i in range(sentences_pkl.shape[0]):\n",
    "    sentence_size = len(sentence_list[i])\n",
    "    full_sent_size.append(sentence_size)\n",
    "# sum(full_sent_size)/len(full_sent_size)\n",
    "max(full_sent_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weird Article "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_sent_size.index(max(full_sent_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_pkl.loc[329509].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 341 ms, sys: 81 µs, total: 341 ms\n",
      "Wall time: 340 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "    \n",
    "# create a list of claim index ranging from -15554 to 1 (-15554, -15553, -15552 ... 0)\n",
    "claim_range = range(0, 15555)\n",
    "claim_index = []\n",
    "for i in claim_range:\n",
    "    claim_index.append(i)\n",
    "\n",
    "# create a list of lists of data.tokenized_claim\n",
    "tokenized_claims = []\n",
    "for i in range(data.shape[0]):\n",
    "    tokenized_claims.append(data.tokenized_claim[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert claim index to string to match the sentences ID\n",
    "claim_str_index = list(map(str, claim_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form full list for training\n",
    "full_text_list = sentence_list + tokenized_claims\n",
    "full_ID_list = sentence_ID_list + claim_str_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine above 2 lists in to a dictionary for debugging\n",
    "complete_data = dict(zip(full_ID_list, full_text_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# #create tagged data to train doc2vec w claim\n",
    "# tagged_data = [TaggedDocument(words=full_text_list[i], tags=[full_ID_list[i]]) for i in range(len(full_text_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# 4 hours to run\n",
    "\n",
    "# import multiprocessing\n",
    "# n_cpu = multiprocessing.cpu_count()\n",
    "\n",
    "# # setup training\n",
    "# vec_size = 20\n",
    "# model = Doc2Vec(dm = 1, vector_size = vec_size, min_count = 2, workers = n_cpu, epochs=20)\n",
    "# # build vocab\n",
    "# model.build_vocab(tagged_data)\n",
    "# # train\n",
    "# model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "# # save model\n",
    "# model.save(\"20epoch_20vec_sentences.model\")\n",
    "# print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Find Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.4 s, sys: 460 ms, total: 12.9 s\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load the model\n",
    "model = Doc2Vec.load(\"20epoch_20vec_sentences.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_ID</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125385-0</td>\n",
       "      <td>[dr, ben, carson, welfar, benefactor]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125385-1</td>\n",
       "      <td>[initi, unlik, clearli, formid, contend, 2016,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>125385-2</td>\n",
       "      <td>[unlik, chri, christi, rand, paul, mike, hucka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>125385-3</td>\n",
       "      <td>[carson, becam, somewhat, overnight, sensat, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125385-4</td>\n",
       "      <td>[earliest, version, meme, date, least, decemb,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_ID                                          sentences\n",
       "0    125385-0              [dr, ben, carson, welfar, benefactor]\n",
       "1    125385-1  [initi, unlik, clearli, formid, contend, 2016,...\n",
       "2    125385-2  [unlik, chri, christi, rand, paul, mike, hucka...\n",
       "3    125385-3  [carson, becam, somewhat, overnight, sensat, f...\n",
       "4    125385-4  [earliest, version, meme, date, least, decemb,..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_pkl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 99.99%\n",
      "CPU times: user 6min 1s, sys: 7.85 s, total: 6min 9s\n",
      "Wall time: 6min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# ~ 6 minutes to run\n",
    "# find the top 5 sentence ID from each related article with the highest similarity score with the claim\n",
    "\n",
    "final_best_sentences_ID = []\n",
    "for i in range(data.shape[0]): # iterating over every claim\n",
    "    one_claim_array = data.article_array.loc[i]\n",
    "    best_sentences_one_claim = []\n",
    "    for u in range(len(one_claim_array)): # iterating every article of one claim\n",
    "        one_article_ID = one_claim_array[u]\n",
    "        sent_list = articles.tokenized_cleaned_sentence.loc[str(one_article_ID)].iloc[0]\n",
    "        one_article_sim_list=[]\n",
    "        for y in range(len(sent_list)): #iterating over every sentence of one article\n",
    "            v1 = model.docvecs[str(i)]\n",
    "            sentence_number = str(str(one_article_ID) + '-' + str(y))\n",
    "            v2 = model.docvecs[sentence_number]\n",
    "            similarity = 1 - spatial.distance.cosine(v1, v2)\n",
    "            one_article_sim_list.append(similarity)\n",
    "#             print(\"ID: \" + str(i) + \" \" + str(result))\n",
    "        a = numpy.array(one_article_sim_list)\n",
    "        best_sentences_one_article = heapq.nlargest(5, range(len(a)), a.take)   \n",
    "        best_sentences_one_claim.append(best_sentences_one_article)\n",
    "    final_best_sentences_ID.append(best_sentences_one_claim)\n",
    "    # print progress\n",
    "    progress = round((i/data.shape[0])*100,2)\n",
    "    clear_output(wait=True)\n",
    "    print(\"progress: \" + str(progress) + \"%\")\n",
    "\n",
    "# add it to the data frame\n",
    "data['best_sentences_ID'] = final_best_sentences_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 99.99%\n",
      "CPU times: user 1min 9s, sys: 2.97 s, total: 1min 12s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# ~ 2 minuts to run\n",
    "# calculate the top 5 similarity scores\n",
    "\n",
    "end = data.shape[0]\n",
    "# end = 1\n",
    "avg_sim_for_one_claim_list = []\n",
    "avg_sim_one_claim = []\n",
    "for i in range(end): # per row\n",
    "    one_claim_articles_ID = data.article_array.loc[i]\n",
    "    one_claim_sentences_ID = data.best_sentences_ID.loc[i]\n",
    "    avg_sim_for_one_article = []\n",
    "#     print(\"claim: \" + str(i))\n",
    "    for u in range(len(one_claim_articles_ID)): # article_array index\n",
    "        sim_for_one_article = []\n",
    "        for y in range(len(one_claim_sentences_ID[u])): # sentence ID index      \n",
    "            sentence_ID = str(one_claim_articles_ID[u]) + '-' + str(data.best_sentences_ID.loc[i][u][y])\n",
    "            # calculate sim score\n",
    "            v1 = model.docvecs[str(i)]\n",
    "            v2 = model.docvecs[sentence_ID]\n",
    "            similarity = 1 - spatial.distance.cosine(v1, v2)\n",
    "            # similarity between claim and each sentence is calculated\n",
    "            sim_for_one_article.append(similarity)\n",
    "#             print(str(sentence_ID) + \": \" + str(similarity))\n",
    "        # the similarity between claim and each sentence per article is averaged\n",
    "        avg_sim = sum(sim_for_one_article)/len(sim_for_one_article)\n",
    "        avg_sim_for_one_article.append(avg_sim)\n",
    "#         print(\"\")\n",
    "    \n",
    "    avg_sim_ = sum(avg_sim_for_one_article)/len(avg_sim_for_one_article)\n",
    "    avg_sim_one_claim.append(avg_sim_)\n",
    "    \n",
    "    # a list of lists, big list per claim, and small list per article\n",
    "    avg_sim_for_one_claim_list.append(avg_sim_for_one_article)\n",
    "    # print progress\n",
    "    progress = round((i/end)*100,2)\n",
    "    clear_output(wait=True)\n",
    "    print(\"progress: \" + str(progress) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to data frame\n",
    "data2 = data\n",
    "data2['avg_sentence_sim'] = avg_sim_for_one_claim_list\n",
    "data2['avg_sim_score'] = avg_sim_one_claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_claim</th>\n",
       "      <th>cleaned_claim</th>\n",
       "      <th>stemmed_claims</th>\n",
       "      <th>stemmed_stopword_claims</th>\n",
       "      <th>tokenized_claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>date</th>\n",
       "      <th>cont_days</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>label</th>\n",
       "      <th>article_array</th>\n",
       "      <th>best_sentences_ID</th>\n",
       "      <th>avg_sentence_sim</th>\n",
       "      <th>avg_sim_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A line from George Orwell's novel 1984 predict...</td>\n",
       "      <td>a line from george orwells novel 1984 predicts...</td>\n",
       "      <td>a line from georg orwel novel 1984 predict the...</td>\n",
       "      <td>line georg orwel novel 1984 predict power smar...</td>\n",
       "      <td>[line, georg, orwel, novel, 1984, predict, pow...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>11520</td>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "      <td>[[20, 16, 19, 6, 11], [5, 0, 25, 8, 10], [0, 2...</td>\n",
       "      <td>[0.49315634965896604, 0.40044230222702026, 0.1...</td>\n",
       "      <td>0.332769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maine legislature candidate Leslie Gibson insu...</td>\n",
       "      <td>maine legislature candidate leslie gibson insu...</td>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>[main, legislatur, candid, lesli, gibson, insu...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>11763</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "      <td>[[12, 13, 0, 1, 4], [4, 22, 48, 1, 25], [4, 9,...</td>\n",
       "      <td>[0.7131051063537598, 0.759801709651947, 0.7064...</td>\n",
       "      <td>0.726454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A 17-year-old girl named Alyssa Carson is bein...</td>\n",
       "      <td>a 17yearold girl named alyssa carson is being ...</td>\n",
       "      <td>a 17yearold girl name alyssa carson is be trai...</td>\n",
       "      <td>17yearold girl name alyssa carson train nasa b...</td>\n",
       "      <td>[17yearold, girl, name, alyssa, carson, train,...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2018-07-18</td>\n",
       "      <td>11886</td>\n",
       "      <td>2018</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "      <td>[[5, 26, 28, 31, 9], [86, 33, 77, 34, 37], [1,...</td>\n",
       "      <td>[0.8618260860443115, 0.904123330116272, 0.4341...</td>\n",
       "      <td>0.733378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In 1988 author Roald Dahl penned an open lette...</td>\n",
       "      <td>in 1988 author roald dahl penned an open lette...</td>\n",
       "      <td>in 1988 author roald dahl pen an open letter u...</td>\n",
       "      <td>1988 author roald dahl pen open letter urg par...</td>\n",
       "      <td>[1988, author, roald, dahl, pen, open, letter,...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>12087</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "      <td>[[59, 26, 20, 88, 113], [87, 163, 195, 108, 22...</td>\n",
       "      <td>[0.8745679140090943, 0.8695014953613281, 0.871...</td>\n",
       "      <td>0.871977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When it comes to fighting terrorism, \"Another ...</td>\n",
       "      <td>when it comes to fighting terrorism another th...</td>\n",
       "      <td>when it come to fight terror anoth thing we kn...</td>\n",
       "      <td>come fight terror anoth thing know doe work ba...</td>\n",
       "      <td>[come, fight, terror, anoth, thing, know, doe,...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>2016-03-22</td>\n",
       "      <td>11038</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "      <td>[[72, 120, 65, 168, 48], [6, 0, 1, 19, 2], [3,...</td>\n",
       "      <td>[0.6641334772109986, 0.636748218536377, 0.4504...</td>\n",
       "      <td>0.652727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rhode Island is \"almost dead last\" among North...</td>\n",
       "      <td>rhode island is almost dead last among northea...</td>\n",
       "      <td>rhode island is almost dead last among northea...</td>\n",
       "      <td>rhode island almost dead last among northeaste...</td>\n",
       "      <td>[rhode, island, almost, dead, last, among, nor...</td>\n",
       "      <td>Other</td>\n",
       "      <td>2014-02-11</td>\n",
       "      <td>10268</td>\n",
       "      <td>2014</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[8284, 3768, 20091, 82368, 73148, 4493]</td>\n",
       "      <td>[[6, 0, 10, 7, 2], [8, 2, 11, 13, 22], [6, 4, ...</td>\n",
       "      <td>[-0.00010861875489354134, 0.19662532806396485,...</td>\n",
       "      <td>0.315237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The poorest counties in the U.S. are in Appala...</td>\n",
       "      <td>the poorest counties in the us are in appalach...</td>\n",
       "      <td>the poorest counti in the us are in appalachia...</td>\n",
       "      <td>poorest counti us appalachia happen 90 percent...</td>\n",
       "      <td>[poorest, counti, us, appalachia, happen, 90, ...</td>\n",
       "      <td>Other</td>\n",
       "      <td>2014-11-19</td>\n",
       "      <td>10549</td>\n",
       "      <td>2014</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>[70709, 70708]</td>\n",
       "      <td>[[1, 0], [2, 6, 5, 4, 1]]</td>\n",
       "      <td>[0.34360191226005554, 0.18681026697158815]</td>\n",
       "      <td>0.265206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Koch Industries paid the legal fees of George ...</td>\n",
       "      <td>koch industries paid the legal fees of george ...</td>\n",
       "      <td>koch industri paid the legal fee of georg zimm...</td>\n",
       "      <td>koch industri paid legal fee georg zimmerman</td>\n",
       "      <td>[koch, industri, paid, legal, fee, georg, zimm...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2013-07-18</td>\n",
       "      <td>10060</td>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>[120591, 120592, 127866, 129483]</td>\n",
       "      <td>[[1, 4, 3, 2, 0], [6, 4, 1, 2, 5], [33, 7, 4, ...</td>\n",
       "      <td>[0.5746460080146789, 0.47094449400901794, 0.50...</td>\n",
       "      <td>0.560426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"Minnesota, Michigan, Iowa already have 70 mph...</td>\n",
       "      <td>minnesota michigan iowa already have 70 mph sp...</td>\n",
       "      <td>minnesota michigan iowa alreadi have 70 mph sp...</td>\n",
       "      <td>minnesota michigan iowa alreadi 70 mph speed l...</td>\n",
       "      <td>[minnesota, michigan, iowa, alreadi, 70, mph, ...</td>\n",
       "      <td>Other</td>\n",
       "      <td>2013-08-22</td>\n",
       "      <td>10095</td>\n",
       "      <td>2013</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>[69547, 80095, 7994, 81116, 77621]</td>\n",
       "      <td>[[70, 48, 42, 19, 44], [0, 2, 7, 8, 1], [27, 1...</td>\n",
       "      <td>[0.5380596280097961, 0.590717875957489, 0.6359...</td>\n",
       "      <td>0.516865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"FBI Uniform Crime Report for 2016 shows more ...</td>\n",
       "      <td>fbi uniform crime report for 2016 shows more t...</td>\n",
       "      <td>fbi uniform crime report for 2016 show more th...</td>\n",
       "      <td>fbi uniform crime report 2016 show four time m...</td>\n",
       "      <td>[fbi, uniform, crime, report, 2016, show, four...</td>\n",
       "      <td>Other</td>\n",
       "      <td>2017-10-17</td>\n",
       "      <td>11612</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>[72012, 26005, 43481, 55671]</td>\n",
       "      <td>[[0, 5, 1, 4, 7], [3, 6, 4, 5, 1], [11, 0, 4, ...</td>\n",
       "      <td>[0.7591381788253784, 0.7119978070259094, 0.521...</td>\n",
       "      <td>0.669273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"Pelosi Sinks to New Low, Tells Dems: If You ...</td>\n",
       "      <td>pelosi sinks to new low tells dems if you hav...</td>\n",
       "      <td>pelosi sink to new low tell dem if you have to...</td>\n",
       "      <td>pelosi sink new low tell dem lie voter win</td>\n",
       "      <td>[pelosi, sink, new, low, tell, dem, lie, voter...</td>\n",
       "      <td>Other</td>\n",
       "      <td>2018-08-21</td>\n",
       "      <td>11920</td>\n",
       "      <td>2018</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>[27062, 27061, 20679, 61872, 20677]</td>\n",
       "      <td>[[0, 4, 8, 6, 15], [7, 8, 11, 0, 21], [14, 21,...</td>\n",
       "      <td>[0.7097761988639831, 0.6724220991134644, 0.733...</td>\n",
       "      <td>0.692177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Socialist teachers at South Charlotte Middle S...</td>\n",
       "      <td>socialist teachers at south charlotte middle s...</td>\n",
       "      <td>socialist teacher at south charlott middl scho...</td>\n",
       "      <td>socialist teacher south charlott middl school ...</td>\n",
       "      <td>[socialist, teacher, south, charlott, middl, s...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2018-10-17</td>\n",
       "      <td>11977</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>[104287, 144516]</td>\n",
       "      <td>[[2, 0, 9, 10, 11], [4, 15, 19, 14, 23]]</td>\n",
       "      <td>[0.026715978980064392, 0.29803799986839297]</td>\n",
       "      <td>0.162377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Says that in the U.S. Capitol, \"Stephen F. Aus...</td>\n",
       "      <td>says that in the us capitol stephen f austins ...</td>\n",
       "      <td>say that in the us capitol stephen f austin an...</td>\n",
       "      <td>say us capitol stephen f austin sam houston st...</td>\n",
       "      <td>[say, us, capitol, stephen, f, austin, sam, ho...</td>\n",
       "      <td>Other</td>\n",
       "      <td>2018-03-28</td>\n",
       "      <td>11774</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[16639, 16657, 16667]</td>\n",
       "      <td>[[1, 5, 3, 2, 4], [2, 11, 9, 0, 7], [5, 2, 0, ...</td>\n",
       "      <td>[0.6564371228218079, 0.7978529214859009, 0.748...</td>\n",
       "      <td>0.734398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NASA Has Just Confirmed Earth Has A New Moon</td>\n",
       "      <td>nasa has just confirmed earth has a new moon</td>\n",
       "      <td>nasa ha just confirm earth ha a new moon</td>\n",
       "      <td>nasa ha confirm earth ha new moon</td>\n",
       "      <td>[nasa, ha, confirm, earth, ha, new, moon]</td>\n",
       "      <td>Bloggers</td>\n",
       "      <td>2018-03-29</td>\n",
       "      <td>11775</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>[91455, 72179, 18903, 42080]</td>\n",
       "      <td>[[9, 19, 3, 28, 5], [10, 11, 2, 5, 3], [10, 1,...</td>\n",
       "      <td>[0.6802428364753723, 0.7061182737350464, 0.742...</td>\n",
       "      <td>0.723062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\"We are always going to need architects, docto...</td>\n",
       "      <td>we are always going to need architects doctors...</td>\n",
       "      <td>we are alway go to need architect doctor were ...</td>\n",
       "      <td>alway go need architect doctor go need profess...</td>\n",
       "      <td>[alway, go, need, architect, doctor, go, need,...</td>\n",
       "      <td>Other</td>\n",
       "      <td>2019-01-24</td>\n",
       "      <td>12076</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[42685, 32007, 33562]</td>\n",
       "      <td>[[1, 3, 10, 4, 6], [0], [5, 2, 0, 4, 1]]</td>\n",
       "      <td>[0.21930979192256927, -0.22811169922351837, -0...</td>\n",
       "      <td>-0.028883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"Justin Amash is rated Michigan’s No. 1 conser...</td>\n",
       "      <td>justin amash is rated michigans no 1 conservat...</td>\n",
       "      <td>justin amash is rate michigan no 1 conserv by ...</td>\n",
       "      <td>justin amash rate michigan 1 conserv nation ri...</td>\n",
       "      <td>[justin, amash, rate, michigan, 1, conserv, na...</td>\n",
       "      <td>Other</td>\n",
       "      <td>2014-07-01</td>\n",
       "      <td>10408</td>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>[22383, 72467, 72466, 86512, 73422, 83732, 83730]</td>\n",
       "      <td>[[0], [2, 10, 4, 7, 11], [108, 104, 14, 61, 10...</td>\n",
       "      <td>[0.5148963332176208, 0.6793382167816162, 0.691...</td>\n",
       "      <td>0.641952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BREAKING: NFL Owner Listens to Trump, Fires P...</td>\n",
       "      <td>breaking nfl owner listens to trump fires pla...</td>\n",
       "      <td>break nfl owner listen to trump fire player fo...</td>\n",
       "      <td>break nfl owner listen trump fire player disgr...</td>\n",
       "      <td>[break, nfl, owner, listen, trump, fire, playe...</td>\n",
       "      <td>Other</td>\n",
       "      <td>2017-09-29</td>\n",
       "      <td>11594</td>\n",
       "      <td>2017</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>[20907, 73380, 22540, 2010]</td>\n",
       "      <td>[[0, 2, 1], [4, 2, 8, 30, 50], [67, 51, 34, 16...</td>\n",
       "      <td>[0.5069699088732401, 0.7903198957443237, 0.661...</td>\n",
       "      <td>0.650730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Says one year ago, \"no cities in the South had...</td>\n",
       "      <td>says one year ago no cities in the south had g...</td>\n",
       "      <td>say one year ago no citi in the south had guar...</td>\n",
       "      <td>say one year ago citi south guarante paid sick...</td>\n",
       "      <td>[say, one, year, ago, citi, south, guarante, p...</td>\n",
       "      <td>Other</td>\n",
       "      <td>2019-04-24</td>\n",
       "      <td>12166</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>[87410, 18608, 57313, 35767, 85310, 43631]</td>\n",
       "      <td>[[18, 13, 16, 2, 1], [19, 3, 11, 0, 9], [0, 8,...</td>\n",
       "      <td>[0.5099975764751434, 0.6683658123016357, 0.675...</td>\n",
       "      <td>0.417633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Says North Carolina Republican Senate candidat...</td>\n",
       "      <td>says north carolina republican senate candidat...</td>\n",
       "      <td>say north carolina republican senat candid tho...</td>\n",
       "      <td>say north carolina republican senat candid tho...</td>\n",
       "      <td>[say, north, carolina, republican, senat, cand...</td>\n",
       "      <td>Other</td>\n",
       "      <td>2014-04-17</td>\n",
       "      <td>10333</td>\n",
       "      <td>2014</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[81476, 67734, 73202, 96584, 73198]</td>\n",
       "      <td>[[41, 22, 85, 78, 80], [29, 12, 26, 23, 28], [...</td>\n",
       "      <td>[0.8923457384109497, 0.8810298800468445, 0.889...</td>\n",
       "      <td>0.869995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Says \"the mandate is 71 times that a child’s b...</td>\n",
       "      <td>says the mandate is 71 times that a childs bod...</td>\n",
       "      <td>say the mandat is 71 time that a child bodi wi...</td>\n",
       "      <td>say mandat 71 time child bodi inject diseas immun</td>\n",
       "      <td>[say, mandat, 71, time, child, bodi, inject, d...</td>\n",
       "      <td>Other</td>\n",
       "      <td>2013-06-19</td>\n",
       "      <td>10031</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>[87273, 87227, 11765]</td>\n",
       "      <td>[[1, 12, 18, 11, 10], [8, 6, 4, 12, 7], [0, 1,...</td>\n",
       "      <td>[0.7491824984550476, 0.7040371179580689, 0.242...</td>\n",
       "      <td>0.565311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw_claim  \\\n",
       "0   A line from George Orwell's novel 1984 predict...   \n",
       "1   Maine legislature candidate Leslie Gibson insu...   \n",
       "2   A 17-year-old girl named Alyssa Carson is bein...   \n",
       "3   In 1988 author Roald Dahl penned an open lette...   \n",
       "4   When it comes to fighting terrorism, \"Another ...   \n",
       "5   Rhode Island is \"almost dead last\" among North...   \n",
       "6   The poorest counties in the U.S. are in Appala...   \n",
       "7   Koch Industries paid the legal fees of George ...   \n",
       "8   \"Minnesota, Michigan, Iowa already have 70 mph...   \n",
       "9   \"FBI Uniform Crime Report for 2016 shows more ...   \n",
       "10   \"Pelosi Sinks to New Low, Tells Dems: If You ...   \n",
       "11  Socialist teachers at South Charlotte Middle S...   \n",
       "12  Says that in the U.S. Capitol, \"Stephen F. Aus...   \n",
       "13       NASA Has Just Confirmed Earth Has A New Moon   \n",
       "14  \"We are always going to need architects, docto...   \n",
       "15  \"Justin Amash is rated Michigan’s No. 1 conser...   \n",
       "16   BREAKING: NFL Owner Listens to Trump, Fires P...   \n",
       "17  Says one year ago, \"no cities in the South had...   \n",
       "18  Says North Carolina Republican Senate candidat...   \n",
       "19  Says \"the mandate is 71 times that a child’s b...   \n",
       "\n",
       "                                        cleaned_claim  \\\n",
       "0   a line from george orwells novel 1984 predicts...   \n",
       "1   maine legislature candidate leslie gibson insu...   \n",
       "2   a 17yearold girl named alyssa carson is being ...   \n",
       "3   in 1988 author roald dahl penned an open lette...   \n",
       "4   when it comes to fighting terrorism another th...   \n",
       "5   rhode island is almost dead last among northea...   \n",
       "6   the poorest counties in the us are in appalach...   \n",
       "7   koch industries paid the legal fees of george ...   \n",
       "8   minnesota michigan iowa already have 70 mph sp...   \n",
       "9   fbi uniform crime report for 2016 shows more t...   \n",
       "10   pelosi sinks to new low tells dems if you hav...   \n",
       "11  socialist teachers at south charlotte middle s...   \n",
       "12  says that in the us capitol stephen f austins ...   \n",
       "13       nasa has just confirmed earth has a new moon   \n",
       "14  we are always going to need architects doctors...   \n",
       "15  justin amash is rated michigans no 1 conservat...   \n",
       "16   breaking nfl owner listens to trump fires pla...   \n",
       "17  says one year ago no cities in the south had g...   \n",
       "18  says north carolina republican senate candidat...   \n",
       "19  says the mandate is 71 times that a childs bod...   \n",
       "\n",
       "                                       stemmed_claims  \\\n",
       "0   a line from georg orwel novel 1984 predict the...   \n",
       "1   main legislatur candid lesli gibson insult par...   \n",
       "2   a 17yearold girl name alyssa carson is be trai...   \n",
       "3   in 1988 author roald dahl pen an open letter u...   \n",
       "4   when it come to fight terror anoth thing we kn...   \n",
       "5   rhode island is almost dead last among northea...   \n",
       "6   the poorest counti in the us are in appalachia...   \n",
       "7   koch industri paid the legal fee of georg zimm...   \n",
       "8   minnesota michigan iowa alreadi have 70 mph sp...   \n",
       "9   fbi uniform crime report for 2016 show more th...   \n",
       "10  pelosi sink to new low tell dem if you have to...   \n",
       "11  socialist teacher at south charlott middl scho...   \n",
       "12  say that in the us capitol stephen f austin an...   \n",
       "13           nasa ha just confirm earth ha a new moon   \n",
       "14  we are alway go to need architect doctor were ...   \n",
       "15  justin amash is rate michigan no 1 conserv by ...   \n",
       "16  break nfl owner listen to trump fire player fo...   \n",
       "17  say one year ago no citi in the south had guar...   \n",
       "18  say north carolina republican senat candid tho...   \n",
       "19  say the mandat is 71 time that a child bodi wi...   \n",
       "\n",
       "                              stemmed_stopword_claims  \\\n",
       "0   line georg orwel novel 1984 predict power smar...   \n",
       "1   main legislatur candid lesli gibson insult par...   \n",
       "2   17yearold girl name alyssa carson train nasa b...   \n",
       "3   1988 author roald dahl pen open letter urg par...   \n",
       "4   come fight terror anoth thing know doe work ba...   \n",
       "5   rhode island almost dead last among northeaste...   \n",
       "6   poorest counti us appalachia happen 90 percent...   \n",
       "7        koch industri paid legal fee georg zimmerman   \n",
       "8   minnesota michigan iowa alreadi 70 mph speed l...   \n",
       "9   fbi uniform crime report 2016 show four time m...   \n",
       "10         pelosi sink new low tell dem lie voter win   \n",
       "11  socialist teacher south charlott middl school ...   \n",
       "12  say us capitol stephen f austin sam houston st...   \n",
       "13                  nasa ha confirm earth ha new moon   \n",
       "14  alway go need architect doctor go need profess...   \n",
       "15  justin amash rate michigan 1 conserv nation ri...   \n",
       "16  break nfl owner listen trump fire player disgr...   \n",
       "17  say one year ago citi south guarante paid sick...   \n",
       "18  say north carolina republican senat candid tho...   \n",
       "19  say mandat 71 time child bodi inject diseas immun   \n",
       "\n",
       "                                      tokenized_claim         claimant  \\\n",
       "0   [line, georg, orwel, novel, 1984, predict, pow...          Unknown   \n",
       "1   [main, legislatur, candid, lesli, gibson, insu...          Unknown   \n",
       "2   [17yearold, girl, name, alyssa, carson, train,...          Unknown   \n",
       "3   [1988, author, roald, dahl, pen, open, letter,...          Unknown   \n",
       "4   [come, fight, terror, anoth, thing, know, doe,...  Hillary Clinton   \n",
       "5   [rhode, island, almost, dead, last, among, nor...            Other   \n",
       "6   [poorest, counti, us, appalachia, happen, 90, ...            Other   \n",
       "7   [koch, industri, paid, legal, fee, georg, zimm...          Unknown   \n",
       "8   [minnesota, michigan, iowa, alreadi, 70, mph, ...            Other   \n",
       "9   [fbi, uniform, crime, report, 2016, show, four...            Other   \n",
       "10  [pelosi, sink, new, low, tell, dem, lie, voter...            Other   \n",
       "11  [socialist, teacher, south, charlott, middl, s...          Unknown   \n",
       "12  [say, us, capitol, stephen, f, austin, sam, ho...            Other   \n",
       "13          [nasa, ha, confirm, earth, ha, new, moon]         Bloggers   \n",
       "14  [alway, go, need, architect, doctor, go, need,...            Other   \n",
       "15  [justin, amash, rate, michigan, 1, conserv, na...            Other   \n",
       "16  [break, nfl, owner, listen, trump, fire, playe...            Other   \n",
       "17  [say, one, year, ago, citi, south, guarante, p...            Other   \n",
       "18  [say, north, carolina, republican, senat, cand...            Other   \n",
       "19  [say, mandat, 71, time, child, bodi, inject, d...            Other   \n",
       "\n",
       "         date  cont_days  year  month  label  \\\n",
       "0  2017-07-17      11520  2017      7      0   \n",
       "1  2018-03-17      11763  2018      3      2   \n",
       "2  2018-07-18      11886  2018      7      1   \n",
       "3  2019-02-04      12087  2019      2      2   \n",
       "4  2016-03-22      11038  2016      3      2   \n",
       "5  2014-02-11      10268  2014      2      2   \n",
       "6  2014-11-19      10549  2014     11      1   \n",
       "7  2013-07-18      10060  2013      7      0   \n",
       "8  2013-08-22      10095  2013      8      1   \n",
       "9  2017-10-17      11612  2017     10      1   \n",
       "10 2018-08-21      11920  2018      8      0   \n",
       "11 2018-10-17      11977  2018     10      1   \n",
       "12 2018-03-28      11774  2018      3      1   \n",
       "13 2018-03-29      11775  2018      3      0   \n",
       "14 2019-01-24      12076  2019      1      2   \n",
       "15 2014-07-01      10408  2014      7      0   \n",
       "16 2017-09-29      11594  2017      9      0   \n",
       "17 2019-04-24      12166  2019      4      0   \n",
       "18 2014-04-17      10333  2014      4      1   \n",
       "19 2013-06-19      10031  2013      6      0   \n",
       "\n",
       "                                        article_array  \\\n",
       "0                    [122094, 122580, 130685, 134765]   \n",
       "1                            [106868, 127320, 128060]   \n",
       "2                            [132130, 132132, 149722]   \n",
       "3                            [123254, 123418, 127464]   \n",
       "4          [41099, 89899, 72543, 82644, 95344, 88361]   \n",
       "5             [8284, 3768, 20091, 82368, 73148, 4493]   \n",
       "6                                      [70709, 70708]   \n",
       "7                    [120591, 120592, 127866, 129483]   \n",
       "8                  [69547, 80095, 7994, 81116, 77621]   \n",
       "9                        [72012, 26005, 43481, 55671]   \n",
       "10                [27062, 27061, 20679, 61872, 20677]   \n",
       "11                                   [104287, 144516]   \n",
       "12                              [16639, 16657, 16667]   \n",
       "13                       [91455, 72179, 18903, 42080]   \n",
       "14                              [42685, 32007, 33562]   \n",
       "15  [22383, 72467, 72466, 86512, 73422, 83732, 83730]   \n",
       "16                        [20907, 73380, 22540, 2010]   \n",
       "17         [87410, 18608, 57313, 35767, 85310, 43631]   \n",
       "18                [81476, 67734, 73202, 96584, 73198]   \n",
       "19                              [87273, 87227, 11765]   \n",
       "\n",
       "                                    best_sentences_ID  \\\n",
       "0   [[20, 16, 19, 6, 11], [5, 0, 25, 8, 10], [0, 2...   \n",
       "1   [[12, 13, 0, 1, 4], [4, 22, 48, 1, 25], [4, 9,...   \n",
       "2   [[5, 26, 28, 31, 9], [86, 33, 77, 34, 37], [1,...   \n",
       "3   [[59, 26, 20, 88, 113], [87, 163, 195, 108, 22...   \n",
       "4   [[72, 120, 65, 168, 48], [6, 0, 1, 19, 2], [3,...   \n",
       "5   [[6, 0, 10, 7, 2], [8, 2, 11, 13, 22], [6, 4, ...   \n",
       "6                           [[1, 0], [2, 6, 5, 4, 1]]   \n",
       "7   [[1, 4, 3, 2, 0], [6, 4, 1, 2, 5], [33, 7, 4, ...   \n",
       "8   [[70, 48, 42, 19, 44], [0, 2, 7, 8, 1], [27, 1...   \n",
       "9   [[0, 5, 1, 4, 7], [3, 6, 4, 5, 1], [11, 0, 4, ...   \n",
       "10  [[0, 4, 8, 6, 15], [7, 8, 11, 0, 21], [14, 21,...   \n",
       "11           [[2, 0, 9, 10, 11], [4, 15, 19, 14, 23]]   \n",
       "12  [[1, 5, 3, 2, 4], [2, 11, 9, 0, 7], [5, 2, 0, ...   \n",
       "13  [[9, 19, 3, 28, 5], [10, 11, 2, 5, 3], [10, 1,...   \n",
       "14           [[1, 3, 10, 4, 6], [0], [5, 2, 0, 4, 1]]   \n",
       "15  [[0], [2, 10, 4, 7, 11], [108, 104, 14, 61, 10...   \n",
       "16  [[0, 2, 1], [4, 2, 8, 30, 50], [67, 51, 34, 16...   \n",
       "17  [[18, 13, 16, 2, 1], [19, 3, 11, 0, 9], [0, 8,...   \n",
       "18  [[41, 22, 85, 78, 80], [29, 12, 26, 23, 28], [...   \n",
       "19  [[1, 12, 18, 11, 10], [8, 6, 4, 12, 7], [0, 1,...   \n",
       "\n",
       "                                     avg_sentence_sim  avg_sim_score  \n",
       "0   [0.49315634965896604, 0.40044230222702026, 0.1...       0.332769  \n",
       "1   [0.7131051063537598, 0.759801709651947, 0.7064...       0.726454  \n",
       "2   [0.8618260860443115, 0.904123330116272, 0.4341...       0.733378  \n",
       "3   [0.8745679140090943, 0.8695014953613281, 0.871...       0.871977  \n",
       "4   [0.6641334772109986, 0.636748218536377, 0.4504...       0.652727  \n",
       "5   [-0.00010861875489354134, 0.19662532806396485,...       0.315237  \n",
       "6          [0.34360191226005554, 0.18681026697158815]       0.265206  \n",
       "7   [0.5746460080146789, 0.47094449400901794, 0.50...       0.560426  \n",
       "8   [0.5380596280097961, 0.590717875957489, 0.6359...       0.516865  \n",
       "9   [0.7591381788253784, 0.7119978070259094, 0.521...       0.669273  \n",
       "10  [0.7097761988639831, 0.6724220991134644, 0.733...       0.692177  \n",
       "11        [0.026715978980064392, 0.29803799986839297]       0.162377  \n",
       "12  [0.6564371228218079, 0.7978529214859009, 0.748...       0.734398  \n",
       "13  [0.6802428364753723, 0.7061182737350464, 0.742...       0.723062  \n",
       "14  [0.21930979192256927, -0.22811169922351837, -0...      -0.028883  \n",
       "15  [0.5148963332176208, 0.6793382167816162, 0.691...       0.641952  \n",
       "16  [0.5069699088732401, 0.7903198957443237, 0.661...       0.650730  \n",
       "17  [0.5099975764751434, 0.6683658123016357, 0.675...       0.417633  \n",
       "18  [0.8923457384109497, 0.8810298800468445, 0.889...       0.869995  \n",
       "19  [0.7491824984550476, 0.7040371179580689, 0.242...       0.565311  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.to_pickle(\"./data_w_sim_scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the average scores of each label\n",
    "_ = data2.plot(x='label', y='avg_sim_score', style='o')\n",
    "_ = plt.xlim(-1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.docvecs['122094-20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = model.docvecs['0']\n",
    "v2 = model.docvecs['122094-20']\n",
    "result = 1 - spatial.distance.cosine(v1, v2)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to work with the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to go to a specific cleaned claim\n",
    "data.cleaned_claim.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to go to a specific cleaned article\n",
    "articles.raw_articles.loc['134765'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample code to read elements from each article_array from the data dataframe\n",
    "\n",
    "# to iterate the article array\n",
    "for i in range(data.shape[0]):\n",
    "    # i iterates row by row till the end\n",
    "    for u in range(len(data.article_array[i])):\n",
    "        # u holds the index of each element, within each array. Uncomment the following to understand\n",
    "        # print(u)\n",
    "        art_array = data.article_array[i]\n",
    "        # print specific elements of each array\n",
    "        print(art_array[u])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
