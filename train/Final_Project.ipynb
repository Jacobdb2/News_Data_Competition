{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIE 1624: Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group 22 Members\n",
    "\n",
    "Aaron Hao Tan, Gurtej Bhasin, Elise Emma, Jacob Bulir, Alex Kwan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "< -- Insert brief intro to project -- >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Preliminaries\n",
    "#### 1. Data Cleaning\n",
    "    - 1.1 Claim Cleaning\n",
    "    - 1.2 Related Articles Cleaning\n",
    "        - 1.2.1 Extract Sentences\n",
    "    - 1.3 Removing Stopwords and Stemming\n",
    "        - 1.3.1 Claims\n",
    "        - 1.3.2 Related Articles\n",
    "    - 1.4 Date Cleaning\n",
    "    - 1.5 Claimant Cleaning\n",
    "    - 1.6 Finalize Cleaned Data\n",
    "        - 1.6.1 Finalize Train Data\n",
    "        - 1.6.2 Finalzie Related Articles Data\n",
    "#### 2. Exploratory Data Analysis\n",
    "    - 2.1 Date Features\n",
    "    - 2.2 Claimant Features\n",
    "    - 2.3 Doc2Vec Features\n",
    "        - 2.3.1 Preprocess the Data\n",
    "        - 2.3.2 Prepare for Training\n",
    "        - 2.3.3 Doc2Vec Training\n",
    "        - 2.3.4 Find Similarities\n",
    "            - 2.3.4.1 Finding Top 5 Sentences per Related Articles per Claim\n",
    "            - 2.3.4.2 Finding Top 5 Sentences Amongst all Related Articles per Claim\n",
    "        - 2.3.5 Feature Encoding\n",
    "            - 2.3.5.1 Feature Encode the Claims\n",
    "            - 2.3.5.2 Feature Encode the Top 5 Related Article Sentences for Each Claim\n",
    "            - 2.3.5.3 Feature Encode the Similarity Scores\n",
    "        - 2.3.6 Combine All Features (Claim Vector, Top 5 Related Sentence Vector, Similarity Scores)\n",
    "    - 2.4 Sentiment Features\n",
    "    - 2.5 Exploration of Features\n",
    "#### 3. Feature Selection\n",
    "#### 4. Model Implementation\n",
    "#### 5. Model Tuning\n",
    "#### 6. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "# Import Libraries\n",
    "#----------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "import math\n",
    "from IPython.display import clear_output, display\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import string\n",
    "import heapq\n",
    "from heapq import nlargest\n",
    "from operator import itemgetter\n",
    "\n",
    "# The following line is needed to show plots inline in notebooks\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the train.json file as well as the related articles are cleaned. The output of this section are two data frames that hold the cleaned information. The following shows the subsections:\n",
    "\n",
    "- 1.1 Claim Cleaning\n",
    "- 1.2 Related Articles Cleaning\n",
    "    - 1.2.1 Extract Sentences\n",
    "- 1.3 Removing Stopwords and Stemming\n",
    "    - 1.3.1 Claims\n",
    "    - 1.3.2 Related Articles\n",
    "- 1.4 Date Cleaning\n",
    "- 1.5 Claimant Cleaning\n",
    "- 1.6 Finalize Cleaned Data\n",
    "    - 1.6.1 Finalize Train Data\n",
    "    - 1.6.2 Finalzie Related Articles Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Claim Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the \"train.json\" file and perform some basic cleaning on the claims. This includes the following key points:\n",
    "\n",
    "    - Lowercase all claims\n",
    "    - Remove \"\\n\" new lines\n",
    "    - Remove URL links\n",
    "    - Remove unicode hex\n",
    "    - Remove punctuations\n",
    "    - Remove misc. items such as \"-\", \"'\" ... etc.\n",
    "    - Remove any extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15555, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read train.json file for training data\n",
    "data = pd.read_json(\"train.json\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 445 ms, sys: 2.7 ms, total: 448 ms\n",
      "Wall time: 447 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# CLEAN CLAIM DATA\n",
    "\n",
    "# create a new dataframe of just claims\n",
    "cleaned_claim = data.claim.to_frame()\n",
    "\n",
    "# convert all string values to lower case\n",
    "cleaned_claim = cleaned_claim.apply(lambda x: x.str.lower())\n",
    "\n",
    "# replace new line with space\n",
    "cleaned_claim = cleaned_claim.replace('\\n', ' ', regex=True)\n",
    "\n",
    "# get rid of all links\n",
    "cleaned_claim = cleaned_claim.claim.replace(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}     /)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', regex = True).to_frame()\n",
    "\n",
    "# get rid of unicode hex\n",
    "cleaned_claim = cleaned_claim.claim.replace({r'[^\\x00-\\x7F]+':''}, regex=True).to_frame()\n",
    "\n",
    "# remove punctuation\n",
    "cleaned_claim = cleaned_claim.claim.str.replace('[{}]'.format(string.punctuation), '').to_frame()\n",
    "\n",
    "# remove misc items\n",
    "cleaned_claim = cleaned_claim.replace(' — ', ' ', regex=True)\n",
    "cleaned_claim = cleaned_claim.replace('-', ' ', regex=True)\n",
    "cleaned_claim = cleaned_claim.replace('’', '', regex=True)\n",
    "cleaned_claim = cleaned_claim.replace('‘', '', regex=True)\n",
    "cleaned_claim = cleaned_claim.replace('”', '', regex=True)\n",
    "cleaned_claim = cleaned_claim.replace('“', '', regex=True)\n",
    "\n",
    "# replace consecutive spaces with just one space\n",
    "cleaned_claim = cleaned_claim.replace('\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of manipulation, the related articles information are converted in to numpy arrays with the following code segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert related_articles list to a row of array\n",
    "\n",
    "article_array = []\n",
    "for i in range(data.shape[0]):\n",
    "    array = np.asarray(data['related_articles'].loc[i])\n",
    "    article_array.append(array)\n",
    "    \n",
    "data['article_array'] = article_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned claims, labels and the related articles in array format are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>label</th>\n",
       "      <th>article_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a line from george orwells novel 1984 predicts...</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>maine legislature candidate leslie gibson insu...</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a 17yearold girl named alyssa carson is being ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in 1988 author roald dahl penned an open lette...</td>\n",
       "      <td>2</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when it comes to fighting terrorism another th...</td>\n",
       "      <td>2</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim  label  \\\n",
       "0  a line from george orwells novel 1984 predicts...      0   \n",
       "1  maine legislature candidate leslie gibson insu...      2   \n",
       "2  a 17yearold girl named alyssa carson is being ...      1   \n",
       "3  in 1988 author roald dahl penned an open lette...      2   \n",
       "4  when it comes to fighting terrorism another th...      2   \n",
       "\n",
       "                                article_array  \n",
       "0            [122094, 122580, 130685, 134765]  \n",
       "1                    [106868, 127320, 128060]  \n",
       "2                    [132130, 132132, 149722]  \n",
       "3                    [123254, 123418, 127464]  \n",
       "4  [41099, 89899, 72543, 82644, 95344, 88361]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate cleaned_claims with label and article_array\n",
    "cleaned_claim = pd.concat([cleaned_claim, data.label, data.article_array], axis=1)\n",
    "\n",
    "# cleaned_claim now holds the claims that are cleaned, the label, and the article array\n",
    "cleaned_claim.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Related Articles Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the procedures used to clean the claims, we clean the related articles in this section. To begin, we first create a dataframe that holds every related article. The index in this case are the article ID's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.44 s, sys: 459 ms, total: 1.9 s\n",
      "Wall time: 1.91 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125923</th>\n",
       "      <td>FACT CHECK: The Five Pillars of Curriculum\\nFA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57270</th>\n",
       "      <td>The State Department Spent $52,701 on the Curt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144732</th>\n",
       "      <td>US show of force sends Russia a message in Bla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28766</th>\n",
       "      <td>Mexico missing students: Protesters clash with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85305</th>\n",
       "      <td>The U.S. Refugee Resettlement Program: A Prime...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Article\n",
       "125923  FACT CHECK: The Five Pillars of Curriculum\\nFA...\n",
       "57270   The State Department Spent $52,701 on the Curt...\n",
       "144732  US show of force sends Russia a message in Bla...\n",
       "28766   Mexico missing students: Protesters clash with...\n",
       "85305   The U.S. Refugee Resettlement Program: A Prime..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# set paths\n",
    "cur_path = os.path.dirname(os.path.abspath(\"Final_Project.ipynb\"))\n",
    "articles_dir = cur_path + '/train_articles/'\n",
    "\n",
    "# create a dictionary of article ID and content\n",
    "article_dict = {}\n",
    "for filename in os.listdir(articles_dir):\n",
    "    filenumber = filename.replace('.txt', '')\n",
    "    file_open = open(articles_dir + filename, \"r\")\n",
    "    text = file_open.read()\n",
    "    article_dict[filenumber] = text\n",
    "    \n",
    "# use the dictionary created to create a dataframe of articles\n",
    "articles  = pd.DataFrame.from_dict(article_dict, orient='index')\n",
    "articles.columns = ['Article']\n",
    "\n",
    "# a dataframe that holds all the articles\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the related articles dataframe, the following performs the basic cleaning that was also applied to the claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 54s, sys: 653 ms, total: 1min 55s\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# CLEAN ARTICLE DATA - ~5 minutes to run\n",
    "\n",
    "# convert all string values to lower case\n",
    "articles_cleaned = articles.apply(lambda x: x.str.lower())\n",
    "\n",
    "# replace new line with space\n",
    "articles_cleaned = articles_cleaned.replace('\\n', ' ', regex=True)\n",
    "\n",
    "# get rid of all links\n",
    "articles_cleaned = articles_cleaned.Article.replace(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}     /)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', regex = True).to_frame()\n",
    "\n",
    "# get rid of unicode hex\n",
    "articles_cleaned = articles_cleaned.Article.replace({r'[^\\x00-\\x7F]+':''}, regex=True).to_frame()\n",
    "\n",
    "# remove misc items\n",
    "articles_cleaned = articles_cleaned.replace(' — ', ' ', regex=True)\n",
    "articles_cleaned = articles_cleaned.replace('-', '', regex=True)\n",
    "articles_cleaned = articles_cleaned.replace('’', '', regex=True)\n",
    "articles_cleaned = articles_cleaned.replace('‘', '', regex=True)\n",
    "articles_cleaned = articles_cleaned.replace('”', '', regex=True)\n",
    "articles_cleaned = articles_cleaned.replace('“', '', regex=True)\n",
    "\n",
    "# replace consecutive spaces with just one space\n",
    "articles_cleaned = articles_cleaned.replace('\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Extract Sentences from Related Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we perform sentence tokenization on all the related articles to extract individual sentences from each article. Puntuations are then removed from the extracted sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 2s, sys: 552 ms, total: 2min 2s\n",
      "Wall time: 2min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# split each article in to sentences by \"sent_tokenize\"\n",
    "from nltk import sent_tokenize\n",
    "article_sentences = []\n",
    "for i in range(articles_cleaned.shape[0]):\n",
    "    sentence = articles_cleaned.Article[articles_cleaned.index[i]]\n",
    "    tok_sen = sent_tokenize(sentence)\n",
    "    article_sentences.append(tok_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation from cleaned articles - had to do this after the sentences are separated\n",
    "articles_cleaned = articles_cleaned.Article.str.replace('[{}]'.format(string.punctuation), '').to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.4 s, sys: 388 ms, total: 50.8 s\n",
      "Wall time: 50.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# remove punctuation from sentences\n",
    "cleaned_article_sent = []\n",
    "for i in range(articles_cleaned.shape[0]):\n",
    "    cleaned_sent = [''.join(c for c in s if c not in string.punctuation) for s in article_sentences[i]]\n",
    "    # cleaned_article_sent is a list of lists\n",
    "    # the sentences from each article is grouped in to a list, and then grouped together based on article\n",
    "    cleaned_article_sent.append(cleaned_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows the article sentences dataframe, where each cell holds a list of sentences, indexed by the related article ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125923</th>\n",
       "      <td>[fact check the five pillars of curriculum fac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57270</th>\n",
       "      <td>[the state department spent 52701 on the curta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144732</th>\n",
       "      <td>[us show of force sends russia a message in bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28766</th>\n",
       "      <td>[mexico missing students protesters clash with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85305</th>\n",
       "      <td>[the us refugee resettlement program a primer ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Sentence\n",
       "125923  [fact check the five pillars of curriculum fac...\n",
       "57270   [the state department spent 52701 on the curta...\n",
       "144732  [us show of force sends russia a message in bl...\n",
       "28766   [mexico missing students protesters clash with...\n",
       "85305   [the us refugee resettlement program a primer ..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the cleaned_Article_sent in to a dataframe that's indexed by article ID\n",
    "article_sentences = pd.Series(cleaned_article_sent).to_frame()\n",
    "article_sentences.columns = ['Sentence']\n",
    "article_sentences.index = articles_cleaned.index\n",
    "\n",
    "# article sentences in a dataframe\n",
    "article_sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Remove Stopwords and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the basic cleaning and sentence extraction are completed, the next step is to remove stopwords and perform stemming on the remaining words. The purpose is to reduce the vocabulary space by keeping only the words that are meaningful.The following imports the necessary libraries to conduct these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "# the first time running - you may need to uncomment the bottom two lines to download the necessary packages\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section removes the stopwords and stems the claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of claims\n",
    "claim_list=[]\n",
    "for i in range(cleaned_claim.shape[0]):\n",
    "    claim_entry = cleaned_claim.claim.loc[i]\n",
    "    claim_list.append(claim_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 99.99%\n",
      "CPU times: user 34.6 s, sys: 2.33 s, total: 36.9 s\n",
      "Wall time: 34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# tokenize every claim in the claim list generated from above\n",
    "# the result is a list of tokenized claims: tokenized_claims\n",
    "tokenized_claims = []\n",
    "stemmed_claims = []\n",
    "stemmed_sw_claims = []\n",
    "for i in range(cleaned_claim.shape[0]):\n",
    "\n",
    "    #--------------------------------------------------------------\n",
    "    # stemming\n",
    "    word_tokens = word_tokenize(claim_list[i])\n",
    "    stemmed_tok_claims = []\n",
    "    for w in word_tokens:\n",
    "        stemmed_tok_claims.append(ps.stem(w))\n",
    "    stemmed_string = ' '.join(stemmed_tok_claims)\n",
    "    # stemmed_claims is a list of stemmed strings\n",
    "    stemmed_claims.append(stemmed_string)\n",
    "    \n",
    "    #--------------------------------------------------------------\n",
    "    # remove stop words\n",
    "    stemmed_sw_string = []\n",
    "    word_tokens = word_tokenize(stemmed_claims[i])\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    stemmed_sw_string = ' '.join(filtered_sentence)\n",
    "    # stemmed_sw_claims is a list of stemmed strings without stopwords\n",
    "    stemmed_sw_claims.append(stemmed_sw_string)    \n",
    "        \n",
    "    #--------------------------------------------------------------    \n",
    "    # tokenize\n",
    "    tokenized_ = word_tokenize(stemmed_sw_claims[i])\n",
    "    tokenized_claims.append(tokenized_)\n",
    "    \n",
    "    # print progress\n",
    "    progress = round((i/cleaned_claim.shape[0])*100,2)\n",
    "    clear_output(wait=True)\n",
    "    print(\"progress: \" + str(progress) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Claims Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stemmed claims without stopwords are added to a claims dataframe here. In order to keep as much information as possible, each column below represent the claim at each level of the cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_claims</th>\n",
       "      <th>stemmed_stopword_claims</th>\n",
       "      <th>tokenized_claims</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a line from georg orwel novel 1984 predict the...</td>\n",
       "      <td>line georg orwel novel 1984 predict power smar...</td>\n",
       "      <td>[line, georg, orwel, novel, 1984, predict, pow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>[main, legislatur, candid, lesli, gibson, insu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a 17yearold girl name alyssa carson is be trai...</td>\n",
       "      <td>17yearold girl name alyssa carson train nasa b...</td>\n",
       "      <td>[17yearold, girl, name, alyssa, carson, train,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in 1988 author roald dahl pen an open letter u...</td>\n",
       "      <td>1988 author roald dahl pen open letter urg par...</td>\n",
       "      <td>[1988, author, roald, dahl, pen, open, letter,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when it come to fight terror anoth thing we kn...</td>\n",
       "      <td>come fight terror anoth thing know doe work ba...</td>\n",
       "      <td>[come, fight, terror, anoth, thing, know, doe,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      stemmed_claims  \\\n",
       "0  a line from georg orwel novel 1984 predict the...   \n",
       "1  main legislatur candid lesli gibson insult par...   \n",
       "2  a 17yearold girl name alyssa carson is be trai...   \n",
       "3  in 1988 author roald dahl pen an open letter u...   \n",
       "4  when it come to fight terror anoth thing we kn...   \n",
       "\n",
       "                             stemmed_stopword_claims  \\\n",
       "0  line georg orwel novel 1984 predict power smar...   \n",
       "1  main legislatur candid lesli gibson insult par...   \n",
       "2  17yearold girl name alyssa carson train nasa b...   \n",
       "3  1988 author roald dahl pen open letter urg par...   \n",
       "4  come fight terror anoth thing know doe work ba...   \n",
       "\n",
       "                                    tokenized_claims  \n",
       "0  [line, georg, orwel, novel, 1984, predict, pow...  \n",
       "1  [main, legislatur, candid, lesli, gibson, insu...  \n",
       "2  [17yearold, girl, name, alyssa, carson, train,...  \n",
       "3  [1988, author, roald, dahl, pen, open, letter,...  \n",
       "4  [come, fight, terror, anoth, thing, know, doe,...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zip together all the claim lists and create a dataframe\n",
    "zipped_claims = list(zip(stemmed_claims, stemmed_sw_claims, tokenized_claims))\n",
    "claims_ = pd.DataFrame(zipped_claims, columns = ['stemmed_claims', 'stemmed_stopword_claims', 'tokenized_claims'])\n",
    "\n",
    "#display the claims dataframe\n",
    "claims_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Related Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the procedure performed on the claims, this section removes stopwords and stems the sentences extracted from the related articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 100.0%\n",
      "CPU times: user 31min 20s, sys: 21 s, total: 31min 41s\n",
      "Wall time: 31min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create a list of tokenized, non-stop words articles ~ takes 30 min\n",
    "tokenized_articles = []\n",
    "stemmed_art = []\n",
    "stemmed_sw_art = []\n",
    "\n",
    "for i in range(articles_cleaned.shape[0]):\n",
    "\n",
    "    #--------------------------------------------------------------\n",
    "    # stemming\n",
    "    word_tokens = word_tokenize(articles_cleaned.Article[articles_cleaned.index[i]])\n",
    "    stemmed_tok_art = []\n",
    "    for w in word_tokens:\n",
    "        stemmed_tok_art.append(ps.stem(w))\n",
    "    stemmed_string = ' '.join(stemmed_tok_art)\n",
    "    # stemmed_claims is a list of stemmed strings\n",
    "    stemmed_art.append(stemmed_string)\n",
    "    \n",
    "    #--------------------------------------------------------------\n",
    "    # remove stop words\n",
    "    stemmed_sw_string = []\n",
    "    word_tokens = word_tokenize(stemmed_art[i])\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    stemmed_sw_string = ' '.join(filtered_sentence)\n",
    "    # stemmed_sw_claims is a list of stemmed strings without stopwords\n",
    "    stemmed_sw_art.append(stemmed_sw_string)    \n",
    "    \n",
    "    #--------------------------------------------------------------    \n",
    "    # tokenize\n",
    "    tokenized_ = word_tokenize(stemmed_sw_art[i])\n",
    "    tokenized_articles.append(tokenized_)\n",
    "    \n",
    "    # print progress\n",
    "    progress = round((i/articles_cleaned.shape[0])*100,2)\n",
    "    clear_output(wait=True)\n",
    "    print(\"progress: \" + str(progress) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentences for Related Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 100.0%\n",
      "CPU times: user 29min 27s, sys: 26.5 s, total: 29min 54s\n",
      "Wall time: 29min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# stem, remove stop words from sentences ~ takes 30 min\n",
    "\n",
    "one_article = []\n",
    "final_article_sentences = []\n",
    "tokenized_sentences = []\n",
    "final_article_tok_sentences = []\n",
    "\n",
    "for i in range(article_sentences.shape[0]):\n",
    "    # grab a series of sentence for one article\n",
    "    sentence_list = article_sentences.Sentence[article_sentences.index[i]]\n",
    "    for u in range(len(sentence_list)):\n",
    "        # stem, stopwords for each sentence in an article\n",
    "        sent = sentence_list[u]\n",
    "        word_tokens = word_tokenize(sent)\n",
    "        stemmed_tok_sent = []\n",
    "        for w in word_tokens:\n",
    "            stemmed_tok_sent.append(ps.stem(w))\n",
    "        filtered_sentence = [w for w in stemmed_tok_sent if not w in stop_words]\n",
    "        # append the tokenized strings\n",
    "        tokenized_sentences.append(filtered_sentence)\n",
    "        # append the strings\n",
    "        stemmed_sw_string = ' '.join(filtered_sentence)\n",
    "        one_article.append(stemmed_sw_string)\n",
    "    # append each one_article to a full list of all articles\n",
    "    final_article_sentences.append(one_article)  \n",
    "    final_article_tok_sentences.append(tokenized_sentences)\n",
    "    one_article = []\n",
    "    tokenized_sentences = []\n",
    "    \n",
    "    # print progress\n",
    "    progress = round((i/article_sentences.shape[0])*100,2)\n",
    "    clear_output(wait=True)\n",
    "    print(\"progress: \" + str(progress) + \"%\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentences from the related articles at each level of the cleaning are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "      <th>tokenized_cleaned_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125923</th>\n",
       "      <td>[fact check the five pillars of curriculum fac...</td>\n",
       "      <td>[fact check five pillar curriculum fact check ...</td>\n",
       "      <td>[[fact, check, five, pillar, curriculum, fact,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57270</th>\n",
       "      <td>[the state department spent 52701 on the curta...</td>\n",
       "      <td>[state depart spent 52701 curtain un ambassado...</td>\n",
       "      <td>[[state, depart, spent, 52701, curtain, un, am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144732</th>\n",
       "      <td>[us show of force sends russia a message in bl...</td>\n",
       "      <td>[us show forc send russia messag black sea was...</td>\n",
       "      <td>[[us, show, forc, send, russia, messag, black,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28766</th>\n",
       "      <td>[mexico missing students protesters clash with...</td>\n",
       "      <td>[mexico miss student protest clash polic media...</td>\n",
       "      <td>[[mexico, miss, student, protest, clash, polic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85305</th>\n",
       "      <td>[the us refugee resettlement program a primer ...</td>\n",
       "      <td>[us refuge resettl program primer policymak pr...</td>\n",
       "      <td>[[us, refuge, resettl, program, primer, policy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Sentence  \\\n",
       "125923  [fact check the five pillars of curriculum fac...   \n",
       "57270   [the state department spent 52701 on the curta...   \n",
       "144732  [us show of force sends russia a message in bl...   \n",
       "28766   [mexico missing students protesters clash with...   \n",
       "85305   [the us refugee resettlement program a primer ...   \n",
       "\n",
       "                                         cleaned_sentence  \\\n",
       "125923  [fact check five pillar curriculum fact check ...   \n",
       "57270   [state depart spent 52701 curtain un ambassado...   \n",
       "144732  [us show forc send russia messag black sea was...   \n",
       "28766   [mexico miss student protest clash polic media...   \n",
       "85305   [us refuge resettl program primer policymak pr...   \n",
       "\n",
       "                               tokenized_cleaned_sentence  \n",
       "125923  [[fact, check, five, pillar, curriculum, fact,...  \n",
       "57270   [[state, depart, spent, 52701, curtain, un, am...  \n",
       "144732  [[us, show, forc, send, russia, messag, black,...  \n",
       "28766   [[mexico, miss, student, protest, clash, polic...  \n",
       "85305   [[us, refuge, resettl, program, primer, policy...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# article_sentences is the dataframe that holds the sentences\n",
    "article_sentences['cleaned_sentence'] = final_article_sentences\n",
    "article_sentences['tokenized_cleaned_sentence'] = final_article_tok_sentences\n",
    "article_sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Related Articles DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned related articles as well as the sentences are combined together into a single dataframe with the code presented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_articles</th>\n",
       "      <th>stemmed_stopword_articles</th>\n",
       "      <th>tokenized_articles</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "      <th>tokenized_cleaned_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125923</th>\n",
       "      <td>fact check the five pillar of curriculum fact ...</td>\n",
       "      <td>fact check five pillar curriculum fact check s...</td>\n",
       "      <td>[fact, check, five, pillar, curriculum, fact, ...</td>\n",
       "      <td>[fact check five pillar curriculum fact check ...</td>\n",
       "      <td>[[fact, check, five, pillar, curriculum, fact,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57270</th>\n",
       "      <td>the state depart spent 52701 on the curtain in...</td>\n",
       "      <td>state depart spent 52701 curtain un ambassador...</td>\n",
       "      <td>[state, depart, spent, 52701, curtain, un, amb...</td>\n",
       "      <td>[state depart spent 52701 curtain un ambassado...</td>\n",
       "      <td>[[state, depart, spent, 52701, curtain, un, am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144732</th>\n",
       "      <td>us show of forc send russia a messag in black ...</td>\n",
       "      <td>us show forc send russia messag black sea wash...</td>\n",
       "      <td>[us, show, forc, send, russia, messag, black, ...</td>\n",
       "      <td>[us show forc send russia messag black sea was...</td>\n",
       "      <td>[[us, show, forc, send, russia, messag, black,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28766</th>\n",
       "      <td>mexico miss student protest clash with polic m...</td>\n",
       "      <td>mexico miss student protest clash polic media ...</td>\n",
       "      <td>[mexico, miss, student, protest, clash, polic,...</td>\n",
       "      <td>[mexico miss student protest clash polic media...</td>\n",
       "      <td>[[mexico, miss, student, protest, clash, polic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85305</th>\n",
       "      <td>the us refuge resettl program a primer for pol...</td>\n",
       "      <td>us refuge resettl program primer policymak pre...</td>\n",
       "      <td>[us, refuge, resettl, program, primer, policym...</td>\n",
       "      <td>[us refuge resettl program primer policymak pr...</td>\n",
       "      <td>[[us, refuge, resettl, program, primer, policy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         stemmed_articles  \\\n",
       "125923  fact check the five pillar of curriculum fact ...   \n",
       "57270   the state depart spent 52701 on the curtain in...   \n",
       "144732  us show of forc send russia a messag in black ...   \n",
       "28766   mexico miss student protest clash with polic m...   \n",
       "85305   the us refuge resettl program a primer for pol...   \n",
       "\n",
       "                                stemmed_stopword_articles  \\\n",
       "125923  fact check five pillar curriculum fact check s...   \n",
       "57270   state depart spent 52701 curtain un ambassador...   \n",
       "144732  us show forc send russia messag black sea wash...   \n",
       "28766   mexico miss student protest clash polic media ...   \n",
       "85305   us refuge resettl program primer policymak pre...   \n",
       "\n",
       "                                       tokenized_articles  \\\n",
       "125923  [fact, check, five, pillar, curriculum, fact, ...   \n",
       "57270   [state, depart, spent, 52701, curtain, un, amb...   \n",
       "144732  [us, show, forc, send, russia, messag, black, ...   \n",
       "28766   [mexico, miss, student, protest, clash, polic,...   \n",
       "85305   [us, refuge, resettl, program, primer, policym...   \n",
       "\n",
       "                                         cleaned_sentence  \\\n",
       "125923  [fact check five pillar curriculum fact check ...   \n",
       "57270   [state depart spent 52701 curtain un ambassado...   \n",
       "144732  [us show forc send russia messag black sea was...   \n",
       "28766   [mexico miss student protest clash polic media...   \n",
       "85305   [us refuge resettl program primer policymak pr...   \n",
       "\n",
       "                               tokenized_cleaned_sentence  \n",
       "125923  [[fact, check, five, pillar, curriculum, fact,...  \n",
       "57270   [[state, depart, spent, 52701, curtain, un, am...  \n",
       "144732  [[us, show, forc, send, russia, messag, black,...  \n",
       "28766   [[mexico, miss, student, protest, clash, polic...  \n",
       "85305   [[us, refuge, resettl, program, primer, policy...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zip together all the articles and create a dataframe\n",
    "zipped_articles = list(zip(stemmed_art, stemmed_sw_art, tokenized_articles, final_article_sentences, final_article_tok_sentences))\n",
    "articles_ = pd.DataFrame(zipped_articles, columns = ['stemmed_articles', 'stemmed_stopword_articles', 'tokenized_articles', 'cleaned_sentence', 'tokenized_cleaned_sentence'])\n",
    "\n",
    "# index the articles based on article ID\n",
    "articles_.index = [articles_cleaned.index]\n",
    "\n",
    "# display the articles_\n",
    "articles_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Date Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of data manipulation, the date is converted to \"datetime\" format and added as a new column \"new_date\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>related_articles</th>\n",
       "      <th>article_array</th>\n",
       "      <th>new_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A line from George Orwell's novel 1984 predict...</td>\n",
       "      <td></td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "      <td>2017-07-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maine legislature candidate Leslie Gibson insu...</td>\n",
       "      <td></td>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "      <td>2018-03-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A 17-year-old girl named Alyssa Carson is bein...</td>\n",
       "      <td></td>\n",
       "      <td>2018-07-18</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "      <td>2018-07-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In 1988 author Roald Dahl penned an open lette...</td>\n",
       "      <td></td>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "      <td>2019-02-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When it comes to fighting terrorism, \"Another ...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>2016-03-22</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "      <td>2016-03-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim         claimant  \\\n",
       "0  A line from George Orwell's novel 1984 predict...                    \n",
       "1  Maine legislature candidate Leslie Gibson insu...                    \n",
       "2  A 17-year-old girl named Alyssa Carson is bein...                    \n",
       "3  In 1988 author Roald Dahl penned an open lette...                    \n",
       "4  When it comes to fighting terrorism, \"Another ...  Hillary Clinton   \n",
       "\n",
       "        date  id  label                            related_articles  \\\n",
       "0 2017-07-17   0      0            [122094, 122580, 130685, 134765]   \n",
       "1 2018-03-17   1      2                    [106868, 127320, 128060]   \n",
       "2 2018-07-18   4      1                    [132130, 132132, 149722]   \n",
       "3 2019-02-04   5      2                    [123254, 123418, 127464]   \n",
       "4 2016-03-22   6      2  [41099, 89899, 72543, 82644, 95344, 88361]   \n",
       "\n",
       "                                article_array   new_date  \n",
       "0            [122094, 122580, 130685, 134765] 2017-07-17  \n",
       "1                    [106868, 127320, 128060] 2018-03-17  \n",
       "2                    [132130, 132132, 149722] 2018-07-18  \n",
       "3                    [123254, 123418, 127464] 2019-02-04  \n",
       "4  [41099, 89899, 72543, 82644, 95344, 88361] 2016-03-22  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert date column to datetime format\n",
    "data['new_date'] = pd.to_datetime(data['date'], dayfirst=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Claimant Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All empty claimant cells are replaced with \"Unknown\" in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>related_articles</th>\n",
       "      <th>article_array</th>\n",
       "      <th>new_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A line from George Orwell's novel 1984 predict...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "      <td>2017-07-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maine legislature candidate Leslie Gibson insu...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "      <td>2018-03-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A 17-year-old girl named Alyssa Carson is bein...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2018-07-18</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "      <td>2018-07-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In 1988 author Roald Dahl penned an open lette...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "      <td>2019-02-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When it comes to fighting terrorism, \"Another ...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>2016-03-22</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "      <td>2016-03-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim         claimant  \\\n",
       "0  A line from George Orwell's novel 1984 predict...          Unknown   \n",
       "1  Maine legislature candidate Leslie Gibson insu...          Unknown   \n",
       "2  A 17-year-old girl named Alyssa Carson is bein...          Unknown   \n",
       "3  In 1988 author Roald Dahl penned an open lette...          Unknown   \n",
       "4  When it comes to fighting terrorism, \"Another ...  Hillary Clinton   \n",
       "\n",
       "        date  id  label                            related_articles  \\\n",
       "0 2017-07-17   0      0            [122094, 122580, 130685, 134765]   \n",
       "1 2018-03-17   1      2                    [106868, 127320, 128060]   \n",
       "2 2018-07-18   4      1                    [132130, 132132, 149722]   \n",
       "3 2019-02-04   5      2                    [123254, 123418, 127464]   \n",
       "4 2016-03-22   6      2  [41099, 89899, 72543, 82644, 95344, 88361]   \n",
       "\n",
       "                                article_array   new_date  \n",
       "0            [122094, 122580, 130685, 134765] 2017-07-17  \n",
       "1                    [106868, 127320, 128060] 2018-03-17  \n",
       "2                    [132130, 132132, 149722] 2018-07-18  \n",
       "3                    [123254, 123418, 127464] 2019-02-04  \n",
       "4  [41099, 89899, 72543, 82644, 95344, 88361] 2016-03-22  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill missing claimants with \"unknown\"\n",
    "data['claimant'] = data['claimant'].fillna('Unknown')\n",
    "data['claimant'] = data['claimant'].replace('', 'Unknown')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Finalize Train and Related Articles Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the train.json and related articles cleaned, this section forms two complete dataframes that hold the necessary information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.1 Finalize Cleaned Train Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section finalizes the cleaned train data which include the raw claim, cleaned claim, stemmed claims, stemmed claims without stopwords, tokenized claims, claimants, label, related article array and the dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all work done above to a single frame\n",
    "final_data = pd.concat([data.claim, cleaned_claim.claim, claims_.stemmed_claims, claims_.stemmed_stopword_claims, claims_.tokenized_claims, data.claimant, cleaned_claim.label, cleaned_claim.article_array, data.new_date], axis=1)\n",
    "\n",
    "# rename columns for clarity\n",
    "final_data.columns = ['raw_claim', 'cleaned_claim', 'stemmed_claims', 'stemmed_stopword_claims', 'tokenized_claim', 'claimant', 'label', 'article_array', 'new_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_claim</th>\n",
       "      <th>cleaned_claim</th>\n",
       "      <th>stemmed_claims</th>\n",
       "      <th>stemmed_stopword_claims</th>\n",
       "      <th>tokenized_claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>label</th>\n",
       "      <th>article_array</th>\n",
       "      <th>new_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A line from George Orwell's novel 1984 predict...</td>\n",
       "      <td>a line from george orwells novel 1984 predicts...</td>\n",
       "      <td>a line from georg orwel novel 1984 predict the...</td>\n",
       "      <td>line georg orwel novel 1984 predict power smar...</td>\n",
       "      <td>[line, georg, orwel, novel, 1984, predict, pow...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "      <td>2017-07-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maine legislature candidate Leslie Gibson insu...</td>\n",
       "      <td>maine legislature candidate leslie gibson insu...</td>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>[main, legislatur, candid, lesli, gibson, insu...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "      <td>2018-03-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A 17-year-old girl named Alyssa Carson is bein...</td>\n",
       "      <td>a 17yearold girl named alyssa carson is being ...</td>\n",
       "      <td>a 17yearold girl name alyssa carson is be trai...</td>\n",
       "      <td>17yearold girl name alyssa carson train nasa b...</td>\n",
       "      <td>[17yearold, girl, name, alyssa, carson, train,...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "      <td>2018-07-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In 1988 author Roald Dahl penned an open lette...</td>\n",
       "      <td>in 1988 author roald dahl penned an open lette...</td>\n",
       "      <td>in 1988 author roald dahl pen an open letter u...</td>\n",
       "      <td>1988 author roald dahl pen open letter urg par...</td>\n",
       "      <td>[1988, author, roald, dahl, pen, open, letter,...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "      <td>2019-02-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When it comes to fighting terrorism, \"Another ...</td>\n",
       "      <td>when it comes to fighting terrorism another th...</td>\n",
       "      <td>when it come to fight terror anoth thing we kn...</td>\n",
       "      <td>come fight terror anoth thing know doe work ba...</td>\n",
       "      <td>[come, fight, terror, anoth, thing, know, doe,...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>2</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "      <td>2016-03-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           raw_claim  \\\n",
       "0  A line from George Orwell's novel 1984 predict...   \n",
       "1  Maine legislature candidate Leslie Gibson insu...   \n",
       "2  A 17-year-old girl named Alyssa Carson is bein...   \n",
       "3  In 1988 author Roald Dahl penned an open lette...   \n",
       "4  When it comes to fighting terrorism, \"Another ...   \n",
       "\n",
       "                                       cleaned_claim  \\\n",
       "0  a line from george orwells novel 1984 predicts...   \n",
       "1  maine legislature candidate leslie gibson insu...   \n",
       "2  a 17yearold girl named alyssa carson is being ...   \n",
       "3  in 1988 author roald dahl penned an open lette...   \n",
       "4  when it comes to fighting terrorism another th...   \n",
       "\n",
       "                                      stemmed_claims  \\\n",
       "0  a line from georg orwel novel 1984 predict the...   \n",
       "1  main legislatur candid lesli gibson insult par...   \n",
       "2  a 17yearold girl name alyssa carson is be trai...   \n",
       "3  in 1988 author roald dahl pen an open letter u...   \n",
       "4  when it come to fight terror anoth thing we kn...   \n",
       "\n",
       "                             stemmed_stopword_claims  \\\n",
       "0  line georg orwel novel 1984 predict power smar...   \n",
       "1  main legislatur candid lesli gibson insult par...   \n",
       "2  17yearold girl name alyssa carson train nasa b...   \n",
       "3  1988 author roald dahl pen open letter urg par...   \n",
       "4  come fight terror anoth thing know doe work ba...   \n",
       "\n",
       "                                     tokenized_claim         claimant  label  \\\n",
       "0  [line, georg, orwel, novel, 1984, predict, pow...          Unknown      0   \n",
       "1  [main, legislatur, candid, lesli, gibson, insu...          Unknown      2   \n",
       "2  [17yearold, girl, name, alyssa, carson, train,...          Unknown      1   \n",
       "3  [1988, author, roald, dahl, pen, open, letter,...          Unknown      2   \n",
       "4  [come, fight, terror, anoth, thing, know, doe,...  Hillary Clinton      2   \n",
       "\n",
       "                                article_array   new_date  \n",
       "0            [122094, 122580, 130685, 134765] 2017-07-17  \n",
       "1                    [106868, 127320, 128060] 2018-03-17  \n",
       "2                    [132130, 132132, 149722] 2018-07-18  \n",
       "3                    [123254, 123418, 127464] 2019-02-04  \n",
       "4  [41099, 89899, 72543, 82644, 95344, 88361] 2016-03-22  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is equivalent to the \"train.csv\" that we were given, but cleaned\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save to pkl\n",
    "# final_data.to_pickle(\"./final_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.2 Finalize Cleaned Related Articles Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part combines the necessary related article information in to a single dataframe. The columns contains the raw articles, cleaned articles, stemmed articles, stemmed articles without stopwords, tokenized articles, the cleaned sentences and lastly the tokenized cleaned sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_article_list = articles.Article.tolist()\n",
    "cleaned_article_list = articles_cleaned.Article.tolist()\n",
    "final_articles_zipped = list(zip(raw_article_list, cleaned_article_list, stemmed_art, stemmed_sw_art, tokenized_articles, final_article_sentences, final_article_tok_sentences))\n",
    "final_articles = pd.DataFrame(final_articles_zipped, columns = ['raw_articles', 'cleaned_articles', 'stemmed_articles', 'stemmed_stopwords_articles', 'tokenized_articles', 'cleaned_sentence', 'tokenized_cleaned_sentence'])\n",
    "final_articles.index = [articles_cleaned.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_articles</th>\n",
       "      <th>cleaned_articles</th>\n",
       "      <th>stemmed_articles</th>\n",
       "      <th>stemmed_stopwords_articles</th>\n",
       "      <th>tokenized_articles</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "      <th>tokenized_cleaned_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125923</th>\n",
       "      <td>FACT CHECK: The Five Pillars of Curriculum\\nFA...</td>\n",
       "      <td>fact check the five pillars of curriculum fact...</td>\n",
       "      <td>fact check the five pillar of curriculum fact ...</td>\n",
       "      <td>fact check five pillar curriculum fact check s...</td>\n",
       "      <td>[fact, check, five, pillar, curriculum, fact, ...</td>\n",
       "      <td>[fact check five pillar curriculum fact check ...</td>\n",
       "      <td>[[fact, check, five, pillar, curriculum, fact,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57270</th>\n",
       "      <td>The State Department Spent $52,701 on the Curt...</td>\n",
       "      <td>the state department spent 52701 on the curtai...</td>\n",
       "      <td>the state depart spent 52701 on the curtain in...</td>\n",
       "      <td>state depart spent 52701 curtain un ambassador...</td>\n",
       "      <td>[state, depart, spent, 52701, curtain, un, amb...</td>\n",
       "      <td>[state depart spent 52701 curtain un ambassado...</td>\n",
       "      <td>[[state, depart, spent, 52701, curtain, un, am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144732</th>\n",
       "      <td>US show of force sends Russia a message in Bla...</td>\n",
       "      <td>us show of force sends russia a message in bla...</td>\n",
       "      <td>us show of forc send russia a messag in black ...</td>\n",
       "      <td>us show forc send russia messag black sea wash...</td>\n",
       "      <td>[us, show, forc, send, russia, messag, black, ...</td>\n",
       "      <td>[us show forc send russia messag black sea was...</td>\n",
       "      <td>[[us, show, forc, send, russia, messag, black,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28766</th>\n",
       "      <td>Mexico missing students: Protesters clash with...</td>\n",
       "      <td>mexico missing students protesters clash with ...</td>\n",
       "      <td>mexico miss student protest clash with polic m...</td>\n",
       "      <td>mexico miss student protest clash polic media ...</td>\n",
       "      <td>[mexico, miss, student, protest, clash, polic,...</td>\n",
       "      <td>[mexico miss student protest clash polic media...</td>\n",
       "      <td>[[mexico, miss, student, protest, clash, polic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85305</th>\n",
       "      <td>The U.S. Refugee Resettlement Program: A Prime...</td>\n",
       "      <td>the us refugee resettlement program a primer f...</td>\n",
       "      <td>the us refuge resettl program a primer for pol...</td>\n",
       "      <td>us refuge resettl program primer policymak pre...</td>\n",
       "      <td>[us, refuge, resettl, program, primer, policym...</td>\n",
       "      <td>[us refuge resettl program primer policymak pr...</td>\n",
       "      <td>[[us, refuge, resettl, program, primer, policy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             raw_articles  \\\n",
       "125923  FACT CHECK: The Five Pillars of Curriculum\\nFA...   \n",
       "57270   The State Department Spent $52,701 on the Curt...   \n",
       "144732  US show of force sends Russia a message in Bla...   \n",
       "28766   Mexico missing students: Protesters clash with...   \n",
       "85305   The U.S. Refugee Resettlement Program: A Prime...   \n",
       "\n",
       "                                         cleaned_articles  \\\n",
       "125923  fact check the five pillars of curriculum fact...   \n",
       "57270   the state department spent 52701 on the curtai...   \n",
       "144732  us show of force sends russia a message in bla...   \n",
       "28766   mexico missing students protesters clash with ...   \n",
       "85305   the us refugee resettlement program a primer f...   \n",
       "\n",
       "                                         stemmed_articles  \\\n",
       "125923  fact check the five pillar of curriculum fact ...   \n",
       "57270   the state depart spent 52701 on the curtain in...   \n",
       "144732  us show of forc send russia a messag in black ...   \n",
       "28766   mexico miss student protest clash with polic m...   \n",
       "85305   the us refuge resettl program a primer for pol...   \n",
       "\n",
       "                               stemmed_stopwords_articles  \\\n",
       "125923  fact check five pillar curriculum fact check s...   \n",
       "57270   state depart spent 52701 curtain un ambassador...   \n",
       "144732  us show forc send russia messag black sea wash...   \n",
       "28766   mexico miss student protest clash polic media ...   \n",
       "85305   us refuge resettl program primer policymak pre...   \n",
       "\n",
       "                                       tokenized_articles  \\\n",
       "125923  [fact, check, five, pillar, curriculum, fact, ...   \n",
       "57270   [state, depart, spent, 52701, curtain, un, amb...   \n",
       "144732  [us, show, forc, send, russia, messag, black, ...   \n",
       "28766   [mexico, miss, student, protest, clash, polic,...   \n",
       "85305   [us, refuge, resettl, program, primer, policym...   \n",
       "\n",
       "                                         cleaned_sentence  \\\n",
       "125923  [fact check five pillar curriculum fact check ...   \n",
       "57270   [state depart spent 52701 curtain un ambassado...   \n",
       "144732  [us show forc send russia messag black sea was...   \n",
       "28766   [mexico miss student protest clash polic media...   \n",
       "85305   [us refuge resettl program primer policymak pr...   \n",
       "\n",
       "                               tokenized_cleaned_sentence  \n",
       "125923  [[fact, check, five, pillar, curriculum, fact,...  \n",
       "57270   [[state, depart, spent, 52701, curtain, un, am...  \n",
       "144732  [[us, show, forc, send, russia, messag, black,...  \n",
       "28766   [[mexico, miss, student, protest, clash, polic...  \n",
       "85305   [[us, refuge, resettl, program, primer, policy...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_articles.to_pickle('./final_articles.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECKPOINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At this point, there are 2 dataframes as shown above: final_data and final_articles**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<-- insert brief description of the purpose of this section -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Date Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is focused on creating features from the date information provided.\n",
    "\n",
    "<-- include a list of features added and their description -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_data = pd.read_pickle('./final_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new feature with consecutive days since January 1st, 1986\n",
    "final_data['start_date'] = pd.to_datetime('1986-01-01', format='%Y-%m-%d')\n",
    "final_data['cont_days'] = (final_data['new_date'] - final_data['start_date']).dt.days\n",
    "final_data = final_data.drop(['start_date'], axis=1)\n",
    "\n",
    "# Convert Year and Month features in to int (instead of str before), can be kept as int since it is ordinal\n",
    "\n",
    "#Year\n",
    "final_data['Year'] = final_data['new_date'].apply(lambda x: \"%d\" % (x.year))\n",
    "final_data['Year'] = final_data['Year'].astype(int)\n",
    "\n",
    "# Month\n",
    "final_data['Month'] = final_data['new_date'].apply(lambda x: \"%d\" % (x.month))\n",
    "final_data['Month'] = final_data['Month'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_claim</th>\n",
       "      <th>cleaned_claim</th>\n",
       "      <th>stemmed_claims</th>\n",
       "      <th>stemmed_stopword_claims</th>\n",
       "      <th>tokenized_claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>label</th>\n",
       "      <th>article_array</th>\n",
       "      <th>new_date</th>\n",
       "      <th>cont_days</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A line from George Orwell's novel 1984 predict...</td>\n",
       "      <td>a line from george orwells novel 1984 predicts...</td>\n",
       "      <td>a line from georg orwel novel 1984 predict the...</td>\n",
       "      <td>line georg orwel novel 1984 predict power smar...</td>\n",
       "      <td>[line, georg, orwel, novel, 1984, predict, pow...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>11520</td>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maine legislature candidate Leslie Gibson insu...</td>\n",
       "      <td>maine legislature candidate leslie gibson insu...</td>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>[main, legislatur, candid, lesli, gibson, insu...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>11763</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A 17-year-old girl named Alyssa Carson is bein...</td>\n",
       "      <td>a 17yearold girl named alyssa carson is being ...</td>\n",
       "      <td>a 17yearold girl name alyssa carson is be trai...</td>\n",
       "      <td>17yearold girl name alyssa carson train nasa b...</td>\n",
       "      <td>[17yearold, girl, name, alyssa, carson, train,...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "      <td>2018-07-18</td>\n",
       "      <td>11886</td>\n",
       "      <td>2018</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In 1988 author Roald Dahl penned an open lette...</td>\n",
       "      <td>in 1988 author roald dahl penned an open lette...</td>\n",
       "      <td>in 1988 author roald dahl pen an open letter u...</td>\n",
       "      <td>1988 author roald dahl pen open letter urg par...</td>\n",
       "      <td>[1988, author, roald, dahl, pen, open, letter,...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>12087</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When it comes to fighting terrorism, \"Another ...</td>\n",
       "      <td>when it comes to fighting terrorism another th...</td>\n",
       "      <td>when it come to fight terror anoth thing we kn...</td>\n",
       "      <td>come fight terror anoth thing know doe work ba...</td>\n",
       "      <td>[come, fight, terror, anoth, thing, know, doe,...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>2</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "      <td>2016-03-22</td>\n",
       "      <td>11038</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           raw_claim  \\\n",
       "0  A line from George Orwell's novel 1984 predict...   \n",
       "1  Maine legislature candidate Leslie Gibson insu...   \n",
       "2  A 17-year-old girl named Alyssa Carson is bein...   \n",
       "3  In 1988 author Roald Dahl penned an open lette...   \n",
       "4  When it comes to fighting terrorism, \"Another ...   \n",
       "\n",
       "                                       cleaned_claim  \\\n",
       "0  a line from george orwells novel 1984 predicts...   \n",
       "1  maine legislature candidate leslie gibson insu...   \n",
       "2  a 17yearold girl named alyssa carson is being ...   \n",
       "3  in 1988 author roald dahl penned an open lette...   \n",
       "4  when it comes to fighting terrorism another th...   \n",
       "\n",
       "                                      stemmed_claims  \\\n",
       "0  a line from georg orwel novel 1984 predict the...   \n",
       "1  main legislatur candid lesli gibson insult par...   \n",
       "2  a 17yearold girl name alyssa carson is be trai...   \n",
       "3  in 1988 author roald dahl pen an open letter u...   \n",
       "4  when it come to fight terror anoth thing we kn...   \n",
       "\n",
       "                             stemmed_stopword_claims  \\\n",
       "0  line georg orwel novel 1984 predict power smar...   \n",
       "1  main legislatur candid lesli gibson insult par...   \n",
       "2  17yearold girl name alyssa carson train nasa b...   \n",
       "3  1988 author roald dahl pen open letter urg par...   \n",
       "4  come fight terror anoth thing know doe work ba...   \n",
       "\n",
       "                                     tokenized_claim         claimant  label  \\\n",
       "0  [line, georg, orwel, novel, 1984, predict, pow...          Unknown      0   \n",
       "1  [main, legislatur, candid, lesli, gibson, insu...          Unknown      2   \n",
       "2  [17yearold, girl, name, alyssa, carson, train,...          Unknown      1   \n",
       "3  [1988, author, roald, dahl, pen, open, letter,...          Unknown      2   \n",
       "4  [come, fight, terror, anoth, thing, know, doe,...  Hillary Clinton      2   \n",
       "\n",
       "                                article_array   new_date  cont_days  Year  \\\n",
       "0            [122094, 122580, 130685, 134765] 2017-07-17      11520  2017   \n",
       "1                    [106868, 127320, 128060] 2018-03-17      11763  2018   \n",
       "2                    [132130, 132132, 149722] 2018-07-18      11886  2018   \n",
       "3                    [123254, 123418, 127464] 2019-02-04      12087  2019   \n",
       "4  [41099, 89899, 72543, 82644, 95344, 88361] 2016-03-22      11038  2016   \n",
       "\n",
       "   Month  \n",
       "0      7  \n",
       "1      3  \n",
       "2      7  \n",
       "3      2  \n",
       "4      3  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_data.to_pickle('./final_data_dates.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<-- Jacob's code should be inserted here working off of final_data_dates.pkl -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Claimant Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<-- Jacob's code should be inserted here working off of final_Dat_dates.pkl ==>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Doc2Vec Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, Doc2Vec is implemented to generate claim and related article sentence vectors. Cosine similarity is used to find the top 5 related sentences from each related article per claim, and those are added as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read the data\n",
    "# data = pd.read_pickle(\"./final_data_dates.pkl\")\n",
    "# articles = pd.read_pickle('./final_articles.pkl')\n",
    "data = final_data\n",
    "articles = final_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Preprocess the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section preprocesses the claims and related articles data to prepare for Doc2Vec training. To start, a label is generated for every sentence of every related article. For example \"125923-5\" represents the 5th sentence of article ID 125923."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from scipy import spatial\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 2 minutes to run\n",
    "# create a label for every sentence based on article ID (eg. 12345-1, 12345-2, 12345-3 ... etc.)\n",
    "\n",
    "full_sentences_ID = []\n",
    "for i in range(articles.shape[0]):\n",
    "    sentence_ID_list = []\n",
    "    sentence_number = 0\n",
    "    sentences = articles.tokenized_cleaned_sentence.loc[articles.index[i]]\n",
    "    for u in range(len(sentences)):\n",
    "        sentence_ID = str(articles.index[i][0]) +  '-' + str(sentence_number)\n",
    "        sentence_number += 1\n",
    "        sentence_ID_list.append(sentence_ID)\n",
    "    full_sentences_ID.append(sentence_ID_list)\n",
    "    \n",
    "    # print progress\n",
    "    progress = round((i/articles.shape[0])*100,2)\n",
    "    clear_output(wait=True)\n",
    "    print(\"progress: \" + str(progress) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_cleaned_sentence</th>\n",
       "      <th>sentence_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125923</th>\n",
       "      <td>[[fact, check, five, pillar, curriculum, fact,...</td>\n",
       "      <td>[125923-0, 125923-1, 125923-2, 125923-3, 12592...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57270</th>\n",
       "      <td>[[state, depart, spent, 52701, curtain, un, am...</td>\n",
       "      <td>[57270-0, 57270-1, 57270-2, 57270-3, 57270-4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144732</th>\n",
       "      <td>[[us, show, forc, send, russia, messag, black,...</td>\n",
       "      <td>[144732-0, 144732-1, 144732-2, 144732-3, 14473...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28766</th>\n",
       "      <td>[[mexico, miss, student, protest, clash, polic...</td>\n",
       "      <td>[28766-0, 28766-1, 28766-2, 28766-3, 28766-4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85305</th>\n",
       "      <td>[[us, refuge, resettl, program, primer, policy...</td>\n",
       "      <td>[85305-0, 85305-1, 85305-2, 85305-3, 85305-4, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               tokenized_cleaned_sentence  \\\n",
       "125923  [[fact, check, five, pillar, curriculum, fact,...   \n",
       "57270   [[state, depart, spent, 52701, curtain, un, am...   \n",
       "144732  [[us, show, forc, send, russia, messag, black,...   \n",
       "28766   [[mexico, miss, student, protest, clash, polic...   \n",
       "85305   [[us, refuge, resettl, program, primer, policy...   \n",
       "\n",
       "                                              sentence_ID  \n",
       "125923  [125923-0, 125923-1, 125923-2, 125923-3, 12592...  \n",
       "57270   [57270-0, 57270-1, 57270-2, 57270-3, 57270-4, ...  \n",
       "144732  [144732-0, 144732-1, 144732-2, 144732-3, 14473...  \n",
       "28766   [28766-0, 28766-1, 28766-2, 28766-3, 28766-4, ...  \n",
       "85305   [85305-0, 85305-1, 85305-2, 85305-3, 85305-4, ...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the tokenized sentences with their ID\n",
    "articles_tok_sent = articles.tokenized_cleaned_sentence.to_frame()\n",
    "articles_tok_sent['sentence_ID'] = full_sentences_ID\n",
    "articles_tok_sent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # combine the sentence list and index list ~ takes 1.5 hours to run\n",
    "\n",
    "# article_sent_list = []\n",
    "# article_sent_ID = []\n",
    "\n",
    "# for i in range(articles_tok_sent.shape[0]):\n",
    "#     one_article_sent = articles_tok_sent.tokenized_cleaned_sentence.loc[articles.index[i]]\n",
    "#     article_sent_list = article_sent_list + one_article_sent\n",
    "#     one_article_ID = articles_tok_sent.sentence_ID.loc[articles.index[i]]\n",
    "#     article_sent_ID = article_sent_ID + one_article_ID\n",
    "    \n",
    "#     # print progress\n",
    "#     progress = round((i/articles_tok_sent.shape[0])*100,2)\n",
    "#     clear_output(wait=True)\n",
    "#     print(\"progress: \" + str(progress) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a data frame of the independent sentences and ID\n",
    "\n",
    "# # create copies\n",
    "# l1 = article_sent_list\n",
    "# l2 = article_sent_ID\n",
    "# # make dataframe\n",
    "# article_information = pd.Series(l2).to_frame()\n",
    "# article_information.columns = ['sentence_ID']\n",
    "# article_information['sentences'] = l1\n",
    "# # display it\n",
    "# article_information.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save it to pickle\n",
    "# article_information.to_pickle(\"./article_sentences_ind.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every sentence of every related article along with the respective ID's are added to the following dataframe for Doc2Vec training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_ID</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125385-0</td>\n",
       "      <td>[dr, ben, carson, welfar, benefactor]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125385-1</td>\n",
       "      <td>[initi, unlik, clearli, formid, contend, 2016,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>125385-2</td>\n",
       "      <td>[unlik, chri, christi, rand, paul, mike, hucka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>125385-3</td>\n",
       "      <td>[carson, becam, somewhat, overnight, sensat, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125385-4</td>\n",
       "      <td>[earliest, version, meme, date, least, decemb,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_ID                                          sentences\n",
       "0    125385-0              [dr, ben, carson, welfar, benefactor]\n",
       "1    125385-1  [initi, unlik, clearli, formid, contend, 2016,...\n",
       "2    125385-2  [unlik, chri, christi, rand, paul, mike, hucka...\n",
       "3    125385-3  [carson, becam, somewhat, overnight, sensat, f...\n",
       "4    125385-4  [earliest, version, meme, date, least, decemb,..."
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_pkl = pd.read_pickle(\"./article_sentences_ind.pkl\")\n",
    "# sentences_pkl = article_information\n",
    "sentences_pkl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the lists from the dataframes\n",
    "sentence_ID_list = sentences_pkl.sentence_ID.tolist()\n",
    "sentence_list = sentences_pkl.sentences.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Prepare for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The related article sentences along with the claims are added together with their ID's in this section for Doc2Vec training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 450 ms, sys: 4 µs, total: 450 ms\n",
      "Wall time: 449 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "    \n",
    "# create a list of claim index ranging from 0 - 15554\n",
    "claim_range = range(0, 15555)\n",
    "claim_index = []\n",
    "for i in claim_range:\n",
    "    claim_index.append(i)\n",
    "\n",
    "# create a list of lists of data.tokenized_claim\n",
    "tokenized_claims = []\n",
    "for i in range(data.shape[0]):\n",
    "    tokenized_claims.append(data.tokenized_claim[i])\n",
    "    \n",
    "# convert claim index to string to match the sentences ID\n",
    "claim_str_index = list(map(str, claim_index))\n",
    "\n",
    "# form full list for training\n",
    "full_text_list = sentence_list + tokenized_claims\n",
    "full_ID_list = sentence_ID_list + claim_str_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine above 2 lists in to a dictionary for debugging\n",
    "complete_data = dict(zip(full_ID_list, full_text_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Doc2Vec Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to enable Doc2Vec training is implemented below. Some key parameters are a vector size of 150 and 30 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# #create tagged data to train doc2vec w claim\n",
    "# tagged_data = [TaggedDocument(words=full_text_list[i], tags=[full_ID_list[i]]) for i in range(len(full_text_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # 6 hours to run\n",
    "\n",
    "# import multiprocessing\n",
    "# n_cpu = multiprocessing.cpu_count()\n",
    "\n",
    "# # setup training\n",
    "# vec_size = 150\n",
    "# model = Doc2Vec(dm = 1, vector_size = vec_size, min_count = 2, workers = n_cpu, epochs=30)\n",
    "\n",
    "# # build vocab\n",
    "# model.build_vocab(tagged_data)\n",
    "\n",
    "# # train\n",
    "# model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# # save model\n",
    "# model.save(\"30epoch_150vec.model\")\n",
    "# print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 Find Similarities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 s, sys: 2.51 s, total: 16.5 s\n",
      "Wall time: 19.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load the model\n",
    "model = Doc2Vec.load(\"30epoch_150vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4.1 Finding Top 5 Sentences per Related Articles per Claim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the top 5 sentences for each related article per claim. For example, Claim 0 has 5 related articles. Within each related article, top 5 most similar sentences are extracted. As a result, 25 total sentences are extracted from the 5 related articles. The similarity score of the sentences per related article is averaged, and then the 5 averages are averaged again to create an average similarity score between the claim and the 5 top 5 sentences of each 5 related articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 99.99%\n",
      "CPU times: user 6min 1s, sys: 7.29 s, total: 6min 8s\n",
      "Wall time: 6min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# ~ 6 minutes to run\n",
    "# find the top 5 sentence ID from each related article with the highest similarity score with the claim\n",
    "\n",
    "final_best_sentences_ID = []\n",
    "for i in range(data.shape[0]): # iterating over every claim\n",
    "    one_claim_array = data.article_array.loc[i]\n",
    "    best_sentences_one_claim = []\n",
    "    for u in range(len(one_claim_array)): # iterating every article of one claim\n",
    "        one_article_ID = one_claim_array[u]\n",
    "        sent_list = articles.tokenized_cleaned_sentence.loc[str(one_article_ID)].iloc[0]\n",
    "        one_article_sim_list=[]\n",
    "        for y in range(len(sent_list)): #iterating over every sentence of one article\n",
    "            v1 = model.docvecs[str(i)]\n",
    "            sentence_number = str(str(one_article_ID) + '-' + str(y))\n",
    "            v2 = model.docvecs[sentence_number]\n",
    "            similarity = 1 - spatial.distance.cosine(v1, v2)\n",
    "            one_article_sim_list.append(similarity)\n",
    "#             print(\"ID: \" + str(i) + \" \" + str(result))\n",
    "        a = numpy.array(one_article_sim_list)\n",
    "        best_sentences_one_article = heapq.nlargest(5, range(len(a)), a.take)   \n",
    "        best_sentences_one_claim.append(best_sentences_one_article)\n",
    "    final_best_sentences_ID.append(best_sentences_one_claim)\n",
    "    # print progress\n",
    "    progress = round((i/data.shape[0])*100,2)\n",
    "    clear_output(wait=True)\n",
    "    print(\"progress: \" + str(progress) + \"%\")\n",
    "\n",
    "# add it to the data frame\n",
    "data['best_sentences_ID'] = final_best_sentences_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 99.99%\n",
      "CPU times: user 1min 9s, sys: 2.65 s, total: 1min 11s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# ~ 2 minuts to run\n",
    "# calculate the top 5 similarity scores\n",
    "\n",
    "end = data.shape[0]\n",
    "# end = 2\n",
    "avg_sim_for_one_claim_list = []\n",
    "avg_sim_one_claim = []\n",
    "full_sentence_ID = []\n",
    "for i in range(end): # per row\n",
    "    one_claim_articles_ID = data.article_array.loc[i]\n",
    "    one_claim_sentences_ID = data.best_sentences_ID.loc[i]\n",
    "    avg_sim_for_one_article = []\n",
    "    article_sentence_ID = []\n",
    "#     print(\"claim: \" + str(i))\n",
    "    for u in range(len(one_claim_articles_ID)): # article_array index\n",
    "        sim_for_one_article = []\n",
    "        sentence_ID_list = []\n",
    "        for y in range(len(one_claim_sentences_ID[u])): # sentence ID index      \n",
    "            sentence_ID = str(one_claim_articles_ID[u]) + '-' + str(data.best_sentences_ID.loc[i][u][y])\n",
    "            sentence_ID_list.append(sentence_ID)\n",
    "            # calculate sim score\n",
    "            v1 = model.docvecs[str(i)]\n",
    "            v2 = model.docvecs[sentence_ID]\n",
    "            similarity = 1 - spatial.distance.cosine(v1, v2)\n",
    "            # similarity between claim and each sentence is calculated\n",
    "            sim_for_one_article.append(similarity)\n",
    "#             print(str(sentence_ID) + \": \" + str(similarity))\n",
    "        # the similarity between claim and each sentence per article is averaged\n",
    "        avg_sim = sum(sim_for_one_article)/len(sim_for_one_article)\n",
    "        avg_sim_for_one_article.append(avg_sim)\n",
    "#         print(\"\")\n",
    "        article_sentence_ID.append(sentence_ID_list)\n",
    "    \n",
    "    # create a list of sentence ID's\n",
    "    full_sentence_ID.append(article_sentence_ID)\n",
    "    \n",
    "    #calculate average scores for each claim\n",
    "    avg_sim_ = sum(avg_sim_for_one_article)/len(avg_sim_for_one_article)\n",
    "    avg_sim_one_claim.append(avg_sim_)\n",
    "    \n",
    "    # a list of lists, big list per claim, and small list per article\n",
    "    avg_sim_for_one_claim_list.append(avg_sim_for_one_article)\n",
    "    \n",
    "    # print progress\n",
    "    progress = round((i/end)*100,2)\n",
    "    clear_output(wait=True)\n",
    "    print(\"progress: \" + str(progress) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to data frame\n",
    "data['full_sentence_ID'] = full_sentence_ID\n",
    "data['avg_sentence_sim'] = avg_sim_for_one_claim_list\n",
    "data['avg_sim_score'] = avg_sim_one_claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_claim</th>\n",
       "      <th>cleaned_claim</th>\n",
       "      <th>stemmed_claims</th>\n",
       "      <th>stemmed_stopword_claims</th>\n",
       "      <th>tokenized_claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>label</th>\n",
       "      <th>article_array</th>\n",
       "      <th>new_date</th>\n",
       "      <th>cont_days</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>best_sentences_ID</th>\n",
       "      <th>full_sentence_ID</th>\n",
       "      <th>avg_sentence_sim</th>\n",
       "      <th>avg_sim_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A line from George Orwell's novel 1984 predict...</td>\n",
       "      <td>a line from george orwells novel 1984 predicts...</td>\n",
       "      <td>a line from georg orwel novel 1984 predict the...</td>\n",
       "      <td>line georg orwel novel 1984 predict power smar...</td>\n",
       "      <td>[line, georg, orwel, novel, 1984, predict, pow...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>11520</td>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "      <td>[[0, 21, 15, 20, 10], [11, 24, 12, 13, 26], [3...</td>\n",
       "      <td>[[122094-0, 122094-21, 122094-15, 122094-20, 1...</td>\n",
       "      <td>[0.26672267615795137, 0.24059542417526245, 0.1...</td>\n",
       "      <td>0.185311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maine legislature candidate Leslie Gibson insu...</td>\n",
       "      <td>maine legislature candidate leslie gibson insu...</td>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>[main, legislatur, candid, lesli, gibson, insu...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>11763</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>[[11, 7, 13, 3, 4], [22, 24, 0, 1, 19], [0, 8,...</td>\n",
       "      <td>[[106868-11, 106868-7, 106868-13, 106868-3, 10...</td>\n",
       "      <td>[0.382437926530838, 0.374161022901535, 0.28886...</td>\n",
       "      <td>0.348489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A 17-year-old girl named Alyssa Carson is bein...</td>\n",
       "      <td>a 17yearold girl named alyssa carson is being ...</td>\n",
       "      <td>a 17yearold girl name alyssa carson is be trai...</td>\n",
       "      <td>17yearold girl name alyssa carson train nasa b...</td>\n",
       "      <td>[17yearold, girl, name, alyssa, carson, train,...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "      <td>2018-07-18</td>\n",
       "      <td>11886</td>\n",
       "      <td>2018</td>\n",
       "      <td>7</td>\n",
       "      <td>[[22, 30, 8, 32, 29], [3, 6, 59, 60, 83], [6, ...</td>\n",
       "      <td>[[132130-22, 132130-30, 132130-8, 132130-32, 1...</td>\n",
       "      <td>[0.45887559056282046, 0.6421806216239929, 0.29...</td>\n",
       "      <td>0.464089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In 1988 author Roald Dahl penned an open lette...</td>\n",
       "      <td>in 1988 author roald dahl penned an open lette...</td>\n",
       "      <td>in 1988 author roald dahl pen an open letter u...</td>\n",
       "      <td>1988 author roald dahl pen open letter urg par...</td>\n",
       "      <td>[1988, author, roald, dahl, pen, open, letter,...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>12087</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>[[51, 41, 70, 103, 30], [107, 102, 61, 191, 98...</td>\n",
       "      <td>[[123254-51, 123254-41, 123254-70, 123254-103,...</td>\n",
       "      <td>[0.7696409344673156, 0.7453237056732178, 0.755...</td>\n",
       "      <td>0.756903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When it comes to fighting terrorism, \"Another ...</td>\n",
       "      <td>when it comes to fighting terrorism another th...</td>\n",
       "      <td>when it come to fight terror anoth thing we kn...</td>\n",
       "      <td>come fight terror anoth thing know doe work ba...</td>\n",
       "      <td>[come, fight, terror, anoth, thing, know, doe,...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>2</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "      <td>2016-03-22</td>\n",
       "      <td>11038</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>[[168, 170, 45, 191, 32], [10, 12, 4, 16, 25],...</td>\n",
       "      <td>[[41099-168, 41099-170, 41099-45, 41099-191, 4...</td>\n",
       "      <td>[0.4999494135379791, 0.408842134475708, 0.3396...</td>\n",
       "      <td>0.411560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           raw_claim  \\\n",
       "0  A line from George Orwell's novel 1984 predict...   \n",
       "1  Maine legislature candidate Leslie Gibson insu...   \n",
       "2  A 17-year-old girl named Alyssa Carson is bein...   \n",
       "3  In 1988 author Roald Dahl penned an open lette...   \n",
       "4  When it comes to fighting terrorism, \"Another ...   \n",
       "\n",
       "                                       cleaned_claim  \\\n",
       "0  a line from george orwells novel 1984 predicts...   \n",
       "1  maine legislature candidate leslie gibson insu...   \n",
       "2  a 17yearold girl named alyssa carson is being ...   \n",
       "3  in 1988 author roald dahl penned an open lette...   \n",
       "4  when it comes to fighting terrorism another th...   \n",
       "\n",
       "                                      stemmed_claims  \\\n",
       "0  a line from georg orwel novel 1984 predict the...   \n",
       "1  main legislatur candid lesli gibson insult par...   \n",
       "2  a 17yearold girl name alyssa carson is be trai...   \n",
       "3  in 1988 author roald dahl pen an open letter u...   \n",
       "4  when it come to fight terror anoth thing we kn...   \n",
       "\n",
       "                             stemmed_stopword_claims  \\\n",
       "0  line georg orwel novel 1984 predict power smar...   \n",
       "1  main legislatur candid lesli gibson insult par...   \n",
       "2  17yearold girl name alyssa carson train nasa b...   \n",
       "3  1988 author roald dahl pen open letter urg par...   \n",
       "4  come fight terror anoth thing know doe work ba...   \n",
       "\n",
       "                                     tokenized_claim         claimant  label  \\\n",
       "0  [line, georg, orwel, novel, 1984, predict, pow...          Unknown      0   \n",
       "1  [main, legislatur, candid, lesli, gibson, insu...          Unknown      2   \n",
       "2  [17yearold, girl, name, alyssa, carson, train,...          Unknown      1   \n",
       "3  [1988, author, roald, dahl, pen, open, letter,...          Unknown      2   \n",
       "4  [come, fight, terror, anoth, thing, know, doe,...  Hillary Clinton      2   \n",
       "\n",
       "                                article_array   new_date  cont_days  Year  \\\n",
       "0            [122094, 122580, 130685, 134765] 2017-07-17      11520  2017   \n",
       "1                    [106868, 127320, 128060] 2018-03-17      11763  2018   \n",
       "2                    [132130, 132132, 149722] 2018-07-18      11886  2018   \n",
       "3                    [123254, 123418, 127464] 2019-02-04      12087  2019   \n",
       "4  [41099, 89899, 72543, 82644, 95344, 88361] 2016-03-22      11038  2016   \n",
       "\n",
       "   Month                                  best_sentences_ID  \\\n",
       "0      7  [[0, 21, 15, 20, 10], [11, 24, 12, 13, 26], [3...   \n",
       "1      3  [[11, 7, 13, 3, 4], [22, 24, 0, 1, 19], [0, 8,...   \n",
       "2      7  [[22, 30, 8, 32, 29], [3, 6, 59, 60, 83], [6, ...   \n",
       "3      2  [[51, 41, 70, 103, 30], [107, 102, 61, 191, 98...   \n",
       "4      3  [[168, 170, 45, 191, 32], [10, 12, 4, 16, 25],...   \n",
       "\n",
       "                                    full_sentence_ID  \\\n",
       "0  [[122094-0, 122094-21, 122094-15, 122094-20, 1...   \n",
       "1  [[106868-11, 106868-7, 106868-13, 106868-3, 10...   \n",
       "2  [[132130-22, 132130-30, 132130-8, 132130-32, 1...   \n",
       "3  [[123254-51, 123254-41, 123254-70, 123254-103,...   \n",
       "4  [[41099-168, 41099-170, 41099-45, 41099-191, 4...   \n",
       "\n",
       "                                    avg_sentence_sim  avg_sim_score  \n",
       "0  [0.26672267615795137, 0.24059542417526245, 0.1...       0.185311  \n",
       "1  [0.382437926530838, 0.374161022901535, 0.28886...       0.348489  \n",
       "2  [0.45887559056282046, 0.6421806216239929, 0.29...       0.464089  \n",
       "3  [0.7696409344673156, 0.7453237056732178, 0.755...       0.756903  \n",
       "4  [0.4999494135379791, 0.408842134475708, 0.3396...       0.411560  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4.2 Finding Top 5 Sentences Amongst all Related Articles per Claim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, instead of finding top 5 sentences from each related article, the top 5 sentences from ALL related articles of a claim is extracted. The average similarity score between the claim and the top 5 sentences are calculated and added as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 99.99%\n",
      "CPU times: user 50.7 s, sys: 3.26 s, total: 54 s\n",
      "Wall time: 51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1 min to run\n",
    "# finding top 5 sentences amongst all relateld articles per claim\n",
    "\n",
    "end = data.shape[0]\n",
    "# end = 10\n",
    "\n",
    "sim_score_dict = {}\n",
    "\n",
    "final_top_sentences = []\n",
    "\n",
    "for i in range(end):\n",
    "    top_sentence_list = data.full_sentence_ID.loc[i]\n",
    "    sim_score_tuple = []\n",
    "    for u in range(len(top_sentence_list)):\n",
    "        for y in range(len(top_sentence_list[u])):\n",
    "            sentence_ID = top_sentence_list[u][y]\n",
    "            # calculate sim score\n",
    "            v1 = model.docvecs[str(i)]\n",
    "            v2 = model.docvecs[sentence_ID]\n",
    "            similarity = 1 - spatial.distance.cosine(v1, v2)\n",
    "#             sim_for_one_claim.append(similarity)\n",
    "            sim_score_tuple.append(tuple((sentence_ID, similarity)))\n",
    "#             print(str(sentence_ID) + \": \" + str(similarity))\n",
    "    a = nlargest(5, sim_score_tuple, key=itemgetter(1))\n",
    "    top_sentences = [q[0] for q in a]\n",
    "    # list of lists: final top 5 sentences amongst all related articles per claim\n",
    "    final_top_sentences.append(top_sentences)\n",
    "    \n",
    "    # print progress\n",
    "    progress = round((i/end)*100,2)\n",
    "    clear_output(wait=True)\n",
    "    print(\"progress: \" + str(progress) + \"%\")\n",
    "\n",
    "# save it to the dataframe\n",
    "data['top_5_sentences'] = final_top_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 95.2%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# average similarity scores of top 5 sentences amongst all relateld articles per claim\n",
    "\n",
    "end = data.shape[0]\n",
    "# end = 10\n",
    "\n",
    "total_avg_sim = []\n",
    "\n",
    "for i in range(end):\n",
    "    top_sentence_list = data.top_5_sentences.loc[i]\n",
    "    top_sim_scores = []\n",
    "    for u in range(len(top_sentence_list)):\n",
    "        sentence_ID = top_sentence_list[u]\n",
    "        # calculate sim score\n",
    "        v1 = model.docvecs[str(i)]\n",
    "        v2 = model.docvecs[sentence_ID]\n",
    "        similarity = 1 - spatial.distance.cosine(v1, v2)       \n",
    "        top_sim_scores.append(similarity)\n",
    "        \n",
    "    avg_sim = sum(top_sim_scores)/len(top_sim_scores)\n",
    "    total_avg_sim.append(avg_sim)\n",
    "    \n",
    "    # print progress\n",
    "    progress = round((i/end)*100,2)\n",
    "    clear_output(wait=True)\n",
    "    print(\"progress: \" + str(progress) + \"%\")\n",
    "\n",
    "# save it to the dataframe\n",
    "data['top5_avg_sim'] = total_avg_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_claim</th>\n",
       "      <th>cleaned_claim</th>\n",
       "      <th>stemmed_claims</th>\n",
       "      <th>stemmed_stopword_claims</th>\n",
       "      <th>tokenized_claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>label</th>\n",
       "      <th>article_array</th>\n",
       "      <th>new_date</th>\n",
       "      <th>cont_days</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>best_sentences_ID</th>\n",
       "      <th>full_sentence_ID</th>\n",
       "      <th>avg_sentence_sim</th>\n",
       "      <th>avg_sim_score</th>\n",
       "      <th>top_5_sentences</th>\n",
       "      <th>top5_avg_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A line from George Orwell's novel 1984 predict...</td>\n",
       "      <td>a line from george orwells novel 1984 predicts...</td>\n",
       "      <td>a line from georg orwel novel 1984 predict the...</td>\n",
       "      <td>line georg orwel novel 1984 predict power smar...</td>\n",
       "      <td>[line, georg, orwel, novel, 1984, predict, pow...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>11520</td>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "      <td>[[0, 21, 15, 20, 10], [11, 24, 12, 13, 26], [3...</td>\n",
       "      <td>[[122094-0, 122094-21, 122094-15, 122094-20, 1...</td>\n",
       "      <td>[0.26672267615795137, 0.24059542417526245, 0.1...</td>\n",
       "      <td>0.185311</td>\n",
       "      <td>[122094-0, 122580-11, 122094-21, 122094-15, 12...</td>\n",
       "      <td>0.291527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maine legislature candidate Leslie Gibson insu...</td>\n",
       "      <td>maine legislature candidate leslie gibson insu...</td>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>[main, legislatur, candid, lesli, gibson, insu...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>11763</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>[[11, 7, 13, 3, 4], [22, 24, 0, 1, 19], [0, 8,...</td>\n",
       "      <td>[[106868-11, 106868-7, 106868-13, 106868-3, 10...</td>\n",
       "      <td>[0.382437926530838, 0.374161022901535, 0.28886...</td>\n",
       "      <td>0.348489</td>\n",
       "      <td>[106868-11, 106868-7, 127320-22, 127320-24, 12...</td>\n",
       "      <td>0.400307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A 17-year-old girl named Alyssa Carson is bein...</td>\n",
       "      <td>a 17yearold girl named alyssa carson is being ...</td>\n",
       "      <td>a 17yearold girl name alyssa carson is be trai...</td>\n",
       "      <td>17yearold girl name alyssa carson train nasa b...</td>\n",
       "      <td>[17yearold, girl, name, alyssa, carson, train,...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "      <td>2018-07-18</td>\n",
       "      <td>11886</td>\n",
       "      <td>2018</td>\n",
       "      <td>7</td>\n",
       "      <td>[[22, 30, 8, 32, 29], [3, 6, 59, 60, 83], [6, ...</td>\n",
       "      <td>[[132130-22, 132130-30, 132130-8, 132130-32, 1...</td>\n",
       "      <td>[0.45887559056282046, 0.6421806216239929, 0.29...</td>\n",
       "      <td>0.464089</td>\n",
       "      <td>[132132-3, 132132-6, 132132-59, 132132-60, 132...</td>\n",
       "      <td>0.642181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In 1988 author Roald Dahl penned an open lette...</td>\n",
       "      <td>in 1988 author roald dahl penned an open lette...</td>\n",
       "      <td>in 1988 author roald dahl pen an open letter u...</td>\n",
       "      <td>1988 author roald dahl pen open letter urg par...</td>\n",
       "      <td>[1988, author, roald, dahl, pen, open, letter,...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>12087</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>[[51, 41, 70, 103, 30], [107, 102, 61, 191, 98...</td>\n",
       "      <td>[[123254-51, 123254-41, 123254-70, 123254-103,...</td>\n",
       "      <td>[0.7696409344673156, 0.7453237056732178, 0.755...</td>\n",
       "      <td>0.756903</td>\n",
       "      <td>[127464-222, 123254-51, 123254-41, 123254-70, ...</td>\n",
       "      <td>0.775002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When it comes to fighting terrorism, \"Another ...</td>\n",
       "      <td>when it comes to fighting terrorism another th...</td>\n",
       "      <td>when it come to fight terror anoth thing we kn...</td>\n",
       "      <td>come fight terror anoth thing know doe work ba...</td>\n",
       "      <td>[come, fight, terror, anoth, thing, know, doe,...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>2</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "      <td>2016-03-22</td>\n",
       "      <td>11038</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>[[168, 170, 45, 191, 32], [10, 12, 4, 16, 25],...</td>\n",
       "      <td>[[41099-168, 41099-170, 41099-45, 41099-191, 4...</td>\n",
       "      <td>[0.4999494135379791, 0.408842134475708, 0.3396...</td>\n",
       "      <td>0.411560</td>\n",
       "      <td>[41099-168, 95344-259, 95344-247, 95344-74, 95...</td>\n",
       "      <td>0.547029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           raw_claim  \\\n",
       "0  A line from George Orwell's novel 1984 predict...   \n",
       "1  Maine legislature candidate Leslie Gibson insu...   \n",
       "2  A 17-year-old girl named Alyssa Carson is bein...   \n",
       "3  In 1988 author Roald Dahl penned an open lette...   \n",
       "4  When it comes to fighting terrorism, \"Another ...   \n",
       "\n",
       "                                       cleaned_claim  \\\n",
       "0  a line from george orwells novel 1984 predicts...   \n",
       "1  maine legislature candidate leslie gibson insu...   \n",
       "2  a 17yearold girl named alyssa carson is being ...   \n",
       "3  in 1988 author roald dahl penned an open lette...   \n",
       "4  when it comes to fighting terrorism another th...   \n",
       "\n",
       "                                      stemmed_claims  \\\n",
       "0  a line from georg orwel novel 1984 predict the...   \n",
       "1  main legislatur candid lesli gibson insult par...   \n",
       "2  a 17yearold girl name alyssa carson is be trai...   \n",
       "3  in 1988 author roald dahl pen an open letter u...   \n",
       "4  when it come to fight terror anoth thing we kn...   \n",
       "\n",
       "                             stemmed_stopword_claims  \\\n",
       "0  line georg orwel novel 1984 predict power smar...   \n",
       "1  main legislatur candid lesli gibson insult par...   \n",
       "2  17yearold girl name alyssa carson train nasa b...   \n",
       "3  1988 author roald dahl pen open letter urg par...   \n",
       "4  come fight terror anoth thing know doe work ba...   \n",
       "\n",
       "                                     tokenized_claim         claimant  label  \\\n",
       "0  [line, georg, orwel, novel, 1984, predict, pow...          Unknown      0   \n",
       "1  [main, legislatur, candid, lesli, gibson, insu...          Unknown      2   \n",
       "2  [17yearold, girl, name, alyssa, carson, train,...          Unknown      1   \n",
       "3  [1988, author, roald, dahl, pen, open, letter,...          Unknown      2   \n",
       "4  [come, fight, terror, anoth, thing, know, doe,...  Hillary Clinton      2   \n",
       "\n",
       "                                article_array   new_date  cont_days  Year  \\\n",
       "0            [122094, 122580, 130685, 134765] 2017-07-17      11520  2017   \n",
       "1                    [106868, 127320, 128060] 2018-03-17      11763  2018   \n",
       "2                    [132130, 132132, 149722] 2018-07-18      11886  2018   \n",
       "3                    [123254, 123418, 127464] 2019-02-04      12087  2019   \n",
       "4  [41099, 89899, 72543, 82644, 95344, 88361] 2016-03-22      11038  2016   \n",
       "\n",
       "   Month                                  best_sentences_ID  \\\n",
       "0      7  [[0, 21, 15, 20, 10], [11, 24, 12, 13, 26], [3...   \n",
       "1      3  [[11, 7, 13, 3, 4], [22, 24, 0, 1, 19], [0, 8,...   \n",
       "2      7  [[22, 30, 8, 32, 29], [3, 6, 59, 60, 83], [6, ...   \n",
       "3      2  [[51, 41, 70, 103, 30], [107, 102, 61, 191, 98...   \n",
       "4      3  [[168, 170, 45, 191, 32], [10, 12, 4, 16, 25],...   \n",
       "\n",
       "                                    full_sentence_ID  \\\n",
       "0  [[122094-0, 122094-21, 122094-15, 122094-20, 1...   \n",
       "1  [[106868-11, 106868-7, 106868-13, 106868-3, 10...   \n",
       "2  [[132130-22, 132130-30, 132130-8, 132130-32, 1...   \n",
       "3  [[123254-51, 123254-41, 123254-70, 123254-103,...   \n",
       "4  [[41099-168, 41099-170, 41099-45, 41099-191, 4...   \n",
       "\n",
       "                                    avg_sentence_sim  avg_sim_score  \\\n",
       "0  [0.26672267615795137, 0.24059542417526245, 0.1...       0.185311   \n",
       "1  [0.382437926530838, 0.374161022901535, 0.28886...       0.348489   \n",
       "2  [0.45887559056282046, 0.6421806216239929, 0.29...       0.464089   \n",
       "3  [0.7696409344673156, 0.7453237056732178, 0.755...       0.756903   \n",
       "4  [0.4999494135379791, 0.408842134475708, 0.3396...       0.411560   \n",
       "\n",
       "                                     top_5_sentences  top5_avg_sim  \n",
       "0  [122094-0, 122580-11, 122094-21, 122094-15, 12...      0.291527  \n",
       "1  [106868-11, 106868-7, 127320-22, 127320-24, 12...      0.400307  \n",
       "2  [132132-3, 132132-6, 132132-59, 132132-60, 132...      0.642181  \n",
       "3  [127464-222, 123254-51, 123254-41, 123254-70, ...      0.775002  \n",
       "4  [41099-168, 95344-259, 95344-247, 95344-74, 95...      0.547029  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save to data frame\n",
    "# data.to_pickle(\"./data_sentence_simscore.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5 Feature Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the claims, related article sentences and similarity scores are encoded in to numerical values for our fake news classfication model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the files\n",
    "\n",
    "# full_data = pd.read_pickle(\"./data_sentence_simscore.pkl\")\n",
    "full_data = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5.1 Feature Encode the Claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section applies the trained Doc2Vec model to generate a vector of size 150 for each claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of lists containing the claim's vector\n",
    "claim_vectors = []\n",
    "for i in range(0, 15555):\n",
    "    claim_vectors.append(model.docvecs[str(i)])\n",
    "\n",
    "# create a list of column names\n",
    "column_name_list = []\n",
    "for i in range(0,150):\n",
    "    column_name = \"claim_vec_\" + str(i)\n",
    "    column_name_list.append(column_name)\n",
    "\n",
    "# turn the claim vectors in to a dataframe and rename the columns accordingly\n",
    "claim_features = pd.DataFrame.from_records(claim_vectors)\n",
    "claim_features.columns = column_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_vec_0</th>\n",
       "      <th>claim_vec_1</th>\n",
       "      <th>claim_vec_2</th>\n",
       "      <th>claim_vec_3</th>\n",
       "      <th>claim_vec_4</th>\n",
       "      <th>claim_vec_5</th>\n",
       "      <th>claim_vec_6</th>\n",
       "      <th>claim_vec_7</th>\n",
       "      <th>claim_vec_8</th>\n",
       "      <th>claim_vec_9</th>\n",
       "      <th>...</th>\n",
       "      <th>claim_vec_140</th>\n",
       "      <th>claim_vec_141</th>\n",
       "      <th>claim_vec_142</th>\n",
       "      <th>claim_vec_143</th>\n",
       "      <th>claim_vec_144</th>\n",
       "      <th>claim_vec_145</th>\n",
       "      <th>claim_vec_146</th>\n",
       "      <th>claim_vec_147</th>\n",
       "      <th>claim_vec_148</th>\n",
       "      <th>claim_vec_149</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.212091</td>\n",
       "      <td>0.127670</td>\n",
       "      <td>0.106502</td>\n",
       "      <td>0.391641</td>\n",
       "      <td>0.303139</td>\n",
       "      <td>-0.248252</td>\n",
       "      <td>0.236494</td>\n",
       "      <td>0.104728</td>\n",
       "      <td>-0.271278</td>\n",
       "      <td>0.377465</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.283741</td>\n",
       "      <td>-0.166108</td>\n",
       "      <td>-0.125862</td>\n",
       "      <td>0.316487</td>\n",
       "      <td>-0.086704</td>\n",
       "      <td>-0.015346</td>\n",
       "      <td>0.263394</td>\n",
       "      <td>0.236137</td>\n",
       "      <td>0.191386</td>\n",
       "      <td>-0.144861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.512708</td>\n",
       "      <td>0.203220</td>\n",
       "      <td>0.151913</td>\n",
       "      <td>-0.307748</td>\n",
       "      <td>-0.426895</td>\n",
       "      <td>0.143223</td>\n",
       "      <td>0.293944</td>\n",
       "      <td>-0.129267</td>\n",
       "      <td>0.512433</td>\n",
       "      <td>0.322509</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.216366</td>\n",
       "      <td>0.181766</td>\n",
       "      <td>0.237602</td>\n",
       "      <td>0.279045</td>\n",
       "      <td>-0.126447</td>\n",
       "      <td>-0.209171</td>\n",
       "      <td>-0.191039</td>\n",
       "      <td>0.158190</td>\n",
       "      <td>-0.066281</td>\n",
       "      <td>-0.247980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.145354</td>\n",
       "      <td>0.215115</td>\n",
       "      <td>0.078994</td>\n",
       "      <td>0.009343</td>\n",
       "      <td>0.213732</td>\n",
       "      <td>-0.022070</td>\n",
       "      <td>0.126207</td>\n",
       "      <td>-0.130475</td>\n",
       "      <td>-0.093990</td>\n",
       "      <td>0.159534</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.155529</td>\n",
       "      <td>-0.015081</td>\n",
       "      <td>0.274674</td>\n",
       "      <td>0.272769</td>\n",
       "      <td>-0.237696</td>\n",
       "      <td>0.130785</td>\n",
       "      <td>-0.032134</td>\n",
       "      <td>0.104650</td>\n",
       "      <td>0.257081</td>\n",
       "      <td>0.016224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.138208</td>\n",
       "      <td>0.415532</td>\n",
       "      <td>-0.018799</td>\n",
       "      <td>0.094876</td>\n",
       "      <td>0.292537</td>\n",
       "      <td>-0.157601</td>\n",
       "      <td>0.552872</td>\n",
       "      <td>-0.179123</td>\n",
       "      <td>0.080213</td>\n",
       "      <td>-0.131668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.336580</td>\n",
       "      <td>-0.263726</td>\n",
       "      <td>-0.080669</td>\n",
       "      <td>0.152311</td>\n",
       "      <td>-0.093220</td>\n",
       "      <td>0.300876</td>\n",
       "      <td>-0.068453</td>\n",
       "      <td>0.307259</td>\n",
       "      <td>0.150425</td>\n",
       "      <td>-0.206309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.126634</td>\n",
       "      <td>0.311318</td>\n",
       "      <td>-0.125836</td>\n",
       "      <td>-0.143917</td>\n",
       "      <td>-0.092023</td>\n",
       "      <td>-0.118011</td>\n",
       "      <td>-0.178362</td>\n",
       "      <td>-0.358138</td>\n",
       "      <td>0.349950</td>\n",
       "      <td>-0.148357</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029561</td>\n",
       "      <td>-0.171704</td>\n",
       "      <td>0.290350</td>\n",
       "      <td>0.440742</td>\n",
       "      <td>-0.249992</td>\n",
       "      <td>0.430053</td>\n",
       "      <td>-0.086707</td>\n",
       "      <td>0.252833</td>\n",
       "      <td>-0.295727</td>\n",
       "      <td>0.226280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   claim_vec_0  claim_vec_1  claim_vec_2  claim_vec_3  claim_vec_4  \\\n",
       "0    -0.212091     0.127670     0.106502     0.391641     0.303139   \n",
       "1     0.512708     0.203220     0.151913    -0.307748    -0.426895   \n",
       "2     0.145354     0.215115     0.078994     0.009343     0.213732   \n",
       "3    -0.138208     0.415532    -0.018799     0.094876     0.292537   \n",
       "4     0.126634     0.311318    -0.125836    -0.143917    -0.092023   \n",
       "\n",
       "   claim_vec_5  claim_vec_6  claim_vec_7  claim_vec_8  claim_vec_9  ...  \\\n",
       "0    -0.248252     0.236494     0.104728    -0.271278     0.377465  ...   \n",
       "1     0.143223     0.293944    -0.129267     0.512433     0.322509  ...   \n",
       "2    -0.022070     0.126207    -0.130475    -0.093990     0.159534  ...   \n",
       "3    -0.157601     0.552872    -0.179123     0.080213    -0.131668  ...   \n",
       "4    -0.118011    -0.178362    -0.358138     0.349950    -0.148357  ...   \n",
       "\n",
       "   claim_vec_140  claim_vec_141  claim_vec_142  claim_vec_143  claim_vec_144  \\\n",
       "0      -0.283741      -0.166108      -0.125862       0.316487      -0.086704   \n",
       "1      -0.216366       0.181766       0.237602       0.279045      -0.126447   \n",
       "2      -0.155529      -0.015081       0.274674       0.272769      -0.237696   \n",
       "3      -0.336580      -0.263726      -0.080669       0.152311      -0.093220   \n",
       "4      -0.029561      -0.171704       0.290350       0.440742      -0.249992   \n",
       "\n",
       "   claim_vec_145  claim_vec_146  claim_vec_147  claim_vec_148  claim_vec_149  \n",
       "0      -0.015346       0.263394       0.236137       0.191386      -0.144861  \n",
       "1      -0.209171      -0.191039       0.158190      -0.066281      -0.247980  \n",
       "2       0.130785      -0.032134       0.104650       0.257081       0.016224  \n",
       "3       0.300876      -0.068453       0.307259       0.150425      -0.206309  \n",
       "4       0.430053      -0.086707       0.252833      -0.295727       0.226280  \n",
       "\n",
       "[5 rows x 150 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claim_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5.2 Feature Encode the Top 5 Related Article Sentences for Each Claim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In thi section, the top 5 sentences from all related articles per claim are encoded by using the Doc2Vec model to generate a vector for each sentence. (5 sentences, 150 vectors each, 750 columns of features in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 99.99%\n",
      "CPU times: user 24.3 s, sys: 2.6 s, total: 26.9 s\n",
      "Wall time: 23.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "end = full_data.shape[0]\n",
    "# end = 1\n",
    "full_sentence_vector_list = []\n",
    "for i in range(end):\n",
    "    top_5_sentences = full_data.top_5_sentences.loc[i]\n",
    "    sentence_vector_list = []\n",
    "    for u in range(len(top_5_sentences)):\n",
    "        sentence_ID = full_data.top_5_sentences.loc[i][u]\n",
    "        sentence_vector = model.docvecs[sentence_ID]\n",
    "        sentence_vector_list.extend(sentence_vector)\n",
    "    full_sentence_vector_list.append(sentence_vector_list)\n",
    "\n",
    "    # print progress\n",
    "    progress = round((i/end)*100,2)\n",
    "    clear_output(wait=True)\n",
    "    print(\"progress: \" + str(progress) + \"%\")\n",
    "\n",
    "# create column names\n",
    "complete_column_name=[]\n",
    "for i in range(1, 6):\n",
    "    column_name_list = []\n",
    "    for u in range(0,150):\n",
    "        column_name = \"sent_\" + str(i) + \"vec_\" + str(u)\n",
    "        column_name_list.append(column_name)\n",
    "    complete_column_name.extend(column_name_list)\n",
    "\n",
    "# convert full sentence features in to dataframe and name the columns accordingly\n",
    "sentence_features = pd.DataFrame.from_records(full_sentence_vector_list)\n",
    "sentence_features.columns = complete_column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_1vec_0</th>\n",
       "      <th>sent_1vec_1</th>\n",
       "      <th>sent_1vec_2</th>\n",
       "      <th>sent_1vec_3</th>\n",
       "      <th>sent_1vec_4</th>\n",
       "      <th>sent_1vec_5</th>\n",
       "      <th>sent_1vec_6</th>\n",
       "      <th>sent_1vec_7</th>\n",
       "      <th>sent_1vec_8</th>\n",
       "      <th>sent_1vec_9</th>\n",
       "      <th>...</th>\n",
       "      <th>sent_5vec_140</th>\n",
       "      <th>sent_5vec_141</th>\n",
       "      <th>sent_5vec_142</th>\n",
       "      <th>sent_5vec_143</th>\n",
       "      <th>sent_5vec_144</th>\n",
       "      <th>sent_5vec_145</th>\n",
       "      <th>sent_5vec_146</th>\n",
       "      <th>sent_5vec_147</th>\n",
       "      <th>sent_5vec_148</th>\n",
       "      <th>sent_5vec_149</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010075</td>\n",
       "      <td>-0.034665</td>\n",
       "      <td>0.158326</td>\n",
       "      <td>0.114454</td>\n",
       "      <td>0.167062</td>\n",
       "      <td>-0.325514</td>\n",
       "      <td>0.343727</td>\n",
       "      <td>-0.181544</td>\n",
       "      <td>0.427879</td>\n",
       "      <td>-0.064174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.329464</td>\n",
       "      <td>-0.123360</td>\n",
       "      <td>-0.106719</td>\n",
       "      <td>0.596236</td>\n",
       "      <td>-0.361782</td>\n",
       "      <td>-0.154024</td>\n",
       "      <td>0.078335</td>\n",
       "      <td>0.041940</td>\n",
       "      <td>-0.059460</td>\n",
       "      <td>0.239629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.537097</td>\n",
       "      <td>0.346272</td>\n",
       "      <td>-0.127193</td>\n",
       "      <td>0.008937</td>\n",
       "      <td>-0.498761</td>\n",
       "      <td>0.209842</td>\n",
       "      <td>-0.136562</td>\n",
       "      <td>-0.121760</td>\n",
       "      <td>0.598921</td>\n",
       "      <td>0.272825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.310716</td>\n",
       "      <td>1.026762</td>\n",
       "      <td>1.642795</td>\n",
       "      <td>0.402427</td>\n",
       "      <td>-0.656952</td>\n",
       "      <td>-0.680550</td>\n",
       "      <td>1.451327</td>\n",
       "      <td>0.068513</td>\n",
       "      <td>0.286796</td>\n",
       "      <td>0.028587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.034549</td>\n",
       "      <td>0.159652</td>\n",
       "      <td>0.068537</td>\n",
       "      <td>-0.108595</td>\n",
       "      <td>0.401951</td>\n",
       "      <td>0.025913</td>\n",
       "      <td>0.490867</td>\n",
       "      <td>-0.182542</td>\n",
       "      <td>0.126992</td>\n",
       "      <td>0.227730</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.187836</td>\n",
       "      <td>-0.225113</td>\n",
       "      <td>0.046499</td>\n",
       "      <td>0.118527</td>\n",
       "      <td>0.053234</td>\n",
       "      <td>0.126584</td>\n",
       "      <td>-0.112338</td>\n",
       "      <td>0.300140</td>\n",
       "      <td>0.125310</td>\n",
       "      <td>0.023155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.006104</td>\n",
       "      <td>0.218761</td>\n",
       "      <td>-0.044763</td>\n",
       "      <td>-0.014197</td>\n",
       "      <td>0.140955</td>\n",
       "      <td>0.030794</td>\n",
       "      <td>0.227532</td>\n",
       "      <td>-0.064394</td>\n",
       "      <td>0.077120</td>\n",
       "      <td>0.073692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.184586</td>\n",
       "      <td>-0.138933</td>\n",
       "      <td>0.027737</td>\n",
       "      <td>0.214644</td>\n",
       "      <td>-0.101030</td>\n",
       "      <td>0.155582</td>\n",
       "      <td>0.019132</td>\n",
       "      <td>0.178928</td>\n",
       "      <td>0.125431</td>\n",
       "      <td>0.017242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.143222</td>\n",
       "      <td>0.262105</td>\n",
       "      <td>-0.407402</td>\n",
       "      <td>-0.173036</td>\n",
       "      <td>-0.196569</td>\n",
       "      <td>0.068167</td>\n",
       "      <td>0.154569</td>\n",
       "      <td>-0.175273</td>\n",
       "      <td>0.005277</td>\n",
       "      <td>0.168737</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060368</td>\n",
       "      <td>-0.066583</td>\n",
       "      <td>0.038418</td>\n",
       "      <td>0.146584</td>\n",
       "      <td>-0.051212</td>\n",
       "      <td>0.113704</td>\n",
       "      <td>-0.006178</td>\n",
       "      <td>0.114713</td>\n",
       "      <td>0.056479</td>\n",
       "      <td>0.011474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 750 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sent_1vec_0  sent_1vec_1  sent_1vec_2  sent_1vec_3  sent_1vec_4  \\\n",
       "0     0.010075    -0.034665     0.158326     0.114454     0.167062   \n",
       "1     0.537097     0.346272    -0.127193     0.008937    -0.498761   \n",
       "2    -0.034549     0.159652     0.068537    -0.108595     0.401951   \n",
       "3    -0.006104     0.218761    -0.044763    -0.014197     0.140955   \n",
       "4     0.143222     0.262105    -0.407402    -0.173036    -0.196569   \n",
       "\n",
       "   sent_1vec_5  sent_1vec_6  sent_1vec_7  sent_1vec_8  sent_1vec_9  ...  \\\n",
       "0    -0.325514     0.343727    -0.181544     0.427879    -0.064174  ...   \n",
       "1     0.209842    -0.136562    -0.121760     0.598921     0.272825  ...   \n",
       "2     0.025913     0.490867    -0.182542     0.126992     0.227730  ...   \n",
       "3     0.030794     0.227532    -0.064394     0.077120     0.073692  ...   \n",
       "4     0.068167     0.154569    -0.175273     0.005277     0.168737  ...   \n",
       "\n",
       "   sent_5vec_140  sent_5vec_141  sent_5vec_142  sent_5vec_143  sent_5vec_144  \\\n",
       "0      -0.329464      -0.123360      -0.106719       0.596236      -0.361782   \n",
       "1       0.310716       1.026762       1.642795       0.402427      -0.656952   \n",
       "2      -0.187836      -0.225113       0.046499       0.118527       0.053234   \n",
       "3      -0.184586      -0.138933       0.027737       0.214644      -0.101030   \n",
       "4      -0.060368      -0.066583       0.038418       0.146584      -0.051212   \n",
       "\n",
       "   sent_5vec_145  sent_5vec_146  sent_5vec_147  sent_5vec_148  sent_5vec_149  \n",
       "0      -0.154024       0.078335       0.041940      -0.059460       0.239629  \n",
       "1      -0.680550       1.451327       0.068513       0.286796       0.028587  \n",
       "2       0.126584      -0.112338       0.300140       0.125310       0.023155  \n",
       "3       0.155582       0.019132       0.178928       0.125431       0.017242  \n",
       "4       0.113704      -0.006178       0.114713       0.056479       0.011474  \n",
       "\n",
       "[5 rows x 750 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5.3 Feature Encode the Similarity Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average similarity scores are appended here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_score_features = pd.concat([full_data.avg_sim_score, full_data.top5_avg_sim], axis=1)\n",
    "sim_score_features.columns = [\"avg_sent_sim\", \"avg_5_sent_sim\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6 Combine All Features (Claim Vector, Top 5 Related Sentence Vector, Similarity Scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section combines all features mentioned above in to one complete dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_similiarity_sentences_features = pd.concat([claim_features, sentence_features, sim_score_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace nan values with zero\n",
    "final_similiarity_sentences_features.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the final features to a pickle\n",
    "# final_similiarity_sentences_features.to_pickle(\"./doc2vec_features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_vec_0</th>\n",
       "      <th>claim_vec_1</th>\n",
       "      <th>claim_vec_2</th>\n",
       "      <th>claim_vec_3</th>\n",
       "      <th>claim_vec_4</th>\n",
       "      <th>claim_vec_5</th>\n",
       "      <th>claim_vec_6</th>\n",
       "      <th>claim_vec_7</th>\n",
       "      <th>claim_vec_8</th>\n",
       "      <th>claim_vec_9</th>\n",
       "      <th>...</th>\n",
       "      <th>sent_5vec_142</th>\n",
       "      <th>sent_5vec_143</th>\n",
       "      <th>sent_5vec_144</th>\n",
       "      <th>sent_5vec_145</th>\n",
       "      <th>sent_5vec_146</th>\n",
       "      <th>sent_5vec_147</th>\n",
       "      <th>sent_5vec_148</th>\n",
       "      <th>sent_5vec_149</th>\n",
       "      <th>avg_sent_sim</th>\n",
       "      <th>avg_5_sent_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.212091</td>\n",
       "      <td>0.127670</td>\n",
       "      <td>0.106502</td>\n",
       "      <td>0.391641</td>\n",
       "      <td>0.303139</td>\n",
       "      <td>-0.248252</td>\n",
       "      <td>0.236494</td>\n",
       "      <td>0.104728</td>\n",
       "      <td>-0.271278</td>\n",
       "      <td>0.377465</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106719</td>\n",
       "      <td>0.596236</td>\n",
       "      <td>-0.361782</td>\n",
       "      <td>-0.154024</td>\n",
       "      <td>0.078335</td>\n",
       "      <td>0.041940</td>\n",
       "      <td>-0.059460</td>\n",
       "      <td>0.239629</td>\n",
       "      <td>0.185311</td>\n",
       "      <td>0.291527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.512708</td>\n",
       "      <td>0.203220</td>\n",
       "      <td>0.151913</td>\n",
       "      <td>-0.307748</td>\n",
       "      <td>-0.426895</td>\n",
       "      <td>0.143223</td>\n",
       "      <td>0.293944</td>\n",
       "      <td>-0.129267</td>\n",
       "      <td>0.512433</td>\n",
       "      <td>0.322509</td>\n",
       "      <td>...</td>\n",
       "      <td>1.642795</td>\n",
       "      <td>0.402427</td>\n",
       "      <td>-0.656952</td>\n",
       "      <td>-0.680550</td>\n",
       "      <td>1.451327</td>\n",
       "      <td>0.068513</td>\n",
       "      <td>0.286796</td>\n",
       "      <td>0.028587</td>\n",
       "      <td>0.348489</td>\n",
       "      <td>0.400307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.145354</td>\n",
       "      <td>0.215115</td>\n",
       "      <td>0.078994</td>\n",
       "      <td>0.009343</td>\n",
       "      <td>0.213732</td>\n",
       "      <td>-0.022070</td>\n",
       "      <td>0.126207</td>\n",
       "      <td>-0.130475</td>\n",
       "      <td>-0.093990</td>\n",
       "      <td>0.159534</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046499</td>\n",
       "      <td>0.118527</td>\n",
       "      <td>0.053234</td>\n",
       "      <td>0.126584</td>\n",
       "      <td>-0.112338</td>\n",
       "      <td>0.300140</td>\n",
       "      <td>0.125310</td>\n",
       "      <td>0.023155</td>\n",
       "      <td>0.464089</td>\n",
       "      <td>0.642181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.138208</td>\n",
       "      <td>0.415532</td>\n",
       "      <td>-0.018799</td>\n",
       "      <td>0.094876</td>\n",
       "      <td>0.292537</td>\n",
       "      <td>-0.157601</td>\n",
       "      <td>0.552872</td>\n",
       "      <td>-0.179123</td>\n",
       "      <td>0.080213</td>\n",
       "      <td>-0.131668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027737</td>\n",
       "      <td>0.214644</td>\n",
       "      <td>-0.101030</td>\n",
       "      <td>0.155582</td>\n",
       "      <td>0.019132</td>\n",
       "      <td>0.178928</td>\n",
       "      <td>0.125431</td>\n",
       "      <td>0.017242</td>\n",
       "      <td>0.756903</td>\n",
       "      <td>0.775002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.126634</td>\n",
       "      <td>0.311318</td>\n",
       "      <td>-0.125836</td>\n",
       "      <td>-0.143917</td>\n",
       "      <td>-0.092023</td>\n",
       "      <td>-0.118011</td>\n",
       "      <td>-0.178362</td>\n",
       "      <td>-0.358138</td>\n",
       "      <td>0.349950</td>\n",
       "      <td>-0.148357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038418</td>\n",
       "      <td>0.146584</td>\n",
       "      <td>-0.051212</td>\n",
       "      <td>0.113704</td>\n",
       "      <td>-0.006178</td>\n",
       "      <td>0.114713</td>\n",
       "      <td>0.056479</td>\n",
       "      <td>0.011474</td>\n",
       "      <td>0.411560</td>\n",
       "      <td>0.547029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 902 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   claim_vec_0  claim_vec_1  claim_vec_2  claim_vec_3  claim_vec_4  \\\n",
       "0    -0.212091     0.127670     0.106502     0.391641     0.303139   \n",
       "1     0.512708     0.203220     0.151913    -0.307748    -0.426895   \n",
       "2     0.145354     0.215115     0.078994     0.009343     0.213732   \n",
       "3    -0.138208     0.415532    -0.018799     0.094876     0.292537   \n",
       "4     0.126634     0.311318    -0.125836    -0.143917    -0.092023   \n",
       "\n",
       "   claim_vec_5  claim_vec_6  claim_vec_7  claim_vec_8  claim_vec_9  ...  \\\n",
       "0    -0.248252     0.236494     0.104728    -0.271278     0.377465  ...   \n",
       "1     0.143223     0.293944    -0.129267     0.512433     0.322509  ...   \n",
       "2    -0.022070     0.126207    -0.130475    -0.093990     0.159534  ...   \n",
       "3    -0.157601     0.552872    -0.179123     0.080213    -0.131668  ...   \n",
       "4    -0.118011    -0.178362    -0.358138     0.349950    -0.148357  ...   \n",
       "\n",
       "   sent_5vec_142  sent_5vec_143  sent_5vec_144  sent_5vec_145  sent_5vec_146  \\\n",
       "0      -0.106719       0.596236      -0.361782      -0.154024       0.078335   \n",
       "1       1.642795       0.402427      -0.656952      -0.680550       1.451327   \n",
       "2       0.046499       0.118527       0.053234       0.126584      -0.112338   \n",
       "3       0.027737       0.214644      -0.101030       0.155582       0.019132   \n",
       "4       0.038418       0.146584      -0.051212       0.113704      -0.006178   \n",
       "\n",
       "   sent_5vec_147  sent_5vec_148  sent_5vec_149  avg_sent_sim  avg_5_sent_sim  \n",
       "0       0.041940      -0.059460       0.239629      0.185311        0.291527  \n",
       "1       0.068513       0.286796       0.028587      0.348489        0.400307  \n",
       "2       0.300140       0.125310       0.023155      0.464089        0.642181  \n",
       "3       0.178928       0.125431       0.017242      0.756903        0.775002  \n",
       "4       0.114713       0.056479       0.011474      0.411560        0.547029  \n",
       "\n",
       "[5 rows x 902 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_similiarity_sentences_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Sentiment Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Exploration of Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
