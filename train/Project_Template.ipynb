{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "import math\n",
    "from IPython.display import clear_output, display\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have the following two .pkl files in the same directory as the current notebook. You may download the pkl files from https://drive.google.com/open?id=1pUN-NoAbpfjLDUQgatimIuWZV04hg3k2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "data = pd.read_pickle(\"./final_data.pkl\")\n",
    "articles = pd.read_pickle('./final_articles.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the data\n",
    "#------------------------------------------------------------------------------------------\n",
    "# raw_claim: the original claim in string format\n",
    "# cleaned_claim: the original claim with all lowercase, no links, punctuations...etc. in string format\n",
    "# stemmed_claims: the cleaned_claim with stemming (reduced every word to root form) in string format\n",
    "# stemmed_stopword_claims: the stemmed claims without stopwords (the, a, is ...etc.)\n",
    "# tokenized_claim: the stemmed_stopword_claims in tokenized format\n",
    "# claimant: NaN values were filled with unknown, claimants with less than 100 counts were replaced with other\n",
    "# date: in datetime format for ease of manipulation\n",
    "# cont_days: how many days since 1986 jan 1st\n",
    "# year, month: int value representing year and month\n",
    "#article_array: a list of all the related articles per claim\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displayiong the articles\n",
    "\n",
    "# the index is the article ID\n",
    "\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to Work with Sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the tokenized sentences of every related articles with the following link (this dataframe shows the tokeinzed sentences from each article next to the sentence-ID):\n",
    "\n",
    "https://drive.google.com/open?id=1RmGqe1oC9-tmSn25EHWTHrqF1NW-zORy\n",
    "\n",
    "The following link contains the data frame with the top sentences from each related articles ranked:\n",
    "\n",
    "https://drive.google.com/open?id=1YkBME4_J4wKrmVj28AHod6w9RB2f011O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sim_scores = pd.read_pickle(\"./data_simscore_claim_vs_sentence.pkl\")\n",
    "data_sim_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the sentences are tokenized\n",
    "sentence_ID_df = pd.read_pickle(\"./article_sentences_ind.pkl\")\n",
    "sentence_ID_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# this will untokenize every sentence in the sentences column\n",
    "\n",
    "untokenized_sentence_list = []\n",
    "for i in range(sentence_ID_df.shape[0]):\n",
    "    tokenized_sentence = sentence_ID_df.sentences.loc[i]\n",
    "    untokenized_sentence = ' '.join(tokenized_sentence)\n",
    "    untokenized_sentence_list.append(untokenized_sentence)\n",
    "    \n",
    "    # print progress\n",
    "    progress = round((i/sentence_ID_df.shape[0])*100,2)\n",
    "    clear_output(wait=True)\n",
    "    print(\"progress: \" + str(progress) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_ID_df['untokenized'] = untokenized_sentence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to work with the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to go to a specific cleaned claim\n",
    "data.cleaned_claim.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to go to a specific cleaned article\n",
    "articles.cleaned_articles.loc['75770'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample code to read elements from each article_array from the data dataframe\n",
    "\n",
    "# to iterate the article array\n",
    "for i in range(data.shape[0]):\n",
    "    # i iterates row by row till the end\n",
    "    for u in range(len(data.article_array[i])):\n",
    "        # u holds the index of each element, within each array. Uncomment the following to understand\n",
    "        # print(u)\n",
    "        art_array = data.article_array[i]\n",
    "        # print specific elements of each array\n",
    "        print(art_array[u])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "class",
   "language": "python",
   "name": "class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
