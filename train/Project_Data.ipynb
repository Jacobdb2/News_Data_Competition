{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project_Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "1. Claim and Related Article Data:\n",
    "    - Made lowercase, removed punctuations, links, unicode hex amongst other misc items like \", ', - ...etc.\n",
    "    - Removed stopwords and Tokenized\n",
    "        - To run this section, you may have to download the stopwords packages. I have included the code, you just have to uncomment 2 lines on the first run (section 1.3)\n",
    "    - *new - November 15* implemented sentence extraction and applied the same processing as above\n",
    "2. Date\n",
    "    - Converted from string to datetime format (for practicality)\n",
    "    - Created 3 features:\n",
    "        - 1. Days since Jan 1st 1986\n",
    "        - 2. The Month\n",
    "        - 3. The Year\n",
    "3. Claimant\n",
    "    - Replaced missing values with \"unknown\"\n",
    "    - Replaced counts below threshold with \"other\"\n",
    "4. Final Frame\n",
    "    - 2 final frames:\n",
    "        - final_data = this is the frame that holds the claims, claimant, date, label, related articles\n",
    "        - final_articles = this is the frame that holds the related articles\n",
    "    - I have included a few extra lines of code as an example of how to work with the frames\n",
    "        - I have saved the output of the 2 dataframe to csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "import math\n",
    "from IPython.display import clear_output, display\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import string\n",
    "\n",
    "# The following line is needed to show plots inline in notebooks\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert strings to numpy array - used to convert the related_articles column in to arrays\n",
    "# for practicality\n",
    "def str2array(value):\n",
    "    str_list = re.findall(r'\\d+', value)\n",
    "    int_list = list(map(int, str_list))\n",
    "    article_array = np.array(int_list)\n",
    "    return article_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15555, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>related_articles</th>\n",
       "      <th>article_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A line from George Orwell's novel 1984 predict...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17/07/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Maine legislature candidate Leslie Gibson insu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17/03/2018</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A 17-year-old girl named Alyssa Carson is bein...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18/07/2018</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>In 1988 author Roald Dahl penned an open lette...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>04/02/2019</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>When it comes to fighting terrorism, \"Another ...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>22/03/2016</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              claim  \\\n",
       "0           0  A line from George Orwell's novel 1984 predict...   \n",
       "1           1  Maine legislature candidate Leslie Gibson insu...   \n",
       "2           2  A 17-year-old girl named Alyssa Carson is bein...   \n",
       "3           3  In 1988 author Roald Dahl penned an open lette...   \n",
       "4           4  When it comes to fighting terrorism, \"Another ...   \n",
       "\n",
       "          claimant        date  id  label  \\\n",
       "0              NaN  17/07/2017   0      0   \n",
       "1              NaN  17/03/2018   1      2   \n",
       "2              NaN  18/07/2018   4      1   \n",
       "3              NaN  04/02/2019   5      2   \n",
       "4  Hillary Clinton  22/03/2016   6      2   \n",
       "\n",
       "                             related_articles  \\\n",
       "0            [122094, 122580, 130685, 134765]   \n",
       "1                    [106868, 127320, 128060]   \n",
       "2                    [132130, 132132, 149722]   \n",
       "3                    [123254, 123418, 127464]   \n",
       "4  [41099, 89899, 72543, 82644, 95344, 88361]   \n",
       "\n",
       "                                article_array  \n",
       "0            [122094, 122580, 130685, 134765]  \n",
       "1                    [106868, 127320, 128060]  \n",
       "2                    [132130, 132132, 149722]  \n",
       "3                    [123254, 123418, 127464]  \n",
       "4  [41099, 89899, 72543, 82644, 95344, 88361]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new column with the related articles saved as an array called \"article_array\"\n",
    "data['article_array'] = data['related_articles'].apply(str2array)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths\n",
    "cur_path = os.path.dirname(os.path.abspath(\"Project_Data.ipynb\"))\n",
    "articles_dir = cur_path + '/train_articles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.39 s, sys: 466 ms, total: 1.85 s\n",
      "Wall time: 1.86 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125385</th>\n",
       "      <td>Dr. Ben Carson: Welfare Benefactor?\\nAn initia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32238</th>\n",
       "      <td>The World Factbook — Central Intelligence Agen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16051</th>\n",
       "      <td>1014 texaseducationagencypftexas\\n\\nEmails, La...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118633</th>\n",
       "      <td>Clinton camp delays Weather Channel ad buy aft...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117945</th>\n",
       "      <td>Living with kangaroos\\nKangaroos are appealing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Article\n",
       "125385  Dr. Ben Carson: Welfare Benefactor?\\nAn initia...\n",
       "32238   The World Factbook — Central Intelligence Agen...\n",
       "16051   1014 texaseducationagencypftexas\\n\\nEmails, La...\n",
       "118633  Clinton camp delays Weather Channel ad buy aft...\n",
       "117945  Living with kangaroos\\nKangaroos are appealing..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# create a dictionary of article ID and content\n",
    "article_dict = {}\n",
    "for filename in os.listdir(articles_dir):\n",
    "    filenumber = filename.replace('.txt', '')\n",
    "    file_open = open(articles_dir + filename, \"r\")\n",
    "    text = file_open.read()\n",
    "    article_dict[filenumber] = text\n",
    "# use the dictionary created to create a dataframe of articles\n",
    "articles  = pd.DataFrame.from_dict(article_dict, orient='index')\n",
    "articles.columns = ['Article']\n",
    "# a dataframe that holds all the articles\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Related Articles and Claim Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Basic Cleaning for Related Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 53s, sys: 454 ms, total: 1min 53s\n",
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# CLEAN ARTICLE DATA - ~5 minutes to run\n",
    "# convert all string values to lower case\n",
    "articles_cleaned = articles.apply(lambda x: x.str.lower())\n",
    "# replace new line with space\n",
    "articles_cleaned = articles_cleaned.replace('\\n', ' ', regex=True)\n",
    "# get rid of all links\n",
    "articles_cleaned = articles_cleaned.Article.replace(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}     /)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', regex = True).to_frame()\n",
    "# get rid of unicode hex\n",
    "articles_cleaned = articles_cleaned.Article.replace({r'[^\\x00-\\x7F]+':''}, regex=True).to_frame()\n",
    "# remove punctuation\n",
    "# articles_cleaned = articles_cleaned.Article.str.replace('[{}]'.format(string.punctuation), '').to_frame()\n",
    "# remove misc items\n",
    "articles_cleaned = articles_cleaned.replace(' — ', ' ', regex=True)\n",
    "articles_cleaned = articles_cleaned.replace('-', '', regex=True)\n",
    "articles_cleaned = articles_cleaned.replace('’', '', regex=True)\n",
    "articles_cleaned = articles_cleaned.replace('‘', '', regex=True)\n",
    "articles_cleaned = articles_cleaned.replace('”', '', regex=True)\n",
    "articles_cleaned = articles_cleaned.replace('“', '', regex=True)\n",
    "# replace consecutive spaces with just one space\n",
    "articles_cleaned = articles_cleaned.replace('\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Sentences from Related Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min, sys: 620 ms, total: 2min\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# split each article in to sentences\n",
    "from nltk import sent_tokenize\n",
    "article_sentences = []\n",
    "for i in range(articles_cleaned.shape[0]):\n",
    "    sentence = articles_cleaned.Article[articles_cleaned.index[i]]\n",
    "    tok_sen = sent_tokenize(sentence)\n",
    "    article_sentences.append(tok_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation from cleaned articles - had to do this after the sentences are separated\n",
    "articles_cleaned = articles_cleaned.Article.str.replace('[{}]'.format(string.punctuation), '').to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.6 s, sys: 296 ms, total: 48.8 s\n",
      "Wall time: 48.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# remove punctuation from sentences\n",
    "cleaned_article_sent = []\n",
    "for i in range(articles_cleaned.shape[0]):\n",
    "    cleaned_sent = [''.join(c for c in s if c not in string.punctuation) for s in article_sentences[i]]\n",
    "    # cleaned_article_sent is a list of lists\n",
    "    # the sentences from each article is grouped in to a list, and then grouped together based on article\n",
    "    cleaned_article_sent.append(cleaned_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the cleaned_Article_sent in to a dataframe that's indexed by article ID\n",
    "article_sentences = pd.Series(cleaned_article_sent).to_frame()\n",
    "article_sentences.columns = ['Sentence']\n",
    "article_sentences.index = articles_cleaned.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125385</th>\n",
       "      <td>[dr ben carson welfare benefactor, an initiall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32238</th>\n",
       "      <td>[the world factbook central intelligence agenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16051</th>\n",
       "      <td>[1014 texaseducationagencypftexas emails laure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118633</th>\n",
       "      <td>[clinton camp delays weather channel ad buy af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117945</th>\n",
       "      <td>[living with kangaroos kangaroos are appealing...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Sentence\n",
       "125385  [dr ben carson welfare benefactor, an initiall...\n",
       "32238   [the world factbook central intelligence agenc...\n",
       "16051   [1014 texaseducationagencypftexas emails laure...\n",
       "118633  [clinton camp delays weather channel ad buy af...\n",
       "117945  [living with kangaroos kangaroos are appealing..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# article sentences in a dataframe\n",
    "article_sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Basic Cleaning for Claims "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 479 ms, sys: 0 ns, total: 479 ms\n",
      "Wall time: 477 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# CLEAN CLAIM DATA\n",
    "# create a new dataframe of just claims\n",
    "cleaned_claim = data.claim.to_frame()\n",
    "# convert all string values to lower case\n",
    "cleaned_claim = cleaned_claim.apply(lambda x: x.str.lower())\n",
    "# replace new line with space\n",
    "cleaned_claim = cleaned_claim.replace('\\n', ' ', regex=True)\n",
    "# get rid of all links\n",
    "cleaned_claim = cleaned_claim.claim.replace(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}     /)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', regex = True).to_frame()\n",
    "# get rid of unicode hex\n",
    "cleaned_claim = cleaned_claim.claim.replace({r'[^\\x00-\\x7F]+':''}, regex=True).to_frame()\n",
    "# remove punctuation\n",
    "cleaned_claim = cleaned_claim.claim.str.replace('[{}]'.format(string.punctuation), '').to_frame()\n",
    "# remove misc items\n",
    "cleaned_claim = cleaned_claim.replace(' — ', ' ', regex=True)\n",
    "cleaned_claim = cleaned_claim.replace('-', ' ', regex=True)\n",
    "cleaned_claim = cleaned_claim.replace('’', '', regex=True)\n",
    "cleaned_claim = cleaned_claim.replace('‘', '', regex=True)\n",
    "cleaned_claim = cleaned_claim.replace('”', '', regex=True)\n",
    "cleaned_claim = cleaned_claim.replace('“', '', regex=True)\n",
    "# replace consecutive spaces with just one space\n",
    "cleaned_claim = cleaned_claim.replace('\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>label</th>\n",
       "      <th>article_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a line from george orwells novel 1984 predicts...</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>maine legislature candidate leslie gibson insu...</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a 17yearold girl named alyssa carson is being ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in 1988 author roald dahl penned an open lette...</td>\n",
       "      <td>2</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when it comes to fighting terrorism another th...</td>\n",
       "      <td>2</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim  label  \\\n",
       "0  a line from george orwells novel 1984 predicts...      0   \n",
       "1  maine legislature candidate leslie gibson insu...      2   \n",
       "2  a 17yearold girl named alyssa carson is being ...      1   \n",
       "3  in 1988 author roald dahl penned an open lette...      2   \n",
       "4  when it comes to fighting terrorism another th...      2   \n",
       "\n",
       "                                article_array  \n",
       "0            [122094, 122580, 130685, 134765]  \n",
       "1                    [106868, 127320, 128060]  \n",
       "2                    [132130, 132132, 149722]  \n",
       "3                    [123254, 123418, 127464]  \n",
       "4  [41099, 89899, 72543, 82644, 95344, 88361]  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate cleaned_claims with label and article_array\n",
    "cleaned_claim = pd.concat([cleaned_claim, data.label, data.article_array], axis=1)\n",
    "# cleaned_claim now holds the claims that are cleaned, the label, and the article array\n",
    "cleaned_claim.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Stemming, Stop Words and Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "# the first time running - you may need to uncomment the bottom two lines to download the necessary packages\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of claims\n",
    "claim_list=[]\n",
    "for i in range(cleaned_claim.shape[0]):\n",
    "    claim_entry = cleaned_claim.claim.loc[i]\n",
    "    claim_list.append(claim_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 99.99%\n",
      "CPU times: user 33.6 s, sys: 2.39 s, total: 36 s\n",
      "Wall time: 33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# tokenize every claim in the claim list generated from above\n",
    "# the result is a list of tokenized claims: tokenized_claims\n",
    "tokenized_claims = []\n",
    "stemmed_claims = []\n",
    "stemmed_sw_claims = []\n",
    "for i in range(cleaned_claim.shape[0]):\n",
    "\n",
    "    #--------------------------------------------------------------\n",
    "    # stemming\n",
    "    word_tokens = word_tokenize(claim_list[i])\n",
    "    stemmed_tok_claims = []\n",
    "    for w in word_tokens:\n",
    "        stemmed_tok_claims.append(ps.stem(w))\n",
    "    stemmed_string = ' '.join(stemmed_tok_claims)\n",
    "    # stemmed_claims is a list of stemmed strings\n",
    "    stemmed_claims.append(stemmed_string)\n",
    "    \n",
    "    #--------------------------------------------------------------\n",
    "    # remove stop words\n",
    "    stemmed_sw_string = []\n",
    "    word_tokens = word_tokenize(stemmed_claims[i])\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    stemmed_sw_string = ' '.join(filtered_sentence)\n",
    "    # stemmed_sw_claims is a list of stemmed strings without stopwords\n",
    "    stemmed_sw_claims.append(stemmed_sw_string)    \n",
    "        \n",
    "    #--------------------------------------------------------------    \n",
    "    # tokenize\n",
    "    tokenized_ = word_tokenize(stemmed_sw_claims[i])\n",
    "    tokenized_claims.append(tokenized_)\n",
    "    \n",
    "    # print progress\n",
    "    progress = round((i/cleaned_claim.shape[0])*100,2)\n",
    "    clear_output(wait=True)\n",
    "    print(\"progress: \" + str(progress) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of tokenized claims\n",
    "# tokenized_claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Claims Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip together all the claim lists and create a dataframe\n",
    "zipped_claims = list(zip(stemmed_claims, stemmed_sw_claims, tokenized_claims))\n",
    "claims_ = pd.DataFrame(zipped_claims, columns = ['stemmed_claims', 'stemmed_stopword_claims', 'tokenized_claims'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_claims</th>\n",
       "      <th>stemmed_stopword_claims</th>\n",
       "      <th>tokenized_claims</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a line from georg orwel novel 1984 predict the...</td>\n",
       "      <td>line georg orwel novel 1984 predict power smar...</td>\n",
       "      <td>[line, georg, orwel, novel, 1984, predict, pow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>[main, legislatur, candid, lesli, gibson, insu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a 17yearold girl name alyssa carson is be trai...</td>\n",
       "      <td>17yearold girl name alyssa carson train nasa b...</td>\n",
       "      <td>[17yearold, girl, name, alyssa, carson, train,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in 1988 author roald dahl pen an open letter u...</td>\n",
       "      <td>1988 author roald dahl pen open letter urg par...</td>\n",
       "      <td>[1988, author, roald, dahl, pen, open, letter,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when it come to fight terror anoth thing we kn...</td>\n",
       "      <td>come fight terror anoth thing know doe work ba...</td>\n",
       "      <td>[come, fight, terror, anoth, thing, know, doe,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      stemmed_claims  \\\n",
       "0  a line from georg orwel novel 1984 predict the...   \n",
       "1  main legislatur candid lesli gibson insult par...   \n",
       "2  a 17yearold girl name alyssa carson is be trai...   \n",
       "3  in 1988 author roald dahl pen an open letter u...   \n",
       "4  when it come to fight terror anoth thing we kn...   \n",
       "\n",
       "                             stemmed_stopword_claims  \\\n",
       "0  line georg orwel novel 1984 predict power smar...   \n",
       "1  main legislatur candid lesli gibson insult par...   \n",
       "2  17yearold girl name alyssa carson train nasa b...   \n",
       "3  1988 author roald dahl pen open letter urg par...   \n",
       "4  come fight terror anoth thing know doe work ba...   \n",
       "\n",
       "                                    tokenized_claims  \n",
       "0  [line, georg, orwel, novel, 1984, predict, pow...  \n",
       "1  [main, legislatur, candid, lesli, gibson, insu...  \n",
       "2  [17yearold, girl, name, alyssa, carson, train,...  \n",
       "3  [1988, author, roald, dahl, pen, open, letter,...  \n",
       "4  [come, fight, terror, anoth, thing, know, doe,...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claims_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Related Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 100.0%\n",
      "CPU times: user 31min 18s, sys: 20 s, total: 31min 38s\n",
      "Wall time: 31min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create a list of tokenized, non-stop words articles ~ takes 30 min\n",
    "tokenized_articles = []\n",
    "stemmed_art = []\n",
    "stemmed_sw_art = []\n",
    "\n",
    "for i in range(articles_cleaned.shape[0]):\n",
    "\n",
    "    #--------------------------------------------------------------\n",
    "    # stemming\n",
    "    word_tokens = word_tokenize(articles_cleaned.Article[articles_cleaned.index[i]])\n",
    "    stemmed_tok_art = []\n",
    "    for w in word_tokens:\n",
    "        stemmed_tok_art.append(ps.stem(w))\n",
    "    stemmed_string = ' '.join(stemmed_tok_art)\n",
    "    # stemmed_claims is a list of stemmed strings\n",
    "    stemmed_art.append(stemmed_string)\n",
    "    \n",
    "    #--------------------------------------------------------------\n",
    "    # remove stop words\n",
    "    stemmed_sw_string = []\n",
    "    word_tokens = word_tokenize(stemmed_art[i])\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    stemmed_sw_string = ' '.join(filtered_sentence)\n",
    "    # stemmed_sw_claims is a list of stemmed strings without stopwords\n",
    "    stemmed_sw_art.append(stemmed_sw_string)    \n",
    "    \n",
    "    #--------------------------------------------------------------    \n",
    "    # tokenize\n",
    "    tokenized_ = word_tokenize(stemmed_sw_art[i])\n",
    "    tokenized_articles.append(tokenized_)\n",
    "    \n",
    "    # print progress\n",
    "    progress = round((i/articles_cleaned.shape[0])*100,2)\n",
    "    clear_output(wait=True)\n",
    "    print(\"progress: \" + str(progress) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentences for Related Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 100.0%\n",
      "CPU times: user 29min 28s, sys: 27.2 s, total: 29min 55s\n",
      "Wall time: 29min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# stem, remove stop words from sentences ~ takes 30 min\n",
    "\n",
    "one_article = []\n",
    "final_article_sentences = []\n",
    "tokenized_sentences = []\n",
    "final_article_tok_sentences = []\n",
    "\n",
    "for i in range(article_sentences.shape[0]):\n",
    "    # grab a series of sentence for one article\n",
    "    sentence_list = article_sentences.Sentence[article_sentences.index[i]]\n",
    "    for u in range(len(sentence_list)):\n",
    "        # stem, stopwords for each sentence in an article\n",
    "        sent = sentence_list[u]\n",
    "        word_tokens = word_tokenize(sent)\n",
    "        stemmed_tok_sent = []\n",
    "        for w in word_tokens:\n",
    "            stemmed_tok_sent.append(ps.stem(w))\n",
    "        filtered_sentence = [w for w in stemmed_tok_sent if not w in stop_words]\n",
    "        # append the tokenized strings\n",
    "        tokenized_sentences.append(filtered_sentence)\n",
    "        # append the strings\n",
    "        stemmed_sw_string = ' '.join(filtered_sentence)\n",
    "        one_article.append(stemmed_sw_string)\n",
    "    # append each one_article to a full list of all articles\n",
    "    final_article_sentences.append(one_article)  \n",
    "    final_article_tok_sentences.append(tokenized_sentences)\n",
    "    one_article = []\n",
    "    tokenized_sentences = []\n",
    "    \n",
    "    # print progress\n",
    "    progress = round((i/article_sentences.shape[0])*100,2)\n",
    "    clear_output(wait=True)\n",
    "    print(\"progress: \" + str(progress) + \"%\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "      <th>tokenized_cleaned_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125385</th>\n",
       "      <td>[dr ben carson welfare benefactor, an initiall...</td>\n",
       "      <td>[dr ben carson welfar benefactor, initi unlik ...</td>\n",
       "      <td>[[dr, ben, carson, welfar, benefactor], [initi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32238</th>\n",
       "      <td>[the world factbook central intelligence agenc...</td>\n",
       "      <td>[world factbook central intellig agenc unit st...</td>\n",
       "      <td>[[world, factbook, central, intellig, agenc, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16051</th>\n",
       "      <td>[1014 texaseducationagencypftexas emails laure...</td>\n",
       "      <td>[1014 texaseducationagencypftexa email lauren ...</td>\n",
       "      <td>[[1014, texaseducationagencypftexa, email, lau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118633</th>\n",
       "      <td>[clinton camp delays weather channel ad buy af...</td>\n",
       "      <td>[clinton camp delay weather channel ad buy bac...</td>\n",
       "      <td>[[clinton, camp, delay, weather, channel, ad, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117945</th>\n",
       "      <td>[living with kangaroos kangaroos are appealing...</td>\n",
       "      <td>[live kangaroo kangaroo appeal wild power nati...</td>\n",
       "      <td>[[live, kangaroo, kangaroo, appeal, wild, powe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Sentence  \\\n",
       "125385  [dr ben carson welfare benefactor, an initiall...   \n",
       "32238   [the world factbook central intelligence agenc...   \n",
       "16051   [1014 texaseducationagencypftexas emails laure...   \n",
       "118633  [clinton camp delays weather channel ad buy af...   \n",
       "117945  [living with kangaroos kangaroos are appealing...   \n",
       "\n",
       "                                         cleaned_sentence  \\\n",
       "125385  [dr ben carson welfar benefactor, initi unlik ...   \n",
       "32238   [world factbook central intellig agenc unit st...   \n",
       "16051   [1014 texaseducationagencypftexa email lauren ...   \n",
       "118633  [clinton camp delay weather channel ad buy bac...   \n",
       "117945  [live kangaroo kangaroo appeal wild power nati...   \n",
       "\n",
       "                               tokenized_cleaned_sentence  \n",
       "125385  [[dr, ben, carson, welfar, benefactor], [initi...  \n",
       "32238   [[world, factbook, central, intellig, agenc, u...  \n",
       "16051   [[1014, texaseducationagencypftexa, email, lau...  \n",
       "118633  [[clinton, camp, delay, weather, channel, ad, ...  \n",
       "117945  [[live, kangaroo, kangaroo, appeal, wild, powe...  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# article_sentences is the dataframe that holds the sentences\n",
    "article_sentences['cleaned_sentence'] = final_article_sentences\n",
    "article_sentences['tokenized_cleaned_sentence'] = final_article_tok_sentences\n",
    "article_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of tokenized related articles - below is showing only the first entry of the list\n",
    "# tokenized_articles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Related Articles DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip together all the articles and create a dataframe\n",
    "zipped_articles = list(zip(stemmed_art, stemmed_sw_art, tokenized_articles, final_article_sentences, final_article_tok_sentences))\n",
    "articles_ = pd.DataFrame(zipped_articles, columns = ['stemmed_articles', 'stemmed_stopword_articles', 'tokenized_articles', 'cleaned_sentence', 'tokenized_cleaned_sentence'])\n",
    "# index the articles based on article ID\n",
    "articles_.index = [articles_cleaned.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stemmed_articles</th>\n",
       "      <th>stemmed_stopword_articles</th>\n",
       "      <th>tokenized_articles</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "      <th>tokenized_cleaned_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125385</th>\n",
       "      <td>dr ben carson welfar benefactor an initi unlik...</td>\n",
       "      <td>dr ben carson welfar benefactor initi unlik cl...</td>\n",
       "      <td>[dr, ben, carson, welfar, benefactor, initi, u...</td>\n",
       "      <td>[dr ben carson welfar benefactor, initi unlik ...</td>\n",
       "      <td>[[dr, ben, carson, welfar, benefactor], [initi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32238</th>\n",
       "      <td>the world factbook central intellig agenc the ...</td>\n",
       "      <td>world factbook central intellig agenc unit sta...</td>\n",
       "      <td>[world, factbook, central, intellig, agenc, un...</td>\n",
       "      <td>[world factbook central intellig agenc unit st...</td>\n",
       "      <td>[[world, factbook, central, intellig, agenc, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16051</th>\n",
       "      <td>1014 texaseducationagencypftexa email lauren c...</td>\n",
       "      <td>1014 texaseducationagencypftexa email lauren c...</td>\n",
       "      <td>[1014, texaseducationagencypftexa, email, laur...</td>\n",
       "      <td>[1014 texaseducationagencypftexa email lauren ...</td>\n",
       "      <td>[[1014, texaseducationagencypftexa, email, lau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118633</th>\n",
       "      <td>clinton camp delay weather channel ad buy afte...</td>\n",
       "      <td>clinton camp delay weather channel ad buy back...</td>\n",
       "      <td>[clinton, camp, delay, weather, channel, ad, b...</td>\n",
       "      <td>[clinton camp delay weather channel ad buy bac...</td>\n",
       "      <td>[[clinton, camp, delay, weather, channel, ad, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117945</th>\n",
       "      <td>live with kangaroo kangaroo are appeal wild an...</td>\n",
       "      <td>live kangaroo kangaroo appeal wild power nativ...</td>\n",
       "      <td>[live, kangaroo, kangaroo, appeal, wild, power...</td>\n",
       "      <td>[live kangaroo kangaroo appeal wild power nati...</td>\n",
       "      <td>[[live, kangaroo, kangaroo, appeal, wild, powe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         stemmed_articles  \\\n",
       "125385  dr ben carson welfar benefactor an initi unlik...   \n",
       "32238   the world factbook central intellig agenc the ...   \n",
       "16051   1014 texaseducationagencypftexa email lauren c...   \n",
       "118633  clinton camp delay weather channel ad buy afte...   \n",
       "117945  live with kangaroo kangaroo are appeal wild an...   \n",
       "\n",
       "                                stemmed_stopword_articles  \\\n",
       "125385  dr ben carson welfar benefactor initi unlik cl...   \n",
       "32238   world factbook central intellig agenc unit sta...   \n",
       "16051   1014 texaseducationagencypftexa email lauren c...   \n",
       "118633  clinton camp delay weather channel ad buy back...   \n",
       "117945  live kangaroo kangaroo appeal wild power nativ...   \n",
       "\n",
       "                                       tokenized_articles  \\\n",
       "125385  [dr, ben, carson, welfar, benefactor, initi, u...   \n",
       "32238   [world, factbook, central, intellig, agenc, un...   \n",
       "16051   [1014, texaseducationagencypftexa, email, laur...   \n",
       "118633  [clinton, camp, delay, weather, channel, ad, b...   \n",
       "117945  [live, kangaroo, kangaroo, appeal, wild, power...   \n",
       "\n",
       "                                         cleaned_sentence  \\\n",
       "125385  [dr ben carson welfar benefactor, initi unlik ...   \n",
       "32238   [world factbook central intellig agenc unit st...   \n",
       "16051   [1014 texaseducationagencypftexa email lauren ...   \n",
       "118633  [clinton camp delay weather channel ad buy bac...   \n",
       "117945  [live kangaroo kangaroo appeal wild power nati...   \n",
       "\n",
       "                               tokenized_cleaned_sentence  \n",
       "125385  [[dr, ben, carson, welfar, benefactor], [initi...  \n",
       "32238   [[world, factbook, central, intellig, agenc, u...  \n",
       "16051   [[1014, texaseducationagencypftexa, email, lau...  \n",
       "118633  [[clinton, camp, delay, weather, channel, ad, ...  \n",
       "117945  [[live, kangaroo, kangaroo, appeal, wild, powe...  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>related_articles</th>\n",
       "      <th>article_array</th>\n",
       "      <th>new_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A line from George Orwell's novel 1984 predict...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17/07/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "      <td>2017-07-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Maine legislature candidate Leslie Gibson insu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17/03/2018</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "      <td>2018-03-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A 17-year-old girl named Alyssa Carson is bein...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18/07/2018</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "      <td>2018-07-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>In 1988 author Roald Dahl penned an open lette...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>04/02/2019</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "      <td>2019-02-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>When it comes to fighting terrorism, \"Another ...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>22/03/2016</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "      <td>2016-03-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              claim  \\\n",
       "0           0  A line from George Orwell's novel 1984 predict...   \n",
       "1           1  Maine legislature candidate Leslie Gibson insu...   \n",
       "2           2  A 17-year-old girl named Alyssa Carson is bein...   \n",
       "3           3  In 1988 author Roald Dahl penned an open lette...   \n",
       "4           4  When it comes to fighting terrorism, \"Another ...   \n",
       "\n",
       "          claimant        date  id  label  \\\n",
       "0              NaN  17/07/2017   0      0   \n",
       "1              NaN  17/03/2018   1      2   \n",
       "2              NaN  18/07/2018   4      1   \n",
       "3              NaN  04/02/2019   5      2   \n",
       "4  Hillary Clinton  22/03/2016   6      2   \n",
       "\n",
       "                             related_articles  \\\n",
       "0            [122094, 122580, 130685, 134765]   \n",
       "1                    [106868, 127320, 128060]   \n",
       "2                    [132130, 132132, 149722]   \n",
       "3                    [123254, 123418, 127464]   \n",
       "4  [41099, 89899, 72543, 82644, 95344, 88361]   \n",
       "\n",
       "                                article_array   new_date  \n",
       "0            [122094, 122580, 130685, 134765] 2017-07-17  \n",
       "1                    [106868, 127320, 128060] 2018-03-17  \n",
       "2                    [132130, 132132, 149722] 2018-07-18  \n",
       "3                    [123254, 123418, 127464] 2019-02-04  \n",
       "4  [41099, 89899, 72543, 82644, 95344, 88361] 2016-03-22  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert date column to datetime format\n",
    "data['new_date'] = pd.to_datetime(data['date'], dayfirst=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new feature with consecutive days since January 1st, 1986\n",
    "data['start_date'] = pd.to_datetime('1986-01-01', format='%Y-%m-%d')\n",
    "data['cont_days'] = (data['new_date'] - data['start_date']).dt.days\n",
    "data = data.drop(['start_date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Year and Month features in to int (instead of str before), can be kept as int since it is ordinal\n",
    "\n",
    "#Year\n",
    "data['Year'] = data['new_date'].apply(lambda x: \"%d\" % (x.year))\n",
    "data['Year'] = data['Year'].astype(int)\n",
    "# Month\n",
    "data['Month'] = data['new_date'].apply(lambda x: \"%d\" % (x.month))\n",
    "data['Month'] = data['Month'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>related_articles</th>\n",
       "      <th>article_array</th>\n",
       "      <th>new_date</th>\n",
       "      <th>cont_days</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A line from George Orwell's novel 1984 predict...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17/07/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>11520</td>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Maine legislature candidate Leslie Gibson insu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17/03/2018</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>11763</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A 17-year-old girl named Alyssa Carson is bein...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18/07/2018</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "      <td>2018-07-18</td>\n",
       "      <td>11886</td>\n",
       "      <td>2018</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>In 1988 author Roald Dahl penned an open lette...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>04/02/2019</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>12087</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>When it comes to fighting terrorism, \"Another ...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>22/03/2016</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "      <td>2016-03-22</td>\n",
       "      <td>11038</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              claim  \\\n",
       "0           0  A line from George Orwell's novel 1984 predict...   \n",
       "1           1  Maine legislature candidate Leslie Gibson insu...   \n",
       "2           2  A 17-year-old girl named Alyssa Carson is bein...   \n",
       "3           3  In 1988 author Roald Dahl penned an open lette...   \n",
       "4           4  When it comes to fighting terrorism, \"Another ...   \n",
       "\n",
       "          claimant        date  id  label  \\\n",
       "0              NaN  17/07/2017   0      0   \n",
       "1              NaN  17/03/2018   1      2   \n",
       "2              NaN  18/07/2018   4      1   \n",
       "3              NaN  04/02/2019   5      2   \n",
       "4  Hillary Clinton  22/03/2016   6      2   \n",
       "\n",
       "                             related_articles  \\\n",
       "0            [122094, 122580, 130685, 134765]   \n",
       "1                    [106868, 127320, 128060]   \n",
       "2                    [132130, 132132, 149722]   \n",
       "3                    [123254, 123418, 127464]   \n",
       "4  [41099, 89899, 72543, 82644, 95344, 88361]   \n",
       "\n",
       "                                article_array   new_date  cont_days  Year  \\\n",
       "0            [122094, 122580, 130685, 134765] 2017-07-17      11520  2017   \n",
       "1                    [106868, 127320, 128060] 2018-03-17      11763  2018   \n",
       "2                    [132130, 132132, 149722] 2018-07-18      11886  2018   \n",
       "3                    [123254, 123418, 127464] 2019-02-04      12087  2019   \n",
       "4  [41099, 89899, 72543, 82644, 95344, 88361] 2016-03-22      11038  2016   \n",
       "\n",
       "   Month  \n",
       "0      7  \n",
       "1      3  \n",
       "2      7  \n",
       "3      2  \n",
       "4      3  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 columns at the end show the new_date (which is the date in a date format), the continuous days, \n",
    "# the year and month\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Claimant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing claimants with \"unknown\"\n",
    "data['claimant'] = data['claimant'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group together all counts less than 100 in to Others\n",
    "claimant_count = data['claimant'].value_counts()\n",
    "value_mask = data.claimant.isin(claimant_count.index[claimant_count < 100]) \n",
    "data.loc[value_mask,'claimant'] = \"Other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>related_articles</th>\n",
       "      <th>article_array</th>\n",
       "      <th>new_date</th>\n",
       "      <th>cont_days</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A line from George Orwell's novel 1984 predict...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>17/07/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>11520</td>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Maine legislature candidate Leslie Gibson insu...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>17/03/2018</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>11763</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A 17-year-old girl named Alyssa Carson is bein...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>18/07/2018</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "      <td>2018-07-18</td>\n",
       "      <td>11886</td>\n",
       "      <td>2018</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>In 1988 author Roald Dahl penned an open lette...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>04/02/2019</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>12087</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>When it comes to fighting terrorism, \"Another ...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>22/03/2016</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "      <td>2016-03-22</td>\n",
       "      <td>11038</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              claim  \\\n",
       "0           0  A line from George Orwell's novel 1984 predict...   \n",
       "1           1  Maine legislature candidate Leslie Gibson insu...   \n",
       "2           2  A 17-year-old girl named Alyssa Carson is bein...   \n",
       "3           3  In 1988 author Roald Dahl penned an open lette...   \n",
       "4           4  When it comes to fighting terrorism, \"Another ...   \n",
       "\n",
       "          claimant        date  id  label  \\\n",
       "0          Unknown  17/07/2017   0      0   \n",
       "1          Unknown  17/03/2018   1      2   \n",
       "2          Unknown  18/07/2018   4      1   \n",
       "3          Unknown  04/02/2019   5      2   \n",
       "4  Hillary Clinton  22/03/2016   6      2   \n",
       "\n",
       "                             related_articles  \\\n",
       "0            [122094, 122580, 130685, 134765]   \n",
       "1                    [106868, 127320, 128060]   \n",
       "2                    [132130, 132132, 149722]   \n",
       "3                    [123254, 123418, 127464]   \n",
       "4  [41099, 89899, 72543, 82644, 95344, 88361]   \n",
       "\n",
       "                                article_array   new_date  cont_days  Year  \\\n",
       "0            [122094, 122580, 130685, 134765] 2017-07-17      11520  2017   \n",
       "1                    [106868, 127320, 128060] 2018-03-17      11763  2018   \n",
       "2                    [132130, 132132, 149722] 2018-07-18      11886  2018   \n",
       "3                    [123254, 123418, 127464] 2019-02-04      12087  2019   \n",
       "4  [41099, 89899, 72543, 82644, 95344, 88361] 2016-03-22      11038  2016   \n",
       "\n",
       "   Month  \n",
       "0      7  \n",
       "1      3  \n",
       "2      7  \n",
       "3      2  \n",
       "4      3  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 final dataframes, one for the data (claims, claimant, date, label, related_articles) and another for the related articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Final Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all work done above to a single frame\n",
    "final_data = pd.concat([data.claim, cleaned_claim.claim, claims_.stemmed_claims, claims_.stemmed_stopword_claims, claims_.tokenized_claims, data.claimant, data.new_date, data.cont_days, data.Year, data.Month, cleaned_claim.label, cleaned_claim.article_array], axis=1)\n",
    "# rename columns for clarity\n",
    "final_data.columns = ['raw_claim', 'cleaned_claim', 'stemmed_claims', 'stemmed_stopword_claims', 'tokenized_claim', 'claimant', 'date', 'cont_days', 'year', 'month', 'label', 'article_array']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_claim</th>\n",
       "      <th>cleaned_claim</th>\n",
       "      <th>stemmed_claims</th>\n",
       "      <th>stemmed_stopword_claims</th>\n",
       "      <th>tokenized_claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>date</th>\n",
       "      <th>cont_days</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>label</th>\n",
       "      <th>article_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A line from George Orwell's novel 1984 predict...</td>\n",
       "      <td>a line from george orwells novel 1984 predicts...</td>\n",
       "      <td>a line from georg orwel novel 1984 predict the...</td>\n",
       "      <td>line georg orwel novel 1984 predict power smar...</td>\n",
       "      <td>[line, georg, orwel, novel, 1984, predict, pow...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>11520</td>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maine legislature candidate Leslie Gibson insu...</td>\n",
       "      <td>maine legislature candidate leslie gibson insu...</td>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>[main, legislatur, candid, lesli, gibson, insu...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>11763</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A 17-year-old girl named Alyssa Carson is bein...</td>\n",
       "      <td>a 17yearold girl named alyssa carson is being ...</td>\n",
       "      <td>a 17yearold girl name alyssa carson is be trai...</td>\n",
       "      <td>17yearold girl name alyssa carson train nasa b...</td>\n",
       "      <td>[17yearold, girl, name, alyssa, carson, train,...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2018-07-18</td>\n",
       "      <td>11886</td>\n",
       "      <td>2018</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In 1988 author Roald Dahl penned an open lette...</td>\n",
       "      <td>in 1988 author roald dahl penned an open lette...</td>\n",
       "      <td>in 1988 author roald dahl pen an open letter u...</td>\n",
       "      <td>1988 author roald dahl pen open letter urg par...</td>\n",
       "      <td>[1988, author, roald, dahl, pen, open, letter,...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>12087</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When it comes to fighting terrorism, \"Another ...</td>\n",
       "      <td>when it comes to fighting terrorism another th...</td>\n",
       "      <td>when it come to fight terror anoth thing we kn...</td>\n",
       "      <td>come fight terror anoth thing know doe work ba...</td>\n",
       "      <td>[come, fight, terror, anoth, thing, know, doe,...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>2016-03-22</td>\n",
       "      <td>11038</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           raw_claim  \\\n",
       "0  A line from George Orwell's novel 1984 predict...   \n",
       "1  Maine legislature candidate Leslie Gibson insu...   \n",
       "2  A 17-year-old girl named Alyssa Carson is bein...   \n",
       "3  In 1988 author Roald Dahl penned an open lette...   \n",
       "4  When it comes to fighting terrorism, \"Another ...   \n",
       "\n",
       "                                       cleaned_claim  \\\n",
       "0  a line from george orwells novel 1984 predicts...   \n",
       "1  maine legislature candidate leslie gibson insu...   \n",
       "2  a 17yearold girl named alyssa carson is being ...   \n",
       "3  in 1988 author roald dahl penned an open lette...   \n",
       "4  when it comes to fighting terrorism another th...   \n",
       "\n",
       "                                      stemmed_claims  \\\n",
       "0  a line from georg orwel novel 1984 predict the...   \n",
       "1  main legislatur candid lesli gibson insult par...   \n",
       "2  a 17yearold girl name alyssa carson is be trai...   \n",
       "3  in 1988 author roald dahl pen an open letter u...   \n",
       "4  when it come to fight terror anoth thing we kn...   \n",
       "\n",
       "                             stemmed_stopword_claims  \\\n",
       "0  line georg orwel novel 1984 predict power smar...   \n",
       "1  main legislatur candid lesli gibson insult par...   \n",
       "2  17yearold girl name alyssa carson train nasa b...   \n",
       "3  1988 author roald dahl pen open letter urg par...   \n",
       "4  come fight terror anoth thing know doe work ba...   \n",
       "\n",
       "                                     tokenized_claim         claimant  \\\n",
       "0  [line, georg, orwel, novel, 1984, predict, pow...          Unknown   \n",
       "1  [main, legislatur, candid, lesli, gibson, insu...          Unknown   \n",
       "2  [17yearold, girl, name, alyssa, carson, train,...          Unknown   \n",
       "3  [1988, author, roald, dahl, pen, open, letter,...          Unknown   \n",
       "4  [come, fight, terror, anoth, thing, know, doe,...  Hillary Clinton   \n",
       "\n",
       "        date  cont_days  year  month  label  \\\n",
       "0 2017-07-17      11520  2017      7      0   \n",
       "1 2018-03-17      11763  2018      3      2   \n",
       "2 2018-07-18      11886  2018      7      1   \n",
       "3 2019-02-04      12087  2019      2      2   \n",
       "4 2016-03-22      11038  2016      3      2   \n",
       "\n",
       "                                article_array  \n",
       "0            [122094, 122580, 130685, 134765]  \n",
       "1                    [106868, 127320, 128060]  \n",
       "2                    [132130, 132132, 149722]  \n",
       "3                    [123254, 123418, 127464]  \n",
       "4  [41099, 89899, 72543, 82644, 95344, 88361]  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is equivalent to the \"train.csv\" that we were given, but cleaned with a few additional feature\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save to csv\n",
    "# final_data.to_csv(\"final_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pkl\n",
    "final_data.to_pickle(\"./final_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 How to work with final_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaron = pd.read_pickle(\"./final_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to go to a specific cleaned claim\n",
    "aaron.cleaned_claim.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to go to a specific stemmed_stopword_claims\n",
    "aaron.stemmed_stopword_claims.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample code to read elements from each article_array\n",
    "\n",
    "# # to iterate the article array\n",
    "# for i in range(aaron.shape[0]):\n",
    "#     # i iterates row by row till the end\n",
    "#     for u in range(len(aaron.article_array[i])):\n",
    "#         # u holds the index of each element, within each array. Uncomment the following to understand\n",
    "#         # print(u)\n",
    "#         art_array = final_data.article_array[i]\n",
    "#         # print specific elements of each array\n",
    "#         print(art_array[u])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Related Articles Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_article_list = articles.Article.tolist()\n",
    "cleaned_article_list = articles_cleaned.Article.tolist()\n",
    "final_articles_zipped = list(zip(raw_article_list, cleaned_article_list, stemmed_art, stemmed_sw_art, tokenized_articles, final_article_sentences, final_article_tok_sentences))\n",
    "final_articles = pd.DataFrame(final_articles_zipped, columns = ['raw_articles', 'cleaned_articles', 'stemmed_articles', 'stemmed_stopwords_articles', 'tokenized_articles', 'cleaned_sentence', 'tokenized_cleaned_sentence'])\n",
    "final_articles.index = [articles_cleaned.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_articles</th>\n",
       "      <th>cleaned_articles</th>\n",
       "      <th>stemmed_articles</th>\n",
       "      <th>stemmed_stopwords_articles</th>\n",
       "      <th>tokenized_articles</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "      <th>tokenized_cleaned_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125385</th>\n",
       "      <td>Dr. Ben Carson: Welfare Benefactor?\\nAn initia...</td>\n",
       "      <td>dr ben carson welfare benefactor an initially ...</td>\n",
       "      <td>dr ben carson welfar benefactor an initi unlik...</td>\n",
       "      <td>dr ben carson welfar benefactor initi unlik cl...</td>\n",
       "      <td>[dr, ben, carson, welfar, benefactor, initi, u...</td>\n",
       "      <td>[dr ben carson welfar benefactor, initi unlik ...</td>\n",
       "      <td>[[dr, ben, carson, welfar, benefactor], [initi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32238</th>\n",
       "      <td>The World Factbook — Central Intelligence Agen...</td>\n",
       "      <td>the world factbook central intelligence agency...</td>\n",
       "      <td>the world factbook central intellig agenc the ...</td>\n",
       "      <td>world factbook central intellig agenc unit sta...</td>\n",
       "      <td>[world, factbook, central, intellig, agenc, un...</td>\n",
       "      <td>[world factbook central intellig agenc unit st...</td>\n",
       "      <td>[[world, factbook, central, intellig, agenc, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16051</th>\n",
       "      <td>1014 texaseducationagencypftexas\\n\\nEmails, La...</td>\n",
       "      <td>1014 texaseducationagencypftexas emails lauren...</td>\n",
       "      <td>1014 texaseducationagencypftexa email lauren c...</td>\n",
       "      <td>1014 texaseducationagencypftexa email lauren c...</td>\n",
       "      <td>[1014, texaseducationagencypftexa, email, laur...</td>\n",
       "      <td>[1014 texaseducationagencypftexa email lauren ...</td>\n",
       "      <td>[[1014, texaseducationagencypftexa, email, lau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118633</th>\n",
       "      <td>Clinton camp delays Weather Channel ad buy aft...</td>\n",
       "      <td>clinton camp delays weather channel ad buy aft...</td>\n",
       "      <td>clinton camp delay weather channel ad buy afte...</td>\n",
       "      <td>clinton camp delay weather channel ad buy back...</td>\n",
       "      <td>[clinton, camp, delay, weather, channel, ad, b...</td>\n",
       "      <td>[clinton camp delay weather channel ad buy bac...</td>\n",
       "      <td>[[clinton, camp, delay, weather, channel, ad, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117945</th>\n",
       "      <td>Living with kangaroos\\nKangaroos are appealing...</td>\n",
       "      <td>living with kangaroos kangaroos are appealing ...</td>\n",
       "      <td>live with kangaroo kangaroo are appeal wild an...</td>\n",
       "      <td>live kangaroo kangaroo appeal wild power nativ...</td>\n",
       "      <td>[live, kangaroo, kangaroo, appeal, wild, power...</td>\n",
       "      <td>[live kangaroo kangaroo appeal wild power nati...</td>\n",
       "      <td>[[live, kangaroo, kangaroo, appeal, wild, powe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             raw_articles  \\\n",
       "125385  Dr. Ben Carson: Welfare Benefactor?\\nAn initia...   \n",
       "32238   The World Factbook — Central Intelligence Agen...   \n",
       "16051   1014 texaseducationagencypftexas\\n\\nEmails, La...   \n",
       "118633  Clinton camp delays Weather Channel ad buy aft...   \n",
       "117945  Living with kangaroos\\nKangaroos are appealing...   \n",
       "\n",
       "                                         cleaned_articles  \\\n",
       "125385  dr ben carson welfare benefactor an initially ...   \n",
       "32238   the world factbook central intelligence agency...   \n",
       "16051   1014 texaseducationagencypftexas emails lauren...   \n",
       "118633  clinton camp delays weather channel ad buy aft...   \n",
       "117945  living with kangaroos kangaroos are appealing ...   \n",
       "\n",
       "                                         stemmed_articles  \\\n",
       "125385  dr ben carson welfar benefactor an initi unlik...   \n",
       "32238   the world factbook central intellig agenc the ...   \n",
       "16051   1014 texaseducationagencypftexa email lauren c...   \n",
       "118633  clinton camp delay weather channel ad buy afte...   \n",
       "117945  live with kangaroo kangaroo are appeal wild an...   \n",
       "\n",
       "                               stemmed_stopwords_articles  \\\n",
       "125385  dr ben carson welfar benefactor initi unlik cl...   \n",
       "32238   world factbook central intellig agenc unit sta...   \n",
       "16051   1014 texaseducationagencypftexa email lauren c...   \n",
       "118633  clinton camp delay weather channel ad buy back...   \n",
       "117945  live kangaroo kangaroo appeal wild power nativ...   \n",
       "\n",
       "                                       tokenized_articles  \\\n",
       "125385  [dr, ben, carson, welfar, benefactor, initi, u...   \n",
       "32238   [world, factbook, central, intellig, agenc, un...   \n",
       "16051   [1014, texaseducationagencypftexa, email, laur...   \n",
       "118633  [clinton, camp, delay, weather, channel, ad, b...   \n",
       "117945  [live, kangaroo, kangaroo, appeal, wild, power...   \n",
       "\n",
       "                                         cleaned_sentence  \\\n",
       "125385  [dr ben carson welfar benefactor, initi unlik ...   \n",
       "32238   [world factbook central intellig agenc unit st...   \n",
       "16051   [1014 texaseducationagencypftexa email lauren ...   \n",
       "118633  [clinton camp delay weather channel ad buy bac...   \n",
       "117945  [live kangaroo kangaroo appeal wild power nati...   \n",
       "\n",
       "                               tokenized_cleaned_sentence  \n",
       "125385  [[dr, ben, carson, welfar, benefactor], [initi...  \n",
       "32238   [[world, factbook, central, intellig, agenc, u...  \n",
       "16051   [[1014, texaseducationagencypftexa, email, lau...  \n",
       "118633  [[clinton, camp, delay, weather, channel, ad, ...  \n",
       "117945  [[live, kangaroo, kangaroo, appeal, wild, powe...  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_articles.to_pickle('./final_articles.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 How to work with final_articles.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_articles</th>\n",
       "      <th>cleaned_articles</th>\n",
       "      <th>stemmed_articles</th>\n",
       "      <th>stemmed_stopwords_articles</th>\n",
       "      <th>tokenized_articles</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "      <th>tokenized_cleaned_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125385</th>\n",
       "      <td>Dr. Ben Carson: Welfare Benefactor?\\nAn initia...</td>\n",
       "      <td>dr ben carson welfare benefactor an initially ...</td>\n",
       "      <td>dr ben carson welfar benefactor an initi unlik...</td>\n",
       "      <td>dr ben carson welfar benefactor initi unlik cl...</td>\n",
       "      <td>[dr, ben, carson, welfar, benefactor, initi, u...</td>\n",
       "      <td>[dr ben carson welfar benefactor, initi unlik ...</td>\n",
       "      <td>[[dr, ben, carson, welfar, benefactor], [initi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32238</th>\n",
       "      <td>The World Factbook — Central Intelligence Agen...</td>\n",
       "      <td>the world factbook central intelligence agency...</td>\n",
       "      <td>the world factbook central intellig agenc the ...</td>\n",
       "      <td>world factbook central intellig agenc unit sta...</td>\n",
       "      <td>[world, factbook, central, intellig, agenc, un...</td>\n",
       "      <td>[world factbook central intellig agenc unit st...</td>\n",
       "      <td>[[world, factbook, central, intellig, agenc, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16051</th>\n",
       "      <td>1014 texaseducationagencypftexas\\n\\nEmails, La...</td>\n",
       "      <td>1014 texaseducationagencypftexas emails lauren...</td>\n",
       "      <td>1014 texaseducationagencypftexa email lauren c...</td>\n",
       "      <td>1014 texaseducationagencypftexa email lauren c...</td>\n",
       "      <td>[1014, texaseducationagencypftexa, email, laur...</td>\n",
       "      <td>[1014 texaseducationagencypftexa email lauren ...</td>\n",
       "      <td>[[1014, texaseducationagencypftexa, email, lau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118633</th>\n",
       "      <td>Clinton camp delays Weather Channel ad buy aft...</td>\n",
       "      <td>clinton camp delays weather channel ad buy aft...</td>\n",
       "      <td>clinton camp delay weather channel ad buy afte...</td>\n",
       "      <td>clinton camp delay weather channel ad buy back...</td>\n",
       "      <td>[clinton, camp, delay, weather, channel, ad, b...</td>\n",
       "      <td>[clinton camp delay weather channel ad buy bac...</td>\n",
       "      <td>[[clinton, camp, delay, weather, channel, ad, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117945</th>\n",
       "      <td>Living with kangaroos\\nKangaroos are appealing...</td>\n",
       "      <td>living with kangaroos kangaroos are appealing ...</td>\n",
       "      <td>live with kangaroo kangaroo are appeal wild an...</td>\n",
       "      <td>live kangaroo kangaroo appeal wild power nativ...</td>\n",
       "      <td>[live, kangaroo, kangaroo, appeal, wild, power...</td>\n",
       "      <td>[live kangaroo kangaroo appeal wild power nati...</td>\n",
       "      <td>[[live, kangaroo, kangaroo, appeal, wild, powe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             raw_articles  \\\n",
       "125385  Dr. Ben Carson: Welfare Benefactor?\\nAn initia...   \n",
       "32238   The World Factbook — Central Intelligence Agen...   \n",
       "16051   1014 texaseducationagencypftexas\\n\\nEmails, La...   \n",
       "118633  Clinton camp delays Weather Channel ad buy aft...   \n",
       "117945  Living with kangaroos\\nKangaroos are appealing...   \n",
       "\n",
       "                                         cleaned_articles  \\\n",
       "125385  dr ben carson welfare benefactor an initially ...   \n",
       "32238   the world factbook central intelligence agency...   \n",
       "16051   1014 texaseducationagencypftexas emails lauren...   \n",
       "118633  clinton camp delays weather channel ad buy aft...   \n",
       "117945  living with kangaroos kangaroos are appealing ...   \n",
       "\n",
       "                                         stemmed_articles  \\\n",
       "125385  dr ben carson welfar benefactor an initi unlik...   \n",
       "32238   the world factbook central intellig agenc the ...   \n",
       "16051   1014 texaseducationagencypftexa email lauren c...   \n",
       "118633  clinton camp delay weather channel ad buy afte...   \n",
       "117945  live with kangaroo kangaroo are appeal wild an...   \n",
       "\n",
       "                               stemmed_stopwords_articles  \\\n",
       "125385  dr ben carson welfar benefactor initi unlik cl...   \n",
       "32238   world factbook central intellig agenc unit sta...   \n",
       "16051   1014 texaseducationagencypftexa email lauren c...   \n",
       "118633  clinton camp delay weather channel ad buy back...   \n",
       "117945  live kangaroo kangaroo appeal wild power nativ...   \n",
       "\n",
       "                                       tokenized_articles  \\\n",
       "125385  [dr, ben, carson, welfar, benefactor, initi, u...   \n",
       "32238   [world, factbook, central, intellig, agenc, un...   \n",
       "16051   [1014, texaseducationagencypftexa, email, laur...   \n",
       "118633  [clinton, camp, delay, weather, channel, ad, b...   \n",
       "117945  [live, kangaroo, kangaroo, appeal, wild, power...   \n",
       "\n",
       "                                         cleaned_sentence  \\\n",
       "125385  [dr ben carson welfar benefactor, initi unlik ...   \n",
       "32238   [world factbook central intellig agenc unit st...   \n",
       "16051   [1014 texaseducationagencypftexa email lauren ...   \n",
       "118633  [clinton camp delay weather channel ad buy bac...   \n",
       "117945  [live kangaroo kangaroo appeal wild power nati...   \n",
       "\n",
       "                               tokenized_cleaned_sentence  \n",
       "125385  [[dr, ben, carson, welfar, benefactor], [initi...  \n",
       "32238   [[world, factbook, central, intellig, agenc, u...  \n",
       "16051   [[1014, texaseducationagencypftexa, email, lau...  \n",
       "118633  [[clinton, camp, delay, weather, channel, ad, ...  \n",
       "117945  [[live, kangaroo, kangaroo, appeal, wild, powe...  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the csv\n",
    "aaron = pd.read_pickle('./final_articles.pkl')\n",
    "aaron.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'charterschool bill deserves action ohio senate president keith faber who is scouting a run for state auditor or attorney general in two years has a chance to demonstrate an affinity for defending ohioans against misspending and fraud by advancing a charterschool accountability bill ohio senate president keith faber who is scouting a run for state auditor or attorney general in two years has a chance to demonstrate an affinity for defending ohioans against misspending and fraud by advancing a charterschool accountability bill as it stands it appears that fabers senate is playing games with senate bill 298 a measure to ensure that charter schools are actually educating students this crucial reform legislation was oddly assigned to the senate finance committee rather than being sent to the education committee where sen peggy lehner rkettering had made it clear that shed fasttrack the bill senate minority leader joe schiavoni dboardman reasonably suspects his bill is being slotted for inaction the finance committee typically deals with state spending which sb 298 does not schiavonis bill deserves to be promptly heard by a committee revised as necessary and sent to the senate floor for a vote it aims to make certain that 39000 students attending ohios online schools are getting an education in exchange for 275 million in taxpayer subsidies it is natural to question these payments after a state audit found several eschools couldnt document that kids were doing even minimal schoolwork sb 298 includes provisions such as requiring online schools to keep accurate records that show students are participating in coursework and report that data monthly to the state a skeptic might think that faber is trying to block schiavioni an upandcoming democrat from achieving an important reform this is the second time in six months schiavoni said that one of his bills has been placed in the finance committee after lehner agreed to hold hearings the earlier bill to help youngstowns troubled school district has yet to have a hearing faber explained the finance committee assignment by saying one of the big issues in school funding is how you count kids he said yes that is a big issue especially for wellheeled gop donors who own charter schools and who have an interest in stalling reforms the charters are pushing the state to count only the number of hours offered students and not those that students actually participate in sorry but good intentions dont count taxpayers expect results faber promises the eschool attendance bill will get some hearings faber has an ideal opportunity here to show integrity in protecting children and taxpayers lehner a leader in educational accountability already has'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to go to a specific cleaned article\n",
    "aaron.cleaned_articles.loc['75770'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['charterschool',\n",
       "  'bill',\n",
       "  'deserv',\n",
       "  'action',\n",
       "  'ohio',\n",
       "  'senat',\n",
       "  'presid',\n",
       "  'keith',\n",
       "  'faber',\n",
       "  'scout',\n",
       "  'run',\n",
       "  'state',\n",
       "  'auditor',\n",
       "  'attorney',\n",
       "  'gener',\n",
       "  'two',\n",
       "  'year',\n",
       "  'ha',\n",
       "  'chanc',\n",
       "  'demonstr',\n",
       "  'affin',\n",
       "  'defend',\n",
       "  'ohioan',\n",
       "  'misspend',\n",
       "  'fraud',\n",
       "  'advanc',\n",
       "  'charterschool',\n",
       "  'account',\n",
       "  'bill'],\n",
       " ['ohio',\n",
       "  'senat',\n",
       "  'presid',\n",
       "  'keith',\n",
       "  'faber',\n",
       "  'scout',\n",
       "  'run',\n",
       "  'state',\n",
       "  'auditor',\n",
       "  'attorney',\n",
       "  'gener',\n",
       "  'two',\n",
       "  'year',\n",
       "  'ha',\n",
       "  'chanc',\n",
       "  'demonstr',\n",
       "  'affin',\n",
       "  'defend',\n",
       "  'ohioan',\n",
       "  'misspend',\n",
       "  'fraud',\n",
       "  'advanc',\n",
       "  'charterschool',\n",
       "  'account',\n",
       "  'bill'],\n",
       " ['stand',\n",
       "  'appear',\n",
       "  'faber',\n",
       "  'senat',\n",
       "  'play',\n",
       "  'game',\n",
       "  'senat',\n",
       "  'bill',\n",
       "  '298',\n",
       "  'measur',\n",
       "  'ensur',\n",
       "  'charter',\n",
       "  'school',\n",
       "  'actual',\n",
       "  'educ',\n",
       "  'student'],\n",
       " ['thi',\n",
       "  'crucial',\n",
       "  'reform',\n",
       "  'legisl',\n",
       "  'wa',\n",
       "  'oddli',\n",
       "  'assign',\n",
       "  'senat',\n",
       "  'financ',\n",
       "  'committe',\n",
       "  'rather',\n",
       "  'sent',\n",
       "  'educ',\n",
       "  'committe',\n",
       "  'sen',\n",
       "  'peggi',\n",
       "  'lehner',\n",
       "  'rketter',\n",
       "  'made',\n",
       "  'clear',\n",
       "  'shed',\n",
       "  'fasttrack',\n",
       "  'bill'],\n",
       " ['senat',\n",
       "  'minor',\n",
       "  'leader',\n",
       "  'joe',\n",
       "  'schiavoni',\n",
       "  'dboardman',\n",
       "  'reason',\n",
       "  'suspect',\n",
       "  'hi',\n",
       "  'bill',\n",
       "  'slot',\n",
       "  'inact'],\n",
       " ['financ', 'committe', 'typic', 'deal', 'state', 'spend', 'sb'],\n",
       " ['298', 'doe'],\n",
       " ['schiavoni',\n",
       "  'bill',\n",
       "  'deserv',\n",
       "  'promptli',\n",
       "  'heard',\n",
       "  'committe',\n",
       "  'revis',\n",
       "  'necessari',\n",
       "  'sent',\n",
       "  'senat',\n",
       "  'floor',\n",
       "  'vote'],\n",
       " ['aim',\n",
       "  'make',\n",
       "  'certain',\n",
       "  '39000',\n",
       "  'student',\n",
       "  'attend',\n",
       "  'ohio',\n",
       "  'onlin',\n",
       "  'school',\n",
       "  'get',\n",
       "  'educ',\n",
       "  'exchang',\n",
       "  '275',\n",
       "  'million',\n",
       "  'taxpay',\n",
       "  'subsidi'],\n",
       " ['natur',\n",
       "  'question',\n",
       "  'payment',\n",
       "  'state',\n",
       "  'audit',\n",
       "  'found',\n",
       "  'sever',\n",
       "  'eschool',\n",
       "  'couldnt',\n",
       "  'document',\n",
       "  'kid',\n",
       "  'even',\n",
       "  'minim',\n",
       "  'schoolwork'],\n",
       " ['sb'],\n",
       " ['298',\n",
       "  'includ',\n",
       "  'provis',\n",
       "  'requir',\n",
       "  'onlin',\n",
       "  'school',\n",
       "  'keep',\n",
       "  'accur',\n",
       "  'record',\n",
       "  'show',\n",
       "  'student',\n",
       "  'particip',\n",
       "  'coursework',\n",
       "  'report',\n",
       "  'data',\n",
       "  'monthli',\n",
       "  'state'],\n",
       " ['skeptic',\n",
       "  'might',\n",
       "  'think',\n",
       "  'faber',\n",
       "  'tri',\n",
       "  'block',\n",
       "  'schiavioni',\n",
       "  'upandcom',\n",
       "  'democrat',\n",
       "  'achiev',\n",
       "  'import',\n",
       "  'reform'],\n",
       " ['thi',\n",
       "  'second',\n",
       "  'time',\n",
       "  'six',\n",
       "  'month',\n",
       "  'schiavoni',\n",
       "  'said',\n",
       "  'one',\n",
       "  'hi',\n",
       "  'bill',\n",
       "  'ha',\n",
       "  'place',\n",
       "  'financ',\n",
       "  'committe',\n",
       "  'lehner',\n",
       "  'agre',\n",
       "  'hold',\n",
       "  'hear',\n",
       "  'earlier',\n",
       "  'bill',\n",
       "  'help',\n",
       "  'youngstown',\n",
       "  'troubl',\n",
       "  'school',\n",
       "  'district',\n",
       "  'ha',\n",
       "  'yet',\n",
       "  'hear'],\n",
       " ['faber',\n",
       "  'explain',\n",
       "  'financ',\n",
       "  'committe',\n",
       "  'assign',\n",
       "  'say',\n",
       "  'one',\n",
       "  'big',\n",
       "  'issu',\n",
       "  'school',\n",
       "  'fund',\n",
       "  'count',\n",
       "  'kid',\n",
       "  'said'],\n",
       " ['ye',\n",
       "  'big',\n",
       "  'issu',\n",
       "  'especi',\n",
       "  'wellheel',\n",
       "  'gop',\n",
       "  'donor',\n",
       "  'charter',\n",
       "  'school',\n",
       "  'interest',\n",
       "  'stall',\n",
       "  'reform'],\n",
       " ['charter',\n",
       "  'push',\n",
       "  'state',\n",
       "  'count',\n",
       "  'onli',\n",
       "  'number',\n",
       "  'hour',\n",
       "  'offer',\n",
       "  'student',\n",
       "  'student',\n",
       "  'actual',\n",
       "  'particip'],\n",
       " ['sorri', 'good', 'intent', 'dont', 'count'],\n",
       " ['taxpay', 'expect', 'result'],\n",
       " ['faber', 'promis', 'eschool', 'attend', 'bill', 'get', 'hear'],\n",
       " ['faber',\n",
       "  'ha',\n",
       "  'ideal',\n",
       "  'opportun',\n",
       "  'show',\n",
       "  'integr',\n",
       "  'protect',\n",
       "  'children',\n",
       "  'taxpay'],\n",
       " ['lehner', 'leader', 'educ', 'account', 'alreadi', 'ha']]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to go to a specific raw article\n",
    "aaron.tokenized_cleaned_sentence.loc['75770'].iloc[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
