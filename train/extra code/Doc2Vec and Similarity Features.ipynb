{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec and Similarity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "import math\n",
    "from IPython.display import clear_output, display\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import string\n",
    "import heapq\n",
    "import numpy\n",
    "from heapq import nlargest\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "data = pd.read_pickle(\"./final_data.pkl\")\n",
    "articles = pd.read_pickle('./final_articles.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_claim</th>\n",
       "      <th>cleaned_claim</th>\n",
       "      <th>stemmed_claims</th>\n",
       "      <th>stemmed_stopword_claims</th>\n",
       "      <th>tokenized_claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>date</th>\n",
       "      <th>cont_days</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>label</th>\n",
       "      <th>article_array</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A line from George Orwell's novel 1984 predict...</td>\n",
       "      <td>a line from george orwells novel 1984 predicts...</td>\n",
       "      <td>a line from georg orwel novel 1984 predict the...</td>\n",
       "      <td>line georg orwel novel 1984 predict power smar...</td>\n",
       "      <td>[line, georg, orwel, novel, 1984, predict, pow...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>11520</td>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maine legislature candidate Leslie Gibson insu...</td>\n",
       "      <td>maine legislature candidate leslie gibson insu...</td>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>[main, legislatur, candid, lesli, gibson, insu...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>11763</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A 17-year-old girl named Alyssa Carson is bein...</td>\n",
       "      <td>a 17yearold girl named alyssa carson is being ...</td>\n",
       "      <td>a 17yearold girl name alyssa carson is be trai...</td>\n",
       "      <td>17yearold girl name alyssa carson train nasa b...</td>\n",
       "      <td>[17yearold, girl, name, alyssa, carson, train,...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2018-07-18</td>\n",
       "      <td>11886</td>\n",
       "      <td>2018</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In 1988 author Roald Dahl penned an open lette...</td>\n",
       "      <td>in 1988 author roald dahl penned an open lette...</td>\n",
       "      <td>in 1988 author roald dahl pen an open letter u...</td>\n",
       "      <td>1988 author roald dahl pen open letter urg par...</td>\n",
       "      <td>[1988, author, roald, dahl, pen, open, letter,...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>12087</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When it comes to fighting terrorism, \"Another ...</td>\n",
       "      <td>when it comes to fighting terrorism another th...</td>\n",
       "      <td>when it come to fight terror anoth thing we kn...</td>\n",
       "      <td>come fight terror anoth thing know doe work ba...</td>\n",
       "      <td>[come, fight, terror, anoth, thing, know, doe,...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>2016-03-22</td>\n",
       "      <td>11038</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           raw_claim  \\\n",
       "0  A line from George Orwell's novel 1984 predict...   \n",
       "1  Maine legislature candidate Leslie Gibson insu...   \n",
       "2  A 17-year-old girl named Alyssa Carson is bein...   \n",
       "3  In 1988 author Roald Dahl penned an open lette...   \n",
       "4  When it comes to fighting terrorism, \"Another ...   \n",
       "\n",
       "                                       cleaned_claim  \\\n",
       "0  a line from george orwells novel 1984 predicts...   \n",
       "1  maine legislature candidate leslie gibson insu...   \n",
       "2  a 17yearold girl named alyssa carson is being ...   \n",
       "3  in 1988 author roald dahl penned an open lette...   \n",
       "4  when it comes to fighting terrorism another th...   \n",
       "\n",
       "                                      stemmed_claims  \\\n",
       "0  a line from georg orwel novel 1984 predict the...   \n",
       "1  main legislatur candid lesli gibson insult par...   \n",
       "2  a 17yearold girl name alyssa carson is be trai...   \n",
       "3  in 1988 author roald dahl pen an open letter u...   \n",
       "4  when it come to fight terror anoth thing we kn...   \n",
       "\n",
       "                             stemmed_stopword_claims  \\\n",
       "0  line georg orwel novel 1984 predict power smar...   \n",
       "1  main legislatur candid lesli gibson insult par...   \n",
       "2  17yearold girl name alyssa carson train nasa b...   \n",
       "3  1988 author roald dahl pen open letter urg par...   \n",
       "4  come fight terror anoth thing know doe work ba...   \n",
       "\n",
       "                                     tokenized_claim         claimant  \\\n",
       "0  [line, georg, orwel, novel, 1984, predict, pow...          Unknown   \n",
       "1  [main, legislatur, candid, lesli, gibson, insu...          Unknown   \n",
       "2  [17yearold, girl, name, alyssa, carson, train,...          Unknown   \n",
       "3  [1988, author, roald, dahl, pen, open, letter,...          Unknown   \n",
       "4  [come, fight, terror, anoth, thing, know, doe,...  Hillary Clinton   \n",
       "\n",
       "        date  cont_days  year  month  label  \\\n",
       "0 2017-07-17      11520  2017      7      0   \n",
       "1 2018-03-17      11763  2018      3      2   \n",
       "2 2018-07-18      11886  2018      7      1   \n",
       "3 2019-02-04      12087  2019      2      2   \n",
       "4 2016-03-22      11038  2016      3      2   \n",
       "\n",
       "                                article_array  \n",
       "0            [122094, 122580, 130685, 134765]  \n",
       "1                    [106868, 127320, 128060]  \n",
       "2                    [132130, 132132, 149722]  \n",
       "3                    [123254, 123418, 127464]  \n",
       "4  [41099, 89899, 72543, 82644, 95344, 88361]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_articles</th>\n",
       "      <th>cleaned_articles</th>\n",
       "      <th>stemmed_articles</th>\n",
       "      <th>stemmed_stopwords_articles</th>\n",
       "      <th>tokenized_articles</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "      <th>tokenized_cleaned_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>125385</th>\n",
       "      <td>Dr. Ben Carson: Welfare Benefactor?\\nAn initia...</td>\n",
       "      <td>dr ben carson welfare benefactor an initially ...</td>\n",
       "      <td>dr ben carson welfar benefactor an initi unlik...</td>\n",
       "      <td>dr ben carson welfar benefactor initi unlik cl...</td>\n",
       "      <td>[dr, ben, carson, welfar, benefactor, initi, u...</td>\n",
       "      <td>[dr ben carson welfar benefactor, initi unlik ...</td>\n",
       "      <td>[[dr, ben, carson, welfar, benefactor], [initi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32238</th>\n",
       "      <td>The World Factbook — Central Intelligence Agen...</td>\n",
       "      <td>the world factbook central intelligence agency...</td>\n",
       "      <td>the world factbook central intellig agenc the ...</td>\n",
       "      <td>world factbook central intellig agenc unit sta...</td>\n",
       "      <td>[world, factbook, central, intellig, agenc, un...</td>\n",
       "      <td>[world factbook central intellig agenc unit st...</td>\n",
       "      <td>[[world, factbook, central, intellig, agenc, u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16051</th>\n",
       "      <td>1014 texaseducationagencypftexas\\n\\nEmails, La...</td>\n",
       "      <td>1014 texaseducationagencypftexas emails lauren...</td>\n",
       "      <td>1014 texaseducationagencypftexa email lauren c...</td>\n",
       "      <td>1014 texaseducationagencypftexa email lauren c...</td>\n",
       "      <td>[1014, texaseducationagencypftexa, email, laur...</td>\n",
       "      <td>[1014 texaseducationagencypftexa email lauren ...</td>\n",
       "      <td>[[1014, texaseducationagencypftexa, email, lau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118633</th>\n",
       "      <td>Clinton camp delays Weather Channel ad buy aft...</td>\n",
       "      <td>clinton camp delays weather channel ad buy aft...</td>\n",
       "      <td>clinton camp delay weather channel ad buy afte...</td>\n",
       "      <td>clinton camp delay weather channel ad buy back...</td>\n",
       "      <td>[clinton, camp, delay, weather, channel, ad, b...</td>\n",
       "      <td>[clinton camp delay weather channel ad buy bac...</td>\n",
       "      <td>[[clinton, camp, delay, weather, channel, ad, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117945</th>\n",
       "      <td>Living with kangaroos\\nKangaroos are appealing...</td>\n",
       "      <td>living with kangaroos kangaroos are appealing ...</td>\n",
       "      <td>live with kangaroo kangaroo are appeal wild an...</td>\n",
       "      <td>live kangaroo kangaroo appeal wild power nativ...</td>\n",
       "      <td>[live, kangaroo, kangaroo, appeal, wild, power...</td>\n",
       "      <td>[live kangaroo kangaroo appeal wild power nati...</td>\n",
       "      <td>[[live, kangaroo, kangaroo, appeal, wild, powe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             raw_articles  \\\n",
       "125385  Dr. Ben Carson: Welfare Benefactor?\\nAn initia...   \n",
       "32238   The World Factbook — Central Intelligence Agen...   \n",
       "16051   1014 texaseducationagencypftexas\\n\\nEmails, La...   \n",
       "118633  Clinton camp delays Weather Channel ad buy aft...   \n",
       "117945  Living with kangaroos\\nKangaroos are appealing...   \n",
       "\n",
       "                                         cleaned_articles  \\\n",
       "125385  dr ben carson welfare benefactor an initially ...   \n",
       "32238   the world factbook central intelligence agency...   \n",
       "16051   1014 texaseducationagencypftexas emails lauren...   \n",
       "118633  clinton camp delays weather channel ad buy aft...   \n",
       "117945  living with kangaroos kangaroos are appealing ...   \n",
       "\n",
       "                                         stemmed_articles  \\\n",
       "125385  dr ben carson welfar benefactor an initi unlik...   \n",
       "32238   the world factbook central intellig agenc the ...   \n",
       "16051   1014 texaseducationagencypftexa email lauren c...   \n",
       "118633  clinton camp delay weather channel ad buy afte...   \n",
       "117945  live with kangaroo kangaroo are appeal wild an...   \n",
       "\n",
       "                               stemmed_stopwords_articles  \\\n",
       "125385  dr ben carson welfar benefactor initi unlik cl...   \n",
       "32238   world factbook central intellig agenc unit sta...   \n",
       "16051   1014 texaseducationagencypftexa email lauren c...   \n",
       "118633  clinton camp delay weather channel ad buy back...   \n",
       "117945  live kangaroo kangaroo appeal wild power nativ...   \n",
       "\n",
       "                                       tokenized_articles  \\\n",
       "125385  [dr, ben, carson, welfar, benefactor, initi, u...   \n",
       "32238   [world, factbook, central, intellig, agenc, un...   \n",
       "16051   [1014, texaseducationagencypftexa, email, laur...   \n",
       "118633  [clinton, camp, delay, weather, channel, ad, b...   \n",
       "117945  [live, kangaroo, kangaroo, appeal, wild, power...   \n",
       "\n",
       "                                         cleaned_sentence  \\\n",
       "125385  [dr ben carson welfar benefactor, initi unlik ...   \n",
       "32238   [world factbook central intellig agenc unit st...   \n",
       "16051   [1014 texaseducationagencypftexa email lauren ...   \n",
       "118633  [clinton camp delay weather channel ad buy bac...   \n",
       "117945  [live kangaroo kangaroo appeal wild power nati...   \n",
       "\n",
       "                               tokenized_cleaned_sentence  \n",
       "125385  [[dr, ben, carson, welfar, benefactor], [initi...  \n",
       "32238   [[world, factbook, central, intellig, agenc, u...  \n",
       "16051   [[1014, texaseducationagencypftexa, email, lau...  \n",
       "118633  [[clinton, camp, delay, weather, channel, ad, ...  \n",
       "117945  [[live, kangaroo, kangaroo, appeal, wild, powe...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from scipy import spatial\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# 2 minutes to run\n",
    "# # create a label for every sentence based on article ID (eg. 12345-1, 12345-2, 12345-3 ... etc.)\n",
    "\n",
    "# full_sentences_ID = []\n",
    "# for i in range(articles.shape[0]):\n",
    "#     sentence_ID_list = []\n",
    "#     sentence_number = 0\n",
    "#     sentences = articles.tokenized_cleaned_sentence.loc[articles.index[i]]\n",
    "#     for u in range(len(sentences)):\n",
    "#         sentence_ID = str(articles.index[i][0]) +  '-' + str(sentence_number)\n",
    "#         sentence_number += 1\n",
    "#         sentence_ID_list.append(sentence_ID)\n",
    "#     full_sentences_ID.append(sentence_ID_list)\n",
    "    \n",
    "#     # print progress\n",
    "#     progress = round((i/articles.shape[0])*100,2)\n",
    "#     clear_output(wait=True)\n",
    "#     print(\"progress: \" + str(progress) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine the tokenized sentences with their ID\n",
    "# articles_tok_sent = articles.tokenized_cleaned_sentence.to_frame()\n",
    "# articles_tok_sent['sentence_ID'] = full_sentences_ID\n",
    "# articles_tok_sent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # combine the sentence list and index list ~ takes 1.5 hours to run\n",
    "\n",
    "# article_sent_list = []\n",
    "# article_sent_ID = []\n",
    "\n",
    "# for i in range(articles_tok_sent.shape[0]):\n",
    "#     one_article_sent = articles_tok_sent.tokenized_cleaned_sentence.loc[articles.index[i]]\n",
    "#     article_sent_list = article_sent_list + one_article_sent\n",
    "#     one_article_ID = articles_tok_sent.sentence_ID.loc[articles.index[i]]\n",
    "#     article_sent_ID = article_sent_ID + one_article_ID\n",
    "    \n",
    "#     # print progress\n",
    "#     progress = round((i/articles_tok_sent.shape[0])*100,2)\n",
    "#     clear_output(wait=True)\n",
    "#     print(\"progress: \" + str(progress) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a data frame of the independent sentences and ID\n",
    "# # create copies\n",
    "# l1 = article_sent_list\n",
    "# l2 = article_sent_ID\n",
    "# # make dataframe\n",
    "# article_information = pd.Series(l2).to_frame()\n",
    "# article_information.columns = ['sentence_ID']\n",
    "# article_information['sentences'] = l1\n",
    "# # display it\n",
    "# article_information.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save it to pickle\n",
    "# article_information.to_pickle(\"./article_sentences_ind.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Reading Pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_pkl = pd.read_pickle(\"./article_sentences_ind.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_ID</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125385-0</td>\n",
       "      <td>[dr, ben, carson, welfar, benefactor]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125385-1</td>\n",
       "      <td>[initi, unlik, clearli, formid, contend, 2016,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>125385-2</td>\n",
       "      <td>[unlik, chri, christi, rand, paul, mike, hucka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>125385-3</td>\n",
       "      <td>[carson, becam, somewhat, overnight, sensat, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125385-4</td>\n",
       "      <td>[earliest, version, meme, date, least, decemb,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_ID                                          sentences\n",
       "0    125385-0              [dr, ben, carson, welfar, benefactor]\n",
       "1    125385-1  [initi, unlik, clearli, formid, contend, 2016,...\n",
       "2    125385-2  [unlik, chri, christi, rand, paul, mike, hucka...\n",
       "3    125385-3  [carson, becam, somewhat, overnight, sensat, f...\n",
       "4    125385-4  [earliest, version, meme, date, least, decemb,..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_pkl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the lists from the dataframes\n",
    "sentence_ID_list = sentences_pkl.sentence_ID.tolist()\n",
    "sentence_list = sentences_pkl.sentences.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # average sentence size\n",
    "# full_sent_size = []\n",
    "# for i in range(sentences_pkl.shape[0]):\n",
    "#     sentence_size = len(sentence_list[i])\n",
    "#     full_sent_size.append(sentence_size)\n",
    "# # sum(full_sent_size)/len(full_sent_size)\n",
    "# max(full_sent_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Weird Article "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_sent_size.index(max(full_sent_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_pkl.loc[329509].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 418 ms, sys: 16.4 ms, total: 435 ms\n",
      "Wall time: 434 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "    \n",
    "# create a list of claim index ranging from 0 - 15554\n",
    "claim_range = range(0, 15555)\n",
    "claim_index = []\n",
    "for i in claim_range:\n",
    "    claim_index.append(i)\n",
    "\n",
    "# create a list of lists of data.tokenized_claim\n",
    "tokenized_claims = []\n",
    "for i in range(data.shape[0]):\n",
    "    tokenized_claims.append(data.tokenized_claim[i])\n",
    "    \n",
    "# convert claim index to string to match the sentences ID\n",
    "claim_str_index = list(map(str, claim_index))\n",
    "\n",
    "# form full list for training\n",
    "full_text_list = sentence_list + tokenized_claims\n",
    "full_ID_list = sentence_ID_list + claim_str_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine above 2 lists in to a dictionary for debugging\n",
    "complete_data = dict(zip(full_ID_list, full_text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['line', 'georg', 'orwel', 'novel', '1984', 'predict', 'power', 'smartphon']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_data['0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.2 s, sys: 274 ms, total: 13.5 s\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# #create tagged data to train doc2vec w claim\n",
    "# tagged_data = [TaggedDocument(words=full_text_list[i], tags=[full_ID_list[i]]) for i in range(len(full_text_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved\n",
      "CPU times: user 6h 52min 13s, sys: 54min 40s, total: 7h 46min 53s\n",
      "Wall time: 5h 36min 24s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# # 6 hours to run\n",
    "# import multiprocessing\n",
    "# n_cpu = multiprocessing.cpu_count()\n",
    "# # setup training\n",
    "# vec_size = 150\n",
    "# model = Doc2Vec(dm = 1, vector_size = vec_size, min_count = 2, workers = n_cpu, epochs=30)\n",
    "# # build vocab\n",
    "# model.build_vocab(tagged_data)\n",
    "# # train\n",
    "# model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "# # save model\n",
    "# model.save(\"30epoch_150vec.model\")\n",
    "# print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Find Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.5 s, sys: 2.82 s, total: 14.3 s\n",
      "Wall time: 16.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load the model\n",
    "model = Doc2Vec.load(\"30epoch_150vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_ID</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>125385-0</td>\n",
       "      <td>[dr, ben, carson, welfar, benefactor]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125385-1</td>\n",
       "      <td>[initi, unlik, clearli, formid, contend, 2016,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>125385-2</td>\n",
       "      <td>[unlik, chri, christi, rand, paul, mike, hucka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>125385-3</td>\n",
       "      <td>[carson, becam, somewhat, overnight, sensat, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125385-4</td>\n",
       "      <td>[earliest, version, meme, date, least, decemb,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_ID                                          sentences\n",
       "0    125385-0              [dr, ben, carson, welfar, benefactor]\n",
       "1    125385-1  [initi, unlik, clearli, formid, contend, 2016,...\n",
       "2    125385-2  [unlik, chri, christi, rand, paul, mike, hucka...\n",
       "3    125385-3  [carson, becam, somewhat, overnight, sensat, f...\n",
       "4    125385-4  [earliest, version, meme, date, least, decemb,..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_pkl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Finding Top 5 Sentences per Related Articles per Claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 99.99%\n",
      "CPU times: user 6min, sys: 5.12 s, total: 6min 5s\n",
      "Wall time: 6min\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# # ~ 6 minutes to run\n",
    "# # find the top 5 sentence ID from each related article with the highest similarity score with the claim\n",
    "\n",
    "# final_best_sentences_ID = []\n",
    "# for i in range(data.shape[0]): # iterating over every claim\n",
    "#     one_claim_array = data.article_array.loc[i]\n",
    "#     best_sentences_one_claim = []\n",
    "#     for u in range(len(one_claim_array)): # iterating every article of one claim\n",
    "#         one_article_ID = one_claim_array[u]\n",
    "#         sent_list = articles.tokenized_cleaned_sentence.loc[str(one_article_ID)].iloc[0]\n",
    "#         one_article_sim_list=[]\n",
    "#         for y in range(len(sent_list)): #iterating over every sentence of one article\n",
    "#             v1 = model.docvecs[str(i)]\n",
    "#             sentence_number = str(str(one_article_ID) + '-' + str(y))\n",
    "#             v2 = model.docvecs[sentence_number]\n",
    "#             similarity = 1 - spatial.distance.cosine(v1, v2)\n",
    "#             one_article_sim_list.append(similarity)\n",
    "# #             print(\"ID: \" + str(i) + \" \" + str(result))\n",
    "#         a = numpy.array(one_article_sim_list)\n",
    "#         best_sentences_one_article = heapq.nlargest(5, range(len(a)), a.take)   \n",
    "#         best_sentences_one_claim.append(best_sentences_one_article)\n",
    "#     final_best_sentences_ID.append(best_sentences_one_claim)\n",
    "#     # print progress\n",
    "#     progress = round((i/data.shape[0])*100,2)\n",
    "#     clear_output(wait=True)\n",
    "#     print(\"progress: \" + str(progress) + \"%\")\n",
    "\n",
    "# # add it to the data frame\n",
    "# data['best_sentences_ID'] = final_best_sentences_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 99.99%\n",
      "CPU times: user 1min 11s, sys: 3.31 s, total: 1min 14s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# # ~ 2 minuts to run\n",
    "# # calculate the top 5 similarity scores\n",
    "\n",
    "# end = data.shape[0]\n",
    "# # end = 2\n",
    "# avg_sim_for_one_claim_list = []\n",
    "# avg_sim_one_claim = []\n",
    "# full_sentence_ID = []\n",
    "# for i in range(end): # per row\n",
    "#     one_claim_articles_ID = data.article_array.loc[i]\n",
    "#     one_claim_sentences_ID = data.best_sentences_ID.loc[i]\n",
    "#     avg_sim_for_one_article = []\n",
    "#     article_sentence_ID = []\n",
    "# #     print(\"claim: \" + str(i))\n",
    "#     for u in range(len(one_claim_articles_ID)): # article_array index\n",
    "#         sim_for_one_article = []\n",
    "#         sentence_ID_list = []\n",
    "#         for y in range(len(one_claim_sentences_ID[u])): # sentence ID index      \n",
    "#             sentence_ID = str(one_claim_articles_ID[u]) + '-' + str(data.best_sentences_ID.loc[i][u][y])\n",
    "#             sentence_ID_list.append(sentence_ID)\n",
    "#             # calculate sim score\n",
    "#             v1 = model.docvecs[str(i)]\n",
    "#             v2 = model.docvecs[sentence_ID]\n",
    "#             similarity = 1 - spatial.distance.cosine(v1, v2)\n",
    "#             # similarity between claim and each sentence is calculated\n",
    "#             sim_for_one_article.append(similarity)\n",
    "# #             print(str(sentence_ID) + \": \" + str(similarity))\n",
    "#         # the similarity between claim and each sentence per article is averaged\n",
    "#         avg_sim = sum(sim_for_one_article)/len(sim_for_one_article)\n",
    "#         avg_sim_for_one_article.append(avg_sim)\n",
    "# #         print(\"\")\n",
    "#         article_sentence_ID.append(sentence_ID_list)\n",
    "    \n",
    "#     # create a list of sentence ID's\n",
    "#     full_sentence_ID.append(article_sentence_ID)\n",
    "    \n",
    "#     #calculate average scores for each claim\n",
    "#     avg_sim_ = sum(avg_sim_for_one_article)/len(avg_sim_for_one_article)\n",
    "#     avg_sim_one_claim.append(avg_sim_)\n",
    "    \n",
    "#     # a list of lists, big list per claim, and small list per article\n",
    "#     avg_sim_for_one_claim_list.append(avg_sim_for_one_article)\n",
    "    \n",
    "#     # print progress\n",
    "#     progress = round((i/end)*100,2)\n",
    "#     clear_output(wait=True)\n",
    "#     print(\"progress: \" + str(progress) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # add to data frame\n",
    "# data['full_sentence_ID'] = full_sentence_ID\n",
    "# data['avg_sentence_sim'] = avg_sim_for_one_claim_list\n",
    "# data['avg_sim_score'] = avg_sim_one_claim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Finding Top 5 Sentences amongst all Related Articles per Claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 99.99%\n",
      "CPU times: user 48.5 s, sys: 2.73 s, total: 51.2 s\n",
      "Wall time: 48.6 s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# # finding top 5 sentences amongst all relateld articles per claim\n",
    "\n",
    "# end = data.shape[0]\n",
    "# # end = 10\n",
    "\n",
    "# sim_score_dict = {}\n",
    "\n",
    "# final_top_sentences = []\n",
    "\n",
    "# for i in range(end):\n",
    "#     top_sentence_list = data.full_sentence_ID.loc[i]\n",
    "#     sim_score_tuple = []\n",
    "#     for u in range(len(top_sentence_list)):\n",
    "#         for y in range(len(top_sentence_list[u])):\n",
    "#             sentence_ID = top_sentence_list[u][y]\n",
    "#             # calculate sim score\n",
    "#             v1 = model.docvecs[str(i)]\n",
    "#             v2 = model.docvecs[sentence_ID]\n",
    "#             similarity = 1 - spatial.distance.cosine(v1, v2)\n",
    "# #             sim_for_one_claim.append(similarity)\n",
    "#             sim_score_tuple.append(tuple((sentence_ID, similarity)))\n",
    "# #             print(str(sentence_ID) + \": \" + str(similarity))\n",
    "#     a = nlargest(5, sim_score_tuple, key=itemgetter(1))\n",
    "#     top_sentences = [q[0] for q in a]\n",
    "#     # list of lists: final top 5 sentences amongst all related articles per claim\n",
    "#     final_top_sentences.append(top_sentences)\n",
    "    \n",
    "#     # print progress\n",
    "#     progress = round((i/end)*100,2)\n",
    "#     clear_output(wait=True)\n",
    "#     print(\"progress: \" + str(progress) + \"%\")\n",
    "\n",
    "# # save it to the dataframe\n",
    "# data['top_5_sentences'] = final_top_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# # average similarity scores of top 5 sentences amongst all relateld articles per claim\n",
    "\n",
    "# end = data.shape[0]\n",
    "# # end = 10\n",
    "\n",
    "# total_avg_sim = []\n",
    "\n",
    "# for i in range(end):\n",
    "#     top_sentence_list = data.top_5_sentences.loc[i]\n",
    "#     top_sim_scores = []\n",
    "#     for u in range(len(top_sentence_list)):\n",
    "#         sentence_ID = top_sentence_list[u]\n",
    "#         # calculate sim score\n",
    "#         v1 = model.docvecs[str(i)]\n",
    "#         v2 = model.docvecs[sentence_ID]\n",
    "#         similarity = 1 - spatial.distance.cosine(v1, v2)       \n",
    "#         top_sim_scores.append(similarity)\n",
    "        \n",
    "#     avg_sim = sum(top_sim_scores)/len(top_sim_scores)\n",
    "#     total_avg_sim.append(avg_sim)\n",
    "    \n",
    "#     # print progress\n",
    "#     progress = round((i/end)*100,2)\n",
    "#     clear_output(wait=True)\n",
    "#     print(\"progress: \" + str(progress) + \"%\")\n",
    "\n",
    "# # save it to the dataframe\n",
    "# data['top5_avg_sim'] = total_avg_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_claim</th>\n",
       "      <th>cleaned_claim</th>\n",
       "      <th>stemmed_claims</th>\n",
       "      <th>stemmed_stopword_claims</th>\n",
       "      <th>tokenized_claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>date</th>\n",
       "      <th>cont_days</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>label</th>\n",
       "      <th>article_array</th>\n",
       "      <th>best_sentences_ID</th>\n",
       "      <th>full_sentence_ID</th>\n",
       "      <th>avg_sentence_sim</th>\n",
       "      <th>avg_sim_score</th>\n",
       "      <th>top_5_sentences</th>\n",
       "      <th>top5_avg_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A line from George Orwell's novel 1984 predict...</td>\n",
       "      <td>a line from george orwells novel 1984 predicts...</td>\n",
       "      <td>a line from georg orwel novel 1984 predict the...</td>\n",
       "      <td>line georg orwel novel 1984 predict power smar...</td>\n",
       "      <td>[line, georg, orwel, novel, 1984, predict, pow...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>11520</td>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "      <td>[[0, 21, 15, 20, 10], [11, 24, 12, 13, 26], [3...</td>\n",
       "      <td>[[122094-0, 122094-21, 122094-15, 122094-20, 1...</td>\n",
       "      <td>[0.26672267615795137, 0.24059542417526245, 0.1...</td>\n",
       "      <td>0.185311</td>\n",
       "      <td>[122094-0, 122580-11, 122094-21, 122094-15, 12...</td>\n",
       "      <td>0.291527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maine legislature candidate Leslie Gibson insu...</td>\n",
       "      <td>maine legislature candidate leslie gibson insu...</td>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>[main, legislatur, candid, lesli, gibson, insu...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>11763</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "      <td>[[11, 7, 13, 3, 4], [22, 24, 0, 1, 19], [0, 8,...</td>\n",
       "      <td>[[106868-11, 106868-7, 106868-13, 106868-3, 10...</td>\n",
       "      <td>[0.382437926530838, 0.374161022901535, 0.28886...</td>\n",
       "      <td>0.348489</td>\n",
       "      <td>[106868-11, 106868-7, 127320-22, 127320-24, 12...</td>\n",
       "      <td>0.400307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A 17-year-old girl named Alyssa Carson is bein...</td>\n",
       "      <td>a 17yearold girl named alyssa carson is being ...</td>\n",
       "      <td>a 17yearold girl name alyssa carson is be trai...</td>\n",
       "      <td>17yearold girl name alyssa carson train nasa b...</td>\n",
       "      <td>[17yearold, girl, name, alyssa, carson, train,...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2018-07-18</td>\n",
       "      <td>11886</td>\n",
       "      <td>2018</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "      <td>[[22, 30, 8, 32, 29], [3, 6, 59, 60, 83], [6, ...</td>\n",
       "      <td>[[132130-22, 132130-30, 132130-8, 132130-32, 1...</td>\n",
       "      <td>[0.45887559056282046, 0.6421806216239929, 0.29...</td>\n",
       "      <td>0.464089</td>\n",
       "      <td>[132132-3, 132132-6, 132132-59, 132132-60, 132...</td>\n",
       "      <td>0.642181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In 1988 author Roald Dahl penned an open lette...</td>\n",
       "      <td>in 1988 author roald dahl penned an open lette...</td>\n",
       "      <td>in 1988 author roald dahl pen an open letter u...</td>\n",
       "      <td>1988 author roald dahl pen open letter urg par...</td>\n",
       "      <td>[1988, author, roald, dahl, pen, open, letter,...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>12087</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "      <td>[[51, 41, 70, 103, 30], [107, 102, 61, 191, 98...</td>\n",
       "      <td>[[123254-51, 123254-41, 123254-70, 123254-103,...</td>\n",
       "      <td>[0.7696409344673156, 0.7453237056732178, 0.755...</td>\n",
       "      <td>0.756903</td>\n",
       "      <td>[127464-222, 123254-51, 123254-41, 123254-70, ...</td>\n",
       "      <td>0.775002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When it comes to fighting terrorism, \"Another ...</td>\n",
       "      <td>when it comes to fighting terrorism another th...</td>\n",
       "      <td>when it come to fight terror anoth thing we kn...</td>\n",
       "      <td>come fight terror anoth thing know doe work ba...</td>\n",
       "      <td>[come, fight, terror, anoth, thing, know, doe,...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>2016-03-22</td>\n",
       "      <td>11038</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "      <td>[[168, 170, 45, 191, 32], [10, 12, 4, 16, 25],...</td>\n",
       "      <td>[[41099-168, 41099-170, 41099-45, 41099-191, 4...</td>\n",
       "      <td>[0.4999494135379791, 0.408842134475708, 0.3396...</td>\n",
       "      <td>0.411560</td>\n",
       "      <td>[41099-168, 95344-259, 95344-247, 95344-74, 95...</td>\n",
       "      <td>0.547029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           raw_claim  \\\n",
       "0  A line from George Orwell's novel 1984 predict...   \n",
       "1  Maine legislature candidate Leslie Gibson insu...   \n",
       "2  A 17-year-old girl named Alyssa Carson is bein...   \n",
       "3  In 1988 author Roald Dahl penned an open lette...   \n",
       "4  When it comes to fighting terrorism, \"Another ...   \n",
       "\n",
       "                                       cleaned_claim  \\\n",
       "0  a line from george orwells novel 1984 predicts...   \n",
       "1  maine legislature candidate leslie gibson insu...   \n",
       "2  a 17yearold girl named alyssa carson is being ...   \n",
       "3  in 1988 author roald dahl penned an open lette...   \n",
       "4  when it comes to fighting terrorism another th...   \n",
       "\n",
       "                                      stemmed_claims  \\\n",
       "0  a line from georg orwel novel 1984 predict the...   \n",
       "1  main legislatur candid lesli gibson insult par...   \n",
       "2  a 17yearold girl name alyssa carson is be trai...   \n",
       "3  in 1988 author roald dahl pen an open letter u...   \n",
       "4  when it come to fight terror anoth thing we kn...   \n",
       "\n",
       "                             stemmed_stopword_claims  \\\n",
       "0  line georg orwel novel 1984 predict power smar...   \n",
       "1  main legislatur candid lesli gibson insult par...   \n",
       "2  17yearold girl name alyssa carson train nasa b...   \n",
       "3  1988 author roald dahl pen open letter urg par...   \n",
       "4  come fight terror anoth thing know doe work ba...   \n",
       "\n",
       "                                     tokenized_claim         claimant  \\\n",
       "0  [line, georg, orwel, novel, 1984, predict, pow...          Unknown   \n",
       "1  [main, legislatur, candid, lesli, gibson, insu...          Unknown   \n",
       "2  [17yearold, girl, name, alyssa, carson, train,...          Unknown   \n",
       "3  [1988, author, roald, dahl, pen, open, letter,...          Unknown   \n",
       "4  [come, fight, terror, anoth, thing, know, doe,...  Hillary Clinton   \n",
       "\n",
       "        date  cont_days  year  month  label  \\\n",
       "0 2017-07-17      11520  2017      7      0   \n",
       "1 2018-03-17      11763  2018      3      2   \n",
       "2 2018-07-18      11886  2018      7      1   \n",
       "3 2019-02-04      12087  2019      2      2   \n",
       "4 2016-03-22      11038  2016      3      2   \n",
       "\n",
       "                                article_array  \\\n",
       "0            [122094, 122580, 130685, 134765]   \n",
       "1                    [106868, 127320, 128060]   \n",
       "2                    [132130, 132132, 149722]   \n",
       "3                    [123254, 123418, 127464]   \n",
       "4  [41099, 89899, 72543, 82644, 95344, 88361]   \n",
       "\n",
       "                                   best_sentences_ID  \\\n",
       "0  [[0, 21, 15, 20, 10], [11, 24, 12, 13, 26], [3...   \n",
       "1  [[11, 7, 13, 3, 4], [22, 24, 0, 1, 19], [0, 8,...   \n",
       "2  [[22, 30, 8, 32, 29], [3, 6, 59, 60, 83], [6, ...   \n",
       "3  [[51, 41, 70, 103, 30], [107, 102, 61, 191, 98...   \n",
       "4  [[168, 170, 45, 191, 32], [10, 12, 4, 16, 25],...   \n",
       "\n",
       "                                    full_sentence_ID  \\\n",
       "0  [[122094-0, 122094-21, 122094-15, 122094-20, 1...   \n",
       "1  [[106868-11, 106868-7, 106868-13, 106868-3, 10...   \n",
       "2  [[132130-22, 132130-30, 132130-8, 132130-32, 1...   \n",
       "3  [[123254-51, 123254-41, 123254-70, 123254-103,...   \n",
       "4  [[41099-168, 41099-170, 41099-45, 41099-191, 4...   \n",
       "\n",
       "                                    avg_sentence_sim  avg_sim_score  \\\n",
       "0  [0.26672267615795137, 0.24059542417526245, 0.1...       0.185311   \n",
       "1  [0.382437926530838, 0.374161022901535, 0.28886...       0.348489   \n",
       "2  [0.45887559056282046, 0.6421806216239929, 0.29...       0.464089   \n",
       "3  [0.7696409344673156, 0.7453237056732178, 0.755...       0.756903   \n",
       "4  [0.4999494135379791, 0.408842134475708, 0.3396...       0.411560   \n",
       "\n",
       "                                     top_5_sentences  top5_avg_sim  \n",
       "0  [122094-0, 122580-11, 122094-21, 122094-15, 12...      0.291527  \n",
       "1  [106868-11, 106868-7, 127320-22, 127320-24, 12...      0.400307  \n",
       "2  [132132-3, 132132-6, 132132-59, 132132-60, 132...      0.642181  \n",
       "3  [127464-222, 123254-51, 123254-41, 123254-70, ...      0.775002  \n",
       "4  [41099-168, 95344-259, 95344-247, 95344-74, 95...      0.547029  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save to data frame\n",
    "# data.to_pickle(\"./data_sentence_simscore.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('115280-38', 0.6487554311752319),\n",
       " ('68326-486', 0.6099780201911926),\n",
       " ('106754-60', 0.6012491583824158),\n",
       " ('149160-47', 0.5995368957519531),\n",
       " ('76075-65', 0.5932543873786926),\n",
       " ('90436-8', 0.5900030732154846),\n",
       " ('74850-8', 0.5896259546279907),\n",
       " ('55964-12', 0.5870769023895264),\n",
       " ('94499-3', 0.5841438174247742),\n",
       " ('77645-433', 0.5817995071411133)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.docvecs.most_similar('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # plot the average scores of each label\n",
    "# _ = data.plot(x='label', y='top5_avg_sim', style='o')\n",
    "# _ = plt.xlim(-1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.docvecs['122094-20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.347765177488327"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v1 = model.docvecs['0']\n",
    "# v2 = model.docvecs['122094-0']\n",
    "# result = 1 - spatial.distance.cosine(v1, v2)\n",
    "# result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Feature Extraction/Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the files\n",
    "full_data = pd.read_pickle(\"./data_sentence_simscore.pkl\")\n",
    "full_data2 = pd.read_pickle(\"./data_simscore.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_claim</th>\n",
       "      <th>cleaned_claim</th>\n",
       "      <th>stemmed_claims</th>\n",
       "      <th>stemmed_stopword_claims</th>\n",
       "      <th>tokenized_claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>date</th>\n",
       "      <th>cont_days</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>label</th>\n",
       "      <th>article_array</th>\n",
       "      <th>best_sentences_ID</th>\n",
       "      <th>full_sentence_ID</th>\n",
       "      <th>avg_sentence_sim</th>\n",
       "      <th>avg_sim_score</th>\n",
       "      <th>top_5_sentences</th>\n",
       "      <th>top5_avg_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A line from George Orwell's novel 1984 predict...</td>\n",
       "      <td>a line from george orwells novel 1984 predicts...</td>\n",
       "      <td>a line from georg orwel novel 1984 predict the...</td>\n",
       "      <td>line georg orwel novel 1984 predict power smar...</td>\n",
       "      <td>[line, georg, orwel, novel, 1984, predict, pow...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>11520</td>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "      <td>[[0, 21, 15, 20, 10], [11, 24, 12, 13, 26], [3...</td>\n",
       "      <td>[[122094-0, 122094-21, 122094-15, 122094-20, 1...</td>\n",
       "      <td>[0.26672267615795137, 0.24059542417526245, 0.1...</td>\n",
       "      <td>0.185311</td>\n",
       "      <td>[122094-0, 122580-11, 122094-21, 122094-15, 12...</td>\n",
       "      <td>0.291527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maine legislature candidate Leslie Gibson insu...</td>\n",
       "      <td>maine legislature candidate leslie gibson insu...</td>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>[main, legislatur, candid, lesli, gibson, insu...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>11763</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "      <td>[[11, 7, 13, 3, 4], [22, 24, 0, 1, 19], [0, 8,...</td>\n",
       "      <td>[[106868-11, 106868-7, 106868-13, 106868-3, 10...</td>\n",
       "      <td>[0.382437926530838, 0.374161022901535, 0.28886...</td>\n",
       "      <td>0.348489</td>\n",
       "      <td>[106868-11, 106868-7, 127320-22, 127320-24, 12...</td>\n",
       "      <td>0.400307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A 17-year-old girl named Alyssa Carson is bein...</td>\n",
       "      <td>a 17yearold girl named alyssa carson is being ...</td>\n",
       "      <td>a 17yearold girl name alyssa carson is be trai...</td>\n",
       "      <td>17yearold girl name alyssa carson train nasa b...</td>\n",
       "      <td>[17yearold, girl, name, alyssa, carson, train,...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2018-07-18</td>\n",
       "      <td>11886</td>\n",
       "      <td>2018</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "      <td>[[22, 30, 8, 32, 29], [3, 6, 59, 60, 83], [6, ...</td>\n",
       "      <td>[[132130-22, 132130-30, 132130-8, 132130-32, 1...</td>\n",
       "      <td>[0.45887559056282046, 0.6421806216239929, 0.29...</td>\n",
       "      <td>0.464089</td>\n",
       "      <td>[132132-3, 132132-6, 132132-59, 132132-60, 132...</td>\n",
       "      <td>0.642181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In 1988 author Roald Dahl penned an open lette...</td>\n",
       "      <td>in 1988 author roald dahl penned an open lette...</td>\n",
       "      <td>in 1988 author roald dahl pen an open letter u...</td>\n",
       "      <td>1988 author roald dahl pen open letter urg par...</td>\n",
       "      <td>[1988, author, roald, dahl, pen, open, letter,...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>12087</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "      <td>[[51, 41, 70, 103, 30], [107, 102, 61, 191, 98...</td>\n",
       "      <td>[[123254-51, 123254-41, 123254-70, 123254-103,...</td>\n",
       "      <td>[0.7696409344673156, 0.7453237056732178, 0.755...</td>\n",
       "      <td>0.756903</td>\n",
       "      <td>[127464-222, 123254-51, 123254-41, 123254-70, ...</td>\n",
       "      <td>0.775002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When it comes to fighting terrorism, \"Another ...</td>\n",
       "      <td>when it comes to fighting terrorism another th...</td>\n",
       "      <td>when it come to fight terror anoth thing we kn...</td>\n",
       "      <td>come fight terror anoth thing know doe work ba...</td>\n",
       "      <td>[come, fight, terror, anoth, thing, know, doe,...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>2016-03-22</td>\n",
       "      <td>11038</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "      <td>[[168, 170, 45, 191, 32], [10, 12, 4, 16, 25],...</td>\n",
       "      <td>[[41099-168, 41099-170, 41099-45, 41099-191, 4...</td>\n",
       "      <td>[0.4999494135379791, 0.408842134475708, 0.3396...</td>\n",
       "      <td>0.411560</td>\n",
       "      <td>[41099-168, 95344-259, 95344-247, 95344-74, 95...</td>\n",
       "      <td>0.547029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           raw_claim  \\\n",
       "0  A line from George Orwell's novel 1984 predict...   \n",
       "1  Maine legislature candidate Leslie Gibson insu...   \n",
       "2  A 17-year-old girl named Alyssa Carson is bein...   \n",
       "3  In 1988 author Roald Dahl penned an open lette...   \n",
       "4  When it comes to fighting terrorism, \"Another ...   \n",
       "\n",
       "                                       cleaned_claim  \\\n",
       "0  a line from george orwells novel 1984 predicts...   \n",
       "1  maine legislature candidate leslie gibson insu...   \n",
       "2  a 17yearold girl named alyssa carson is being ...   \n",
       "3  in 1988 author roald dahl penned an open lette...   \n",
       "4  when it comes to fighting terrorism another th...   \n",
       "\n",
       "                                      stemmed_claims  \\\n",
       "0  a line from georg orwel novel 1984 predict the...   \n",
       "1  main legislatur candid lesli gibson insult par...   \n",
       "2  a 17yearold girl name alyssa carson is be trai...   \n",
       "3  in 1988 author roald dahl pen an open letter u...   \n",
       "4  when it come to fight terror anoth thing we kn...   \n",
       "\n",
       "                             stemmed_stopword_claims  \\\n",
       "0  line georg orwel novel 1984 predict power smar...   \n",
       "1  main legislatur candid lesli gibson insult par...   \n",
       "2  17yearold girl name alyssa carson train nasa b...   \n",
       "3  1988 author roald dahl pen open letter urg par...   \n",
       "4  come fight terror anoth thing know doe work ba...   \n",
       "\n",
       "                                     tokenized_claim         claimant  \\\n",
       "0  [line, georg, orwel, novel, 1984, predict, pow...          Unknown   \n",
       "1  [main, legislatur, candid, lesli, gibson, insu...          Unknown   \n",
       "2  [17yearold, girl, name, alyssa, carson, train,...          Unknown   \n",
       "3  [1988, author, roald, dahl, pen, open, letter,...          Unknown   \n",
       "4  [come, fight, terror, anoth, thing, know, doe,...  Hillary Clinton   \n",
       "\n",
       "        date  cont_days  year  month  label  \\\n",
       "0 2017-07-17      11520  2017      7      0   \n",
       "1 2018-03-17      11763  2018      3      2   \n",
       "2 2018-07-18      11886  2018      7      1   \n",
       "3 2019-02-04      12087  2019      2      2   \n",
       "4 2016-03-22      11038  2016      3      2   \n",
       "\n",
       "                                article_array  \\\n",
       "0            [122094, 122580, 130685, 134765]   \n",
       "1                    [106868, 127320, 128060]   \n",
       "2                    [132130, 132132, 149722]   \n",
       "3                    [123254, 123418, 127464]   \n",
       "4  [41099, 89899, 72543, 82644, 95344, 88361]   \n",
       "\n",
       "                                   best_sentences_ID  \\\n",
       "0  [[0, 21, 15, 20, 10], [11, 24, 12, 13, 26], [3...   \n",
       "1  [[11, 7, 13, 3, 4], [22, 24, 0, 1, 19], [0, 8,...   \n",
       "2  [[22, 30, 8, 32, 29], [3, 6, 59, 60, 83], [6, ...   \n",
       "3  [[51, 41, 70, 103, 30], [107, 102, 61, 191, 98...   \n",
       "4  [[168, 170, 45, 191, 32], [10, 12, 4, 16, 25],...   \n",
       "\n",
       "                                    full_sentence_ID  \\\n",
       "0  [[122094-0, 122094-21, 122094-15, 122094-20, 1...   \n",
       "1  [[106868-11, 106868-7, 106868-13, 106868-3, 10...   \n",
       "2  [[132130-22, 132130-30, 132130-8, 132130-32, 1...   \n",
       "3  [[123254-51, 123254-41, 123254-70, 123254-103,...   \n",
       "4  [[41099-168, 41099-170, 41099-45, 41099-191, 4...   \n",
       "\n",
       "                                    avg_sentence_sim  avg_sim_score  \\\n",
       "0  [0.26672267615795137, 0.24059542417526245, 0.1...       0.185311   \n",
       "1  [0.382437926530838, 0.374161022901535, 0.28886...       0.348489   \n",
       "2  [0.45887559056282046, 0.6421806216239929, 0.29...       0.464089   \n",
       "3  [0.7696409344673156, 0.7453237056732178, 0.755...       0.756903   \n",
       "4  [0.4999494135379791, 0.408842134475708, 0.3396...       0.411560   \n",
       "\n",
       "                                     top_5_sentences  top5_avg_sim  \n",
       "0  [122094-0, 122580-11, 122094-21, 122094-15, 12...      0.291527  \n",
       "1  [106868-11, 106868-7, 127320-22, 127320-24, 12...      0.400307  \n",
       "2  [132132-3, 132132-6, 132132-59, 132132-60, 132...      0.642181  \n",
       "3  [127464-222, 123254-51, 123254-41, 123254-70, ...      0.775002  \n",
       "4  [41099-168, 95344-259, 95344-247, 95344-74, 95...      0.547029  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_claim</th>\n",
       "      <th>cleaned_claim</th>\n",
       "      <th>stemmed_claims</th>\n",
       "      <th>stemmed_stopword_claims</th>\n",
       "      <th>tokenized_claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>date</th>\n",
       "      <th>cont_days</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>label</th>\n",
       "      <th>article_array</th>\n",
       "      <th>average_score</th>\n",
       "      <th>similarity_variance</th>\n",
       "      <th>max_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A line from George Orwell's novel 1984 predict...</td>\n",
       "      <td>a line from george orwells novel 1984 predicts...</td>\n",
       "      <td>a line from georg orwel novel 1984 predict the...</td>\n",
       "      <td>line georg orwel novel 1984 predict power smar...</td>\n",
       "      <td>[line, georg, orwel, novel, 1984, predict, pow...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>11520</td>\n",
       "      <td>2017</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "      <td>0.307869</td>\n",
       "      <td>0.006805</td>\n",
       "      <td>0.447643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maine legislature candidate Leslie Gibson insu...</td>\n",
       "      <td>maine legislature candidate leslie gibson insu...</td>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>main legislatur candid lesli gibson insult par...</td>\n",
       "      <td>[main, legislatur, candid, lesli, gibson, insu...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>11763</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "      <td>0.684023</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>0.739247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A 17-year-old girl named Alyssa Carson is bein...</td>\n",
       "      <td>a 17yearold girl named alyssa carson is being ...</td>\n",
       "      <td>a 17yearold girl name alyssa carson is be trai...</td>\n",
       "      <td>17yearold girl name alyssa carson train nasa b...</td>\n",
       "      <td>[17yearold, girl, name, alyssa, carson, train,...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2018-07-18</td>\n",
       "      <td>11886</td>\n",
       "      <td>2018</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "      <td>0.564941</td>\n",
       "      <td>0.029931</td>\n",
       "      <td>0.692960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In 1988 author Roald Dahl penned an open lette...</td>\n",
       "      <td>in 1988 author roald dahl penned an open lette...</td>\n",
       "      <td>in 1988 author roald dahl pen an open letter u...</td>\n",
       "      <td>1988 author roald dahl pen open letter urg par...</td>\n",
       "      <td>[1988, author, roald, dahl, pen, open, letter,...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>12087</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "      <td>0.648540</td>\n",
       "      <td>0.003423</td>\n",
       "      <td>0.724962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When it comes to fighting terrorism, \"Another ...</td>\n",
       "      <td>when it comes to fighting terrorism another th...</td>\n",
       "      <td>when it come to fight terror anoth thing we kn...</td>\n",
       "      <td>come fight terror anoth thing know doe work ba...</td>\n",
       "      <td>[come, fight, terror, anoth, thing, know, doe,...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>2016-03-22</td>\n",
       "      <td>11038</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "      <td>0.602941</td>\n",
       "      <td>0.020164</td>\n",
       "      <td>0.780002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           raw_claim  \\\n",
       "0  A line from George Orwell's novel 1984 predict...   \n",
       "1  Maine legislature candidate Leslie Gibson insu...   \n",
       "2  A 17-year-old girl named Alyssa Carson is bein...   \n",
       "3  In 1988 author Roald Dahl penned an open lette...   \n",
       "4  When it comes to fighting terrorism, \"Another ...   \n",
       "\n",
       "                                       cleaned_claim  \\\n",
       "0  a line from george orwells novel 1984 predicts...   \n",
       "1  maine legislature candidate leslie gibson insu...   \n",
       "2  a 17yearold girl named alyssa carson is being ...   \n",
       "3  in 1988 author roald dahl penned an open lette...   \n",
       "4  when it comes to fighting terrorism another th...   \n",
       "\n",
       "                                      stemmed_claims  \\\n",
       "0  a line from georg orwel novel 1984 predict the...   \n",
       "1  main legislatur candid lesli gibson insult par...   \n",
       "2  a 17yearold girl name alyssa carson is be trai...   \n",
       "3  in 1988 author roald dahl pen an open letter u...   \n",
       "4  when it come to fight terror anoth thing we kn...   \n",
       "\n",
       "                             stemmed_stopword_claims  \\\n",
       "0  line georg orwel novel 1984 predict power smar...   \n",
       "1  main legislatur candid lesli gibson insult par...   \n",
       "2  17yearold girl name alyssa carson train nasa b...   \n",
       "3  1988 author roald dahl pen open letter urg par...   \n",
       "4  come fight terror anoth thing know doe work ba...   \n",
       "\n",
       "                                     tokenized_claim         claimant  \\\n",
       "0  [line, georg, orwel, novel, 1984, predict, pow...          Unknown   \n",
       "1  [main, legislatur, candid, lesli, gibson, insu...          Unknown   \n",
       "2  [17yearold, girl, name, alyssa, carson, train,...          Unknown   \n",
       "3  [1988, author, roald, dahl, pen, open, letter,...          Unknown   \n",
       "4  [come, fight, terror, anoth, thing, know, doe,...  Hillary Clinton   \n",
       "\n",
       "        date  cont_days  year  month  label  \\\n",
       "0 2017-07-17      11520  2017      7      0   \n",
       "1 2018-03-17      11763  2018      3      2   \n",
       "2 2018-07-18      11886  2018      7      1   \n",
       "3 2019-02-04      12087  2019      2      2   \n",
       "4 2016-03-22      11038  2016      3      2   \n",
       "\n",
       "                                article_array  average_score  \\\n",
       "0            [122094, 122580, 130685, 134765]       0.307869   \n",
       "1                    [106868, 127320, 128060]       0.684023   \n",
       "2                    [132130, 132132, 149722]       0.564941   \n",
       "3                    [123254, 123418, 127464]       0.648540   \n",
       "4  [41099, 89899, 72543, 82644, 95344, 88361]       0.602941   \n",
       "\n",
       "   similarity_variance  max_similarity  \n",
       "0             0.006805        0.447643  \n",
       "1             0.003215        0.739247  \n",
       "2             0.029931        0.692960  \n",
       "3             0.003423        0.724962  \n",
       "4             0.020164        0.780002  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Feature Encode the Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of lists containing the claim's vector\n",
    "claim_vectors = []\n",
    "for i in range(0, 15555):\n",
    "    claim_vectors.append(model.docvecs[str(i)])\n",
    "\n",
    "# create a list of column names\n",
    "column_name_list = []\n",
    "for i in range(0,150):\n",
    "    column_name = \"claim_vec_\" + str(i)\n",
    "    column_name_list.append(column_name)\n",
    "\n",
    "# turn the claim vectors in to a dataframe and rename the columns accordingly\n",
    "claim_features = pd.DataFrame.from_records(claim_vectors)\n",
    "claim_features.columns = column_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_vec_0</th>\n",
       "      <th>claim_vec_1</th>\n",
       "      <th>claim_vec_2</th>\n",
       "      <th>claim_vec_3</th>\n",
       "      <th>claim_vec_4</th>\n",
       "      <th>claim_vec_5</th>\n",
       "      <th>claim_vec_6</th>\n",
       "      <th>claim_vec_7</th>\n",
       "      <th>claim_vec_8</th>\n",
       "      <th>claim_vec_9</th>\n",
       "      <th>...</th>\n",
       "      <th>claim_vec_140</th>\n",
       "      <th>claim_vec_141</th>\n",
       "      <th>claim_vec_142</th>\n",
       "      <th>claim_vec_143</th>\n",
       "      <th>claim_vec_144</th>\n",
       "      <th>claim_vec_145</th>\n",
       "      <th>claim_vec_146</th>\n",
       "      <th>claim_vec_147</th>\n",
       "      <th>claim_vec_148</th>\n",
       "      <th>claim_vec_149</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.212091</td>\n",
       "      <td>0.127670</td>\n",
       "      <td>0.106502</td>\n",
       "      <td>0.391641</td>\n",
       "      <td>0.303139</td>\n",
       "      <td>-0.248252</td>\n",
       "      <td>0.236494</td>\n",
       "      <td>0.104728</td>\n",
       "      <td>-0.271278</td>\n",
       "      <td>0.377465</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.283741</td>\n",
       "      <td>-0.166108</td>\n",
       "      <td>-0.125862</td>\n",
       "      <td>0.316487</td>\n",
       "      <td>-0.086704</td>\n",
       "      <td>-0.015346</td>\n",
       "      <td>0.263394</td>\n",
       "      <td>0.236137</td>\n",
       "      <td>0.191386</td>\n",
       "      <td>-0.144861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.512708</td>\n",
       "      <td>0.203220</td>\n",
       "      <td>0.151913</td>\n",
       "      <td>-0.307748</td>\n",
       "      <td>-0.426895</td>\n",
       "      <td>0.143223</td>\n",
       "      <td>0.293944</td>\n",
       "      <td>-0.129267</td>\n",
       "      <td>0.512433</td>\n",
       "      <td>0.322509</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.216366</td>\n",
       "      <td>0.181766</td>\n",
       "      <td>0.237602</td>\n",
       "      <td>0.279045</td>\n",
       "      <td>-0.126447</td>\n",
       "      <td>-0.209171</td>\n",
       "      <td>-0.191039</td>\n",
       "      <td>0.158190</td>\n",
       "      <td>-0.066281</td>\n",
       "      <td>-0.247980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.145354</td>\n",
       "      <td>0.215115</td>\n",
       "      <td>0.078994</td>\n",
       "      <td>0.009343</td>\n",
       "      <td>0.213732</td>\n",
       "      <td>-0.022070</td>\n",
       "      <td>0.126207</td>\n",
       "      <td>-0.130475</td>\n",
       "      <td>-0.093990</td>\n",
       "      <td>0.159534</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.155529</td>\n",
       "      <td>-0.015081</td>\n",
       "      <td>0.274674</td>\n",
       "      <td>0.272769</td>\n",
       "      <td>-0.237696</td>\n",
       "      <td>0.130785</td>\n",
       "      <td>-0.032134</td>\n",
       "      <td>0.104650</td>\n",
       "      <td>0.257081</td>\n",
       "      <td>0.016224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.138208</td>\n",
       "      <td>0.415532</td>\n",
       "      <td>-0.018799</td>\n",
       "      <td>0.094876</td>\n",
       "      <td>0.292537</td>\n",
       "      <td>-0.157601</td>\n",
       "      <td>0.552872</td>\n",
       "      <td>-0.179123</td>\n",
       "      <td>0.080213</td>\n",
       "      <td>-0.131668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.336580</td>\n",
       "      <td>-0.263726</td>\n",
       "      <td>-0.080669</td>\n",
       "      <td>0.152311</td>\n",
       "      <td>-0.093220</td>\n",
       "      <td>0.300876</td>\n",
       "      <td>-0.068453</td>\n",
       "      <td>0.307259</td>\n",
       "      <td>0.150425</td>\n",
       "      <td>-0.206309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.126634</td>\n",
       "      <td>0.311318</td>\n",
       "      <td>-0.125836</td>\n",
       "      <td>-0.143917</td>\n",
       "      <td>-0.092023</td>\n",
       "      <td>-0.118011</td>\n",
       "      <td>-0.178362</td>\n",
       "      <td>-0.358138</td>\n",
       "      <td>0.349950</td>\n",
       "      <td>-0.148357</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029561</td>\n",
       "      <td>-0.171704</td>\n",
       "      <td>0.290350</td>\n",
       "      <td>0.440742</td>\n",
       "      <td>-0.249992</td>\n",
       "      <td>0.430053</td>\n",
       "      <td>-0.086707</td>\n",
       "      <td>0.252833</td>\n",
       "      <td>-0.295727</td>\n",
       "      <td>0.226280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   claim_vec_0  claim_vec_1  claim_vec_2  claim_vec_3  claim_vec_4  \\\n",
       "0    -0.212091     0.127670     0.106502     0.391641     0.303139   \n",
       "1     0.512708     0.203220     0.151913    -0.307748    -0.426895   \n",
       "2     0.145354     0.215115     0.078994     0.009343     0.213732   \n",
       "3    -0.138208     0.415532    -0.018799     0.094876     0.292537   \n",
       "4     0.126634     0.311318    -0.125836    -0.143917    -0.092023   \n",
       "\n",
       "   claim_vec_5  claim_vec_6  claim_vec_7  claim_vec_8  claim_vec_9  ...  \\\n",
       "0    -0.248252     0.236494     0.104728    -0.271278     0.377465  ...   \n",
       "1     0.143223     0.293944    -0.129267     0.512433     0.322509  ...   \n",
       "2    -0.022070     0.126207    -0.130475    -0.093990     0.159534  ...   \n",
       "3    -0.157601     0.552872    -0.179123     0.080213    -0.131668  ...   \n",
       "4    -0.118011    -0.178362    -0.358138     0.349950    -0.148357  ...   \n",
       "\n",
       "   claim_vec_140  claim_vec_141  claim_vec_142  claim_vec_143  claim_vec_144  \\\n",
       "0      -0.283741      -0.166108      -0.125862       0.316487      -0.086704   \n",
       "1      -0.216366       0.181766       0.237602       0.279045      -0.126447   \n",
       "2      -0.155529      -0.015081       0.274674       0.272769      -0.237696   \n",
       "3      -0.336580      -0.263726      -0.080669       0.152311      -0.093220   \n",
       "4      -0.029561      -0.171704       0.290350       0.440742      -0.249992   \n",
       "\n",
       "   claim_vec_145  claim_vec_146  claim_vec_147  claim_vec_148  claim_vec_149  \n",
       "0      -0.015346       0.263394       0.236137       0.191386      -0.144861  \n",
       "1      -0.209171      -0.191039       0.158190      -0.066281      -0.247980  \n",
       "2       0.130785      -0.032134       0.104650       0.257081       0.016224  \n",
       "3       0.300876      -0.068453       0.307259       0.150425      -0.206309  \n",
       "4       0.430053      -0.086707       0.252833      -0.295727       0.226280  \n",
       "\n",
       "[5 rows x 150 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claim_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Feature Encode the Top 5 Related Article Sentencese for Each Claim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 99.99%\n",
      "CPU times: user 24.5 s, sys: 2.7 s, total: 27.2 s\n",
      "Wall time: 24.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "end = full_data.shape[0]\n",
    "# end = 1\n",
    "full_sentence_vector_list = []\n",
    "for i in range(end):\n",
    "    top_5_sentences = full_data.top_5_sentences.loc[i]\n",
    "    sentence_vector_list = []\n",
    "    for u in range(len(top_5_sentences)):\n",
    "        sentence_ID = full_data.top_5_sentences.loc[i][u]\n",
    "        sentence_vector = model.docvecs[sentence_ID]\n",
    "        sentence_vector_list.extend(sentence_vector)\n",
    "    full_sentence_vector_list.append(sentence_vector_list)\n",
    "\n",
    "    # print progress\n",
    "    progress = round((i/end)*100,2)\n",
    "    clear_output(wait=True)\n",
    "    print(\"progress: \" + str(progress) + \"%\")\n",
    "\n",
    "# create column names\n",
    "complete_column_name=[]\n",
    "for i in range(1, 6):\n",
    "    column_name_list = []\n",
    "    for u in range(0,150):\n",
    "        column_name = \"sent_\" + str(i) + \"vec_\" + str(u)\n",
    "        column_name_list.append(column_name)\n",
    "    complete_column_name.extend(column_name_list)\n",
    "\n",
    "# convert full sentence features in to dataframe and name the columns accordingly\n",
    "sentence_features = pd.DataFrame.from_records(full_sentence_vector_list)\n",
    "sentence_features.columns = complete_column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_1vec_0</th>\n",
       "      <th>sent_1vec_1</th>\n",
       "      <th>sent_1vec_2</th>\n",
       "      <th>sent_1vec_3</th>\n",
       "      <th>sent_1vec_4</th>\n",
       "      <th>sent_1vec_5</th>\n",
       "      <th>sent_1vec_6</th>\n",
       "      <th>sent_1vec_7</th>\n",
       "      <th>sent_1vec_8</th>\n",
       "      <th>sent_1vec_9</th>\n",
       "      <th>...</th>\n",
       "      <th>sent_5vec_140</th>\n",
       "      <th>sent_5vec_141</th>\n",
       "      <th>sent_5vec_142</th>\n",
       "      <th>sent_5vec_143</th>\n",
       "      <th>sent_5vec_144</th>\n",
       "      <th>sent_5vec_145</th>\n",
       "      <th>sent_5vec_146</th>\n",
       "      <th>sent_5vec_147</th>\n",
       "      <th>sent_5vec_148</th>\n",
       "      <th>sent_5vec_149</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010075</td>\n",
       "      <td>-0.034665</td>\n",
       "      <td>0.158326</td>\n",
       "      <td>0.114454</td>\n",
       "      <td>0.167062</td>\n",
       "      <td>-0.325514</td>\n",
       "      <td>0.343727</td>\n",
       "      <td>-0.181544</td>\n",
       "      <td>0.427879</td>\n",
       "      <td>-0.064174</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.329464</td>\n",
       "      <td>-0.123360</td>\n",
       "      <td>-0.106719</td>\n",
       "      <td>0.596236</td>\n",
       "      <td>-0.361782</td>\n",
       "      <td>-0.154024</td>\n",
       "      <td>0.078335</td>\n",
       "      <td>0.041940</td>\n",
       "      <td>-0.059460</td>\n",
       "      <td>0.239629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.537097</td>\n",
       "      <td>0.346272</td>\n",
       "      <td>-0.127193</td>\n",
       "      <td>0.008937</td>\n",
       "      <td>-0.498761</td>\n",
       "      <td>0.209842</td>\n",
       "      <td>-0.136562</td>\n",
       "      <td>-0.121760</td>\n",
       "      <td>0.598921</td>\n",
       "      <td>0.272825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.310716</td>\n",
       "      <td>1.026762</td>\n",
       "      <td>1.642795</td>\n",
       "      <td>0.402427</td>\n",
       "      <td>-0.656952</td>\n",
       "      <td>-0.680550</td>\n",
       "      <td>1.451327</td>\n",
       "      <td>0.068513</td>\n",
       "      <td>0.286796</td>\n",
       "      <td>0.028587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.034549</td>\n",
       "      <td>0.159652</td>\n",
       "      <td>0.068537</td>\n",
       "      <td>-0.108595</td>\n",
       "      <td>0.401951</td>\n",
       "      <td>0.025913</td>\n",
       "      <td>0.490867</td>\n",
       "      <td>-0.182542</td>\n",
       "      <td>0.126992</td>\n",
       "      <td>0.227730</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.187836</td>\n",
       "      <td>-0.225113</td>\n",
       "      <td>0.046499</td>\n",
       "      <td>0.118527</td>\n",
       "      <td>0.053234</td>\n",
       "      <td>0.126584</td>\n",
       "      <td>-0.112338</td>\n",
       "      <td>0.300140</td>\n",
       "      <td>0.125310</td>\n",
       "      <td>0.023155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.006104</td>\n",
       "      <td>0.218761</td>\n",
       "      <td>-0.044763</td>\n",
       "      <td>-0.014197</td>\n",
       "      <td>0.140955</td>\n",
       "      <td>0.030794</td>\n",
       "      <td>0.227532</td>\n",
       "      <td>-0.064394</td>\n",
       "      <td>0.077120</td>\n",
       "      <td>0.073692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.184586</td>\n",
       "      <td>-0.138933</td>\n",
       "      <td>0.027737</td>\n",
       "      <td>0.214644</td>\n",
       "      <td>-0.101030</td>\n",
       "      <td>0.155582</td>\n",
       "      <td>0.019132</td>\n",
       "      <td>0.178928</td>\n",
       "      <td>0.125431</td>\n",
       "      <td>0.017242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.143222</td>\n",
       "      <td>0.262105</td>\n",
       "      <td>-0.407402</td>\n",
       "      <td>-0.173036</td>\n",
       "      <td>-0.196569</td>\n",
       "      <td>0.068167</td>\n",
       "      <td>0.154569</td>\n",
       "      <td>-0.175273</td>\n",
       "      <td>0.005277</td>\n",
       "      <td>0.168737</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.060368</td>\n",
       "      <td>-0.066583</td>\n",
       "      <td>0.038418</td>\n",
       "      <td>0.146584</td>\n",
       "      <td>-0.051212</td>\n",
       "      <td>0.113704</td>\n",
       "      <td>-0.006178</td>\n",
       "      <td>0.114713</td>\n",
       "      <td>0.056479</td>\n",
       "      <td>0.011474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 750 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sent_1vec_0  sent_1vec_1  sent_1vec_2  sent_1vec_3  sent_1vec_4  \\\n",
       "0     0.010075    -0.034665     0.158326     0.114454     0.167062   \n",
       "1     0.537097     0.346272    -0.127193     0.008937    -0.498761   \n",
       "2    -0.034549     0.159652     0.068537    -0.108595     0.401951   \n",
       "3    -0.006104     0.218761    -0.044763    -0.014197     0.140955   \n",
       "4     0.143222     0.262105    -0.407402    -0.173036    -0.196569   \n",
       "\n",
       "   sent_1vec_5  sent_1vec_6  sent_1vec_7  sent_1vec_8  sent_1vec_9  ...  \\\n",
       "0    -0.325514     0.343727    -0.181544     0.427879    -0.064174  ...   \n",
       "1     0.209842    -0.136562    -0.121760     0.598921     0.272825  ...   \n",
       "2     0.025913     0.490867    -0.182542     0.126992     0.227730  ...   \n",
       "3     0.030794     0.227532    -0.064394     0.077120     0.073692  ...   \n",
       "4     0.068167     0.154569    -0.175273     0.005277     0.168737  ...   \n",
       "\n",
       "   sent_5vec_140  sent_5vec_141  sent_5vec_142  sent_5vec_143  sent_5vec_144  \\\n",
       "0      -0.329464      -0.123360      -0.106719       0.596236      -0.361782   \n",
       "1       0.310716       1.026762       1.642795       0.402427      -0.656952   \n",
       "2      -0.187836      -0.225113       0.046499       0.118527       0.053234   \n",
       "3      -0.184586      -0.138933       0.027737       0.214644      -0.101030   \n",
       "4      -0.060368      -0.066583       0.038418       0.146584      -0.051212   \n",
       "\n",
       "   sent_5vec_145  sent_5vec_146  sent_5vec_147  sent_5vec_148  sent_5vec_149  \n",
       "0      -0.154024       0.078335       0.041940      -0.059460       0.239629  \n",
       "1      -0.680550       1.451327       0.068513       0.286796       0.028587  \n",
       "2       0.126584      -0.112338       0.300140       0.125310       0.023155  \n",
       "3       0.155582       0.019132       0.178928       0.125431       0.017242  \n",
       "4       0.113704      -0.006178       0.114713       0.056479       0.011474  \n",
       "\n",
       "[5 rows x 750 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Feature Encode the Similarity Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_score_features = pd.concat([full_data2.average_score, full_data2.similarity_variance, full_data2.max_similarity, full_data.avg_sim_score, full_data.top5_avg_sim], axis=1)\n",
    "sim_score_features.columns = ['art_claim_sim', \"art_claim_var\", \"art_claim_max_sim\", \"avg_sent_sim\", \"avg_5_sent_sim\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>art_claim_sim</th>\n",
       "      <th>art_claim_var</th>\n",
       "      <th>art_claim_max_sim</th>\n",
       "      <th>avg_sent_sim</th>\n",
       "      <th>avg_5_sent_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.307869</td>\n",
       "      <td>0.006805</td>\n",
       "      <td>0.447643</td>\n",
       "      <td>0.185311</td>\n",
       "      <td>0.291527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.684023</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>0.739247</td>\n",
       "      <td>0.348489</td>\n",
       "      <td>0.400307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.564941</td>\n",
       "      <td>0.029931</td>\n",
       "      <td>0.692960</td>\n",
       "      <td>0.464089</td>\n",
       "      <td>0.642181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.648540</td>\n",
       "      <td>0.003423</td>\n",
       "      <td>0.724962</td>\n",
       "      <td>0.756903</td>\n",
       "      <td>0.775002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.602941</td>\n",
       "      <td>0.020164</td>\n",
       "      <td>0.780002</td>\n",
       "      <td>0.411560</td>\n",
       "      <td>0.547029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   art_claim_sim  art_claim_var  art_claim_max_sim  avg_sent_sim  \\\n",
       "0       0.307869       0.006805           0.447643      0.185311   \n",
       "1       0.684023       0.003215           0.739247      0.348489   \n",
       "2       0.564941       0.029931           0.692960      0.464089   \n",
       "3       0.648540       0.003423           0.724962      0.756903   \n",
       "4       0.602941       0.020164           0.780002      0.411560   \n",
       "\n",
       "   avg_5_sent_sim  \n",
       "0        0.291527  \n",
       "1        0.400307  \n",
       "2        0.642181  \n",
       "3        0.775002  \n",
       "4        0.547029  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_score_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Combine All Features (Claim Vector, Top 5 Related Sentences Vector, Similarity Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_similiarity_sentences_features = pd.concat([claim_features, sentence_features, sim_score_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_vec_0</th>\n",
       "      <th>claim_vec_1</th>\n",
       "      <th>claim_vec_2</th>\n",
       "      <th>claim_vec_3</th>\n",
       "      <th>claim_vec_4</th>\n",
       "      <th>claim_vec_5</th>\n",
       "      <th>claim_vec_6</th>\n",
       "      <th>claim_vec_7</th>\n",
       "      <th>claim_vec_8</th>\n",
       "      <th>claim_vec_9</th>\n",
       "      <th>...</th>\n",
       "      <th>sent_5vec_145</th>\n",
       "      <th>sent_5vec_146</th>\n",
       "      <th>sent_5vec_147</th>\n",
       "      <th>sent_5vec_148</th>\n",
       "      <th>sent_5vec_149</th>\n",
       "      <th>art_claim_sim</th>\n",
       "      <th>art_claim_var</th>\n",
       "      <th>art_claim_max_sim</th>\n",
       "      <th>avg_sent_sim</th>\n",
       "      <th>avg_5_sent_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.212091</td>\n",
       "      <td>0.127670</td>\n",
       "      <td>0.106502</td>\n",
       "      <td>0.391641</td>\n",
       "      <td>0.303139</td>\n",
       "      <td>-0.248252</td>\n",
       "      <td>0.236494</td>\n",
       "      <td>0.104728</td>\n",
       "      <td>-0.271278</td>\n",
       "      <td>0.377465</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.154024</td>\n",
       "      <td>0.078335</td>\n",
       "      <td>0.041940</td>\n",
       "      <td>-0.059460</td>\n",
       "      <td>0.239629</td>\n",
       "      <td>0.307869</td>\n",
       "      <td>0.006805</td>\n",
       "      <td>0.447643</td>\n",
       "      <td>0.185311</td>\n",
       "      <td>0.291527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.512708</td>\n",
       "      <td>0.203220</td>\n",
       "      <td>0.151913</td>\n",
       "      <td>-0.307748</td>\n",
       "      <td>-0.426895</td>\n",
       "      <td>0.143223</td>\n",
       "      <td>0.293944</td>\n",
       "      <td>-0.129267</td>\n",
       "      <td>0.512433</td>\n",
       "      <td>0.322509</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.680550</td>\n",
       "      <td>1.451327</td>\n",
       "      <td>0.068513</td>\n",
       "      <td>0.286796</td>\n",
       "      <td>0.028587</td>\n",
       "      <td>0.684023</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>0.739247</td>\n",
       "      <td>0.348489</td>\n",
       "      <td>0.400307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.145354</td>\n",
       "      <td>0.215115</td>\n",
       "      <td>0.078994</td>\n",
       "      <td>0.009343</td>\n",
       "      <td>0.213732</td>\n",
       "      <td>-0.022070</td>\n",
       "      <td>0.126207</td>\n",
       "      <td>-0.130475</td>\n",
       "      <td>-0.093990</td>\n",
       "      <td>0.159534</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126584</td>\n",
       "      <td>-0.112338</td>\n",
       "      <td>0.300140</td>\n",
       "      <td>0.125310</td>\n",
       "      <td>0.023155</td>\n",
       "      <td>0.564941</td>\n",
       "      <td>0.029931</td>\n",
       "      <td>0.692960</td>\n",
       "      <td>0.464089</td>\n",
       "      <td>0.642181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.138208</td>\n",
       "      <td>0.415532</td>\n",
       "      <td>-0.018799</td>\n",
       "      <td>0.094876</td>\n",
       "      <td>0.292537</td>\n",
       "      <td>-0.157601</td>\n",
       "      <td>0.552872</td>\n",
       "      <td>-0.179123</td>\n",
       "      <td>0.080213</td>\n",
       "      <td>-0.131668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155582</td>\n",
       "      <td>0.019132</td>\n",
       "      <td>0.178928</td>\n",
       "      <td>0.125431</td>\n",
       "      <td>0.017242</td>\n",
       "      <td>0.648540</td>\n",
       "      <td>0.003423</td>\n",
       "      <td>0.724962</td>\n",
       "      <td>0.756903</td>\n",
       "      <td>0.775002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.126634</td>\n",
       "      <td>0.311318</td>\n",
       "      <td>-0.125836</td>\n",
       "      <td>-0.143917</td>\n",
       "      <td>-0.092023</td>\n",
       "      <td>-0.118011</td>\n",
       "      <td>-0.178362</td>\n",
       "      <td>-0.358138</td>\n",
       "      <td>0.349950</td>\n",
       "      <td>-0.148357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113704</td>\n",
       "      <td>-0.006178</td>\n",
       "      <td>0.114713</td>\n",
       "      <td>0.056479</td>\n",
       "      <td>0.011474</td>\n",
       "      <td>0.602941</td>\n",
       "      <td>0.020164</td>\n",
       "      <td>0.780002</td>\n",
       "      <td>0.411560</td>\n",
       "      <td>0.547029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 905 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   claim_vec_0  claim_vec_1  claim_vec_2  claim_vec_3  claim_vec_4  \\\n",
       "0    -0.212091     0.127670     0.106502     0.391641     0.303139   \n",
       "1     0.512708     0.203220     0.151913    -0.307748    -0.426895   \n",
       "2     0.145354     0.215115     0.078994     0.009343     0.213732   \n",
       "3    -0.138208     0.415532    -0.018799     0.094876     0.292537   \n",
       "4     0.126634     0.311318    -0.125836    -0.143917    -0.092023   \n",
       "\n",
       "   claim_vec_5  claim_vec_6  claim_vec_7  claim_vec_8  claim_vec_9  ...  \\\n",
       "0    -0.248252     0.236494     0.104728    -0.271278     0.377465  ...   \n",
       "1     0.143223     0.293944    -0.129267     0.512433     0.322509  ...   \n",
       "2    -0.022070     0.126207    -0.130475    -0.093990     0.159534  ...   \n",
       "3    -0.157601     0.552872    -0.179123     0.080213    -0.131668  ...   \n",
       "4    -0.118011    -0.178362    -0.358138     0.349950    -0.148357  ...   \n",
       "\n",
       "   sent_5vec_145  sent_5vec_146  sent_5vec_147  sent_5vec_148  sent_5vec_149  \\\n",
       "0      -0.154024       0.078335       0.041940      -0.059460       0.239629   \n",
       "1      -0.680550       1.451327       0.068513       0.286796       0.028587   \n",
       "2       0.126584      -0.112338       0.300140       0.125310       0.023155   \n",
       "3       0.155582       0.019132       0.178928       0.125431       0.017242   \n",
       "4       0.113704      -0.006178       0.114713       0.056479       0.011474   \n",
       "\n",
       "   art_claim_sim  art_claim_var  art_claim_max_sim  avg_sent_sim  \\\n",
       "0       0.307869       0.006805           0.447643      0.185311   \n",
       "1       0.684023       0.003215           0.739247      0.348489   \n",
       "2       0.564941       0.029931           0.692960      0.464089   \n",
       "3       0.648540       0.003423           0.724962      0.756903   \n",
       "4       0.602941       0.020164           0.780002      0.411560   \n",
       "\n",
       "   avg_5_sent_sim  \n",
       "0        0.291527  \n",
       "1        0.400307  \n",
       "2        0.642181  \n",
       "3        0.775002  \n",
       "4        0.547029  \n",
       "\n",
       "[5 rows x 905 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_similiarity_sentences_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final features to a pickle\n",
    "final_similiarity_sentences_features.to_pickle(\"./doc2vec_features.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
