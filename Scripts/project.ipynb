{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import make_scorer, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# The following line is needed to show plots inline in notebooks\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>related_articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A line from George Orwell's novel 1984 predict...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17/07/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Maine legislature candidate Leslie Gibson insu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17/03/2018</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A 17-year-old girl named Alyssa Carson is bein...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18/07/2018</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>In 1988 author Roald Dahl penned an open lette...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>04/02/2019</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>When it comes to fighting terrorism, \"Another ...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>22/03/2016</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              claim  \\\n",
       "0           0  A line from George Orwell's novel 1984 predict...   \n",
       "1           1  Maine legislature candidate Leslie Gibson insu...   \n",
       "2           2  A 17-year-old girl named Alyssa Carson is bein...   \n",
       "3           3  In 1988 author Roald Dahl penned an open lette...   \n",
       "4           4  When it comes to fighting terrorism, \"Another ...   \n",
       "\n",
       "          claimant        date  id  label  \\\n",
       "0              NaN  17/07/2017   0      0   \n",
       "1              NaN  17/03/2018   1      2   \n",
       "2              NaN  18/07/2018   4      1   \n",
       "3              NaN  04/02/2019   5      2   \n",
       "4  Hillary Clinton  22/03/2016   6      2   \n",
       "\n",
       "                             related_articles  \n",
       "0            [122094, 122580, 130685, 134765]  \n",
       "1                    [106868, 127320, 128060]  \n",
       "2                    [132130, 132132, 149722]  \n",
       "3                    [123254, 123418, 127464]  \n",
       "4  [41099, 89899, 72543, 82644, 95344, 88361]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15555, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claimant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4962"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 4962 missing values in claimant\n",
    "data[data['claimant'].isna()].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Donald Trump                                          1233\n",
       "Bloggers                                               372\n",
       "Barack Obama                                           234\n",
       "Hillary Clinton                                        220\n",
       "Viral image                                            127\n",
       "Facebook posts                                         108\n",
       "Ted Cruz                                               106\n",
       "Various websites                                       106\n",
       "Bernie Sanders                                         101\n",
       "Marco Rubio                                             97\n",
       "Scott Walker                                            90\n",
       "John McCain                                             88\n",
       "Rick Perry                                              77\n",
       "Rick Scott                                              73\n",
       "Chain email                                             71\n",
       "Facebook user                                           71\n",
       "multiple sources                                        70\n",
       "Mike Pence                                              60\n",
       "Jeb Bush                                                58\n",
       "Paul Ryan                                               51\n",
       "Multiple sources                                        50\n",
       "Vladimir Putin                                          44\n",
       "Greg Abbott                                             43\n",
       "Charlie Crist                                           43\n",
       "Chris Christie                                          41\n",
       "Ron Johnson                                             39\n",
       "Tim Kaine                                               39\n",
       "Mitt Romney                                             39\n",
       "Rand Paul                                               36\n",
       "Tammy Baldwin                                           35\n",
       "                                                      ... \n",
       "John Bolton                                              1\n",
       "Martha Laning                                            1\n",
       "Leonard Pitts, Jr.                                       1\n",
       "David Watts                                              1\n",
       "Bob Casey                                                1\n",
       "John Thomaides                                           1\n",
       "Facebook Posts                                           1\n",
       "XpouZAR.com                                              1\n",
       "states-news.com                                          1\n",
       "Environmental Justice League of R.I                      1\n",
       "Brendan Boyle                                            1\n",
       "Marcus Low                                               1\n",
       "Minka Kelly                                              1\n",
       "Leo Berman                                               1\n",
       "Marylanders to Prevent Gun Violence Education Fund       1\n",
       "Antiwar.com                                              1\n",
       "Stronger New Mexico                                      1\n",
       "Daniel Greenfield                                        1\n",
       "Tom Letson                                               1\n",
       "Brian Robinson                                           1\n",
       "Tennesseans for a Better Tomorrow                        1\n",
       "Andrew Wheeler                                           1\n",
       "John Stone                                               1\n",
       "Charles Boustany                                         1\n",
       "Vets for Trump                                           1\n",
       "Philadelphia Daily News                                  1\n",
       "news.groopspeak.com                                      1\n",
       "Eric Trump                                               1\n",
       "BreakingNews247.net                                      1\n",
       "Andrey Klimov                                            1\n",
       "Name: claimant, Length: 3104, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claimant_count = data['claimant'].value_counts()\n",
    "claimant_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other               7986\n",
       "Donald Trump        1233\n",
       "Bloggers             372\n",
       "Barack Obama         234\n",
       "Hillary Clinton      220\n",
       "Viral image          127\n",
       "Facebook posts       108\n",
       "Various websites     106\n",
       "Ted Cruz             106\n",
       "Bernie Sanders       101\n",
       "Name: claimant, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group together all counts less than 100 in to Others\n",
    "value_mask = data.claimant.isin(claimant_count.index[claimant_count < 100]) \n",
    "data.loc[value_mask,'claimant'] = \"Other\"\n",
    "data.claimant.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.611447\n",
       "1    0.259371\n",
       "2    0.129182\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Of the claimant missing, how many are false-0, partly true-1, true-2?\n",
    "missing_data = data[data['claimant'].isna()]\n",
    "missing_data.label.value_counts(normalize=True)\n",
    "# roughly 87% of claims with missing claimants are not completely true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "06/11/2017    407\n",
       "23/10/2016    339\n",
       "07/07/2016     97\n",
       "26/09/2018     59\n",
       "04/10/2016     35\n",
       "25/02/2019     33\n",
       "30/01/2019     31\n",
       "07/03/2019     30\n",
       "09/10/2016     29\n",
       "20/02/2019     29\n",
       "19/10/2016     29\n",
       "22/10/2018     28\n",
       "30/01/2018     27\n",
       "07/02/2019     26\n",
       "14/09/2018     26\n",
       "12/12/2018     25\n",
       "08/05/2018     25\n",
       "05/02/2019     25\n",
       "10/04/2019     25\n",
       "14/02/2019     25\n",
       "13/03/2018     24\n",
       "04/04/2019     24\n",
       "26/10/2018     24\n",
       "25/09/2018     23\n",
       "12/03/2019     23\n",
       "07/09/2016     23\n",
       "11/02/2019     23\n",
       "23/08/2017     23\n",
       "27/07/2016     23\n",
       "10/10/2018     23\n",
       "             ... \n",
       "14/12/2011      1\n",
       "01/11/2011      1\n",
       "12/09/2012      1\n",
       "21/12/2012      1\n",
       "20/07/2011      1\n",
       "04/05/2012      1\n",
       "17/06/2010      1\n",
       "18/04/2010      1\n",
       "06/02/2010      1\n",
       "25/02/2017      1\n",
       "11/09/2016      1\n",
       "08/07/2009      1\n",
       "28/05/2015      1\n",
       "21/12/2009      1\n",
       "06/05/2010      1\n",
       "25/04/2011      1\n",
       "31/08/2013      1\n",
       "28/09/2013      1\n",
       "06/02/2015      1\n",
       "25/05/2011      1\n",
       "27/05/2018      1\n",
       "26/01/2011      1\n",
       "18/05/2013      1\n",
       "06/07/2008      1\n",
       "01/08/2015      1\n",
       "30/12/2015      1\n",
       "24/09/2012      1\n",
       "28/06/2009      1\n",
       "15/09/2009      1\n",
       "16/07/2011      1\n",
       "Name: date, Length: 3019, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5% of data happen on 2 separate days, November 6th 2017 and October 23rd 2016\n",
    "date_count = data['date'].value_counts()\n",
    "date_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.152368333885392"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On average, about 5 claims happen a day\n",
    "data.date.value_counts().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other         14524\n",
       "06/11/2017      407\n",
       "23/10/2016      339\n",
       "07/07/2016       97\n",
       "26/09/2018       59\n",
       "04/10/2016       35\n",
       "25/02/2019       33\n",
       "30/01/2019       31\n",
       "07/03/2019       30\n",
       "Name: date, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group together all counts less than average in to Others\n",
    "value_mask = data.date.isin(date_count.index[date_count < 30]) \n",
    "data.loc[value_mask,'date'] = \"Other\"\n",
    "data.date.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claimant Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Barack Obama',\n",
       " 1: 'Bernie Sanders',\n",
       " 2: 'Bloggers',\n",
       " 3: 'Donald Trump',\n",
       " 4: 'Facebook posts',\n",
       " 5: 'Hillary Clinton',\n",
       " 6: 'Other',\n",
       " 7: 'Ted Cruz',\n",
       " 8: 'Various websites',\n",
       " 9: 'Viral image'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label encoding\n",
    "data['claimant']=data['claimant'].astype('category')\n",
    "data['claimant_cat']=data['claimant'].cat.codes\n",
    "claimant_labels = dict(enumerate(data['claimant'].cat.categories))\n",
    "claimant_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claim Encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", # the feature should be made of word\n",
    "                             tokenizer = None, \n",
    "                             preprocessor = None, \n",
    "                             stop_words = 'english', # Remove stop words such as “the”, “a”, etc.\n",
    "                             max_features = 500)\n",
    "claim_vec = vectorizer.fit_transform(data['claim'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_array = claim_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000',\n",
       " '10',\n",
       " '100',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '20',\n",
       " '200',\n",
       " '2011',\n",
       " '2012',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '2016',\n",
       " '2017',\n",
       " '2018',\n",
       " '25',\n",
       " '30',\n",
       " '300',\n",
       " '40',\n",
       " '50',\n",
       " '500',\n",
       " '60',\n",
       " '70',\n",
       " '80',\n",
       " 'able',\n",
       " 'abortion',\n",
       " 'according',\n",
       " 'act',\n",
       " 'actually',\n",
       " 'administration',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'age',\n",
       " 'ago',\n",
       " 'air',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'announced',\n",
       " 'anti',\n",
       " 'april',\n",
       " 'arrested',\n",
       " 'article',\n",
       " 'asked',\n",
       " 'attack',\n",
       " 'attacks',\n",
       " 'august',\n",
       " 'average',\n",
       " 'away',\n",
       " 'baby',\n",
       " 'ban',\n",
       " 'banned',\n",
       " 'barack',\n",
       " 'believe',\n",
       " 'benefits',\n",
       " 'bernie',\n",
       " 'best',\n",
       " 'better',\n",
       " 'big',\n",
       " 'billion',\n",
       " 'birth',\n",
       " 'black',\n",
       " 'border',\n",
       " 'born',\n",
       " 'budget',\n",
       " 'bush',\n",
       " 'business',\n",
       " 'buy',\n",
       " 'california',\n",
       " 'called',\n",
       " 'came',\n",
       " 'campaign',\n",
       " 'canada',\n",
       " 'cancer',\n",
       " 'candidate',\n",
       " 'car',\n",
       " 'care',\n",
       " 'carolina',\n",
       " 'case',\n",
       " 'caught',\n",
       " 'cause',\n",
       " 'change',\n",
       " 'check',\n",
       " 'child',\n",
       " 'children',\n",
       " 'china',\n",
       " 'cities',\n",
       " 'citizens',\n",
       " 'city',\n",
       " 'claimed',\n",
       " 'claims',\n",
       " 'class',\n",
       " 'climate',\n",
       " 'clinton',\n",
       " 'close',\n",
       " 'collected',\n",
       " 'college',\n",
       " 'com',\n",
       " 'come',\n",
       " 'coming',\n",
       " 'companies',\n",
       " 'company',\n",
       " 'conditions',\n",
       " 'congress',\n",
       " 'congressional',\n",
       " 'control',\n",
       " 'cost',\n",
       " 'costs',\n",
       " 'countries',\n",
       " 'country',\n",
       " 'county',\n",
       " 'court',\n",
       " 'coverage',\n",
       " 'created',\n",
       " 'crime',\n",
       " 'criminal',\n",
       " 'cruz',\n",
       " 'cut',\n",
       " 'cuts',\n",
       " 'david',\n",
       " 'day',\n",
       " 'days',\n",
       " 'dead',\n",
       " 'deal',\n",
       " 'death',\n",
       " 'debt',\n",
       " 'december',\n",
       " 'defense',\n",
       " 'deficit',\n",
       " 'democrat',\n",
       " 'democratic',\n",
       " 'democrats',\n",
       " 'department',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'died',\n",
       " 'does',\n",
       " 'doesn',\n",
       " 'doing',\n",
       " 'dollar',\n",
       " 'dollars',\n",
       " 'don',\n",
       " 'donald',\n",
       " 'drug',\n",
       " 'economic',\n",
       " 'economy',\n",
       " 'education',\n",
       " 'elected',\n",
       " 'election',\n",
       " 'email',\n",
       " 'end',\n",
       " 'energy',\n",
       " 'entire',\n",
       " 'example',\n",
       " 'executive',\n",
       " 'facebook',\n",
       " 'fact',\n",
       " 'fake',\n",
       " 'families',\n",
       " 'family',\n",
       " 'far',\n",
       " 'fbi',\n",
       " 'february',\n",
       " 'federal',\n",
       " 'fired',\n",
       " 'flag',\n",
       " 'florida',\n",
       " 'food',\n",
       " 'force',\n",
       " 'foreign',\n",
       " 'foundation',\n",
       " 'fraud',\n",
       " 'free',\n",
       " 'fund',\n",
       " 'funding',\n",
       " 'gas',\n",
       " 'gave',\n",
       " 'general',\n",
       " 'george',\n",
       " 'georgia',\n",
       " 'getting',\n",
       " 'girl',\n",
       " 'given',\n",
       " 'giving',\n",
       " 'going',\n",
       " 'good',\n",
       " 'gop',\n",
       " 'got',\n",
       " 'gov',\n",
       " 'government',\n",
       " 'governor',\n",
       " 'great',\n",
       " 'group',\n",
       " 'growth',\n",
       " 'gun',\n",
       " 'guns',\n",
       " 'half',\n",
       " 'having',\n",
       " 'health',\n",
       " 'help',\n",
       " 'high',\n",
       " 'higher',\n",
       " 'highest',\n",
       " 'hillary',\n",
       " 'history',\n",
       " 'home',\n",
       " 'house',\n",
       " 'https',\n",
       " 'human',\n",
       " 'hundreds',\n",
       " 'hurricane',\n",
       " 'illegal',\n",
       " 'image',\n",
       " 'immigrants',\n",
       " 'immigration',\n",
       " 'including',\n",
       " 'income',\n",
       " 'increase',\n",
       " 'increased',\n",
       " 'information',\n",
       " 'insurance',\n",
       " 'international',\n",
       " 'internet',\n",
       " 'investigation',\n",
       " 'iran',\n",
       " 'iraq',\n",
       " 'isis',\n",
       " 'islamic',\n",
       " 'issued',\n",
       " 'january',\n",
       " 'job',\n",
       " 'jobs',\n",
       " 'joe',\n",
       " 'john',\n",
       " 'judge',\n",
       " 'july',\n",
       " 'june',\n",
       " 'just',\n",
       " 'justice',\n",
       " 'kids',\n",
       " 'kill',\n",
       " 'killed',\n",
       " 'know',\n",
       " 'known',\n",
       " 'large',\n",
       " 'largest',\n",
       " 'law',\n",
       " 'laws',\n",
       " 'leader',\n",
       " 'left',\n",
       " 'legal',\n",
       " 'legislation',\n",
       " 'let',\n",
       " 'life',\n",
       " 'like',\n",
       " 'list',\n",
       " 'little',\n",
       " 'live',\n",
       " 'lives',\n",
       " 'local',\n",
       " 'long',\n",
       " 'look',\n",
       " 'lost',\n",
       " 'lot',\n",
       " 'mail',\n",
       " 'major',\n",
       " 'majority',\n",
       " 'make',\n",
       " 'making',\n",
       " 'man',\n",
       " 'march',\n",
       " 'marijuana',\n",
       " 'mass',\n",
       " 'massive',\n",
       " 'matter',\n",
       " 'mccain',\n",
       " 'means',\n",
       " 'media',\n",
       " 'medicaid',\n",
       " 'medical',\n",
       " 'medicare',\n",
       " 'members',\n",
       " 'meme',\n",
       " 'men',\n",
       " 'mexico',\n",
       " 'michelle',\n",
       " 'middle',\n",
       " 'mike',\n",
       " 'military',\n",
       " 'million',\n",
       " 'millions',\n",
       " 'minimum',\n",
       " 'money',\n",
       " 'month',\n",
       " 'months',\n",
       " 'muslim',\n",
       " 'muslims',\n",
       " 'named',\n",
       " 'nation',\n",
       " 'national',\n",
       " 'nearly',\n",
       " 'need',\n",
       " 'new',\n",
       " 'news',\n",
       " 'non',\n",
       " 'north',\n",
       " 'november',\n",
       " 'nuclear',\n",
       " 'number',\n",
       " 'numbers',\n",
       " 'obama',\n",
       " 'obamacare',\n",
       " 'october',\n",
       " 'office',\n",
       " 'ohio',\n",
       " 'oil',\n",
       " 'old',\n",
       " 'open',\n",
       " 'order',\n",
       " 'paid',\n",
       " 'parents',\n",
       " 'party',\n",
       " 'passed',\n",
       " 'past',\n",
       " 'pay',\n",
       " 'paying',\n",
       " 'people',\n",
       " 'percent',\n",
       " 'person',\n",
       " 'phone',\n",
       " 'photo',\n",
       " 'photograph',\n",
       " 'pic',\n",
       " 'place',\n",
       " 'plan',\n",
       " 'planned',\n",
       " 'plans',\n",
       " 'police',\n",
       " 'policy',\n",
       " 'political',\n",
       " 'population',\n",
       " 'post',\n",
       " 'posted',\n",
       " 'power',\n",
       " 'president',\n",
       " 'presidential',\n",
       " 'prison',\n",
       " 'private',\n",
       " 'program',\n",
       " 'property',\n",
       " 'proposed',\n",
       " 'public',\n",
       " 'raise',\n",
       " 'raised',\n",
       " 'rape',\n",
       " 'rate',\n",
       " 'rates',\n",
       " 'real',\n",
       " 'really',\n",
       " 'received',\n",
       " 'recent',\n",
       " 'recently',\n",
       " 'record',\n",
       " 'reform',\n",
       " 'refugees',\n",
       " 'refused',\n",
       " 'released',\n",
       " 'rep',\n",
       " 'report',\n",
       " 'republican',\n",
       " 'republicans',\n",
       " 'residents',\n",
       " 'rick',\n",
       " 'right',\n",
       " 'rights',\n",
       " 'run',\n",
       " 'russia',\n",
       " 'russian',\n",
       " 'said',\n",
       " 'san',\n",
       " 'sanders',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'school',\n",
       " 'schools',\n",
       " 'scott',\n",
       " 'second',\n",
       " 'secretary',\n",
       " 'security',\n",
       " 'seen',\n",
       " 'sen',\n",
       " 'senate',\n",
       " 'senator',\n",
       " 'sent',\n",
       " 'september',\n",
       " 'service',\n",
       " 'services',\n",
       " 'set',\n",
       " 'seven',\n",
       " 'sex',\n",
       " 'sexual',\n",
       " 'shooting',\n",
       " 'shot',\n",
       " 'shows',\n",
       " 'sign',\n",
       " 'signed',\n",
       " 'single',\n",
       " 'small',\n",
       " 'social',\n",
       " 'south',\n",
       " 'special',\n",
       " 'speech',\n",
       " 'spending',\n",
       " 'spent',\n",
       " 'started',\n",
       " 'state',\n",
       " 'states',\n",
       " 'stop',\n",
       " 'story',\n",
       " 'student',\n",
       " 'students',\n",
       " 'study',\n",
       " 'support',\n",
       " 'supported',\n",
       " 'supreme',\n",
       " 'syria',\n",
       " 'taken',\n",
       " 'taking',\n",
       " 'tax',\n",
       " 'taxes',\n",
       " 'taxpayer',\n",
       " 'taxpayers',\n",
       " 'ted',\n",
       " 'terrorist',\n",
       " 'terrorists',\n",
       " 'texas',\n",
       " 'think',\n",
       " 'thousands',\n",
       " 'time',\n",
       " 'times',\n",
       " 'today',\n",
       " 'told',\n",
       " 'took',\n",
       " 'trade',\n",
       " 'trillion',\n",
       " 'true',\n",
       " 'trump',\n",
       " 'trying',\n",
       " 'twitter',\n",
       " 'unemployment',\n",
       " 'union',\n",
       " 'united',\n",
       " 'university',\n",
       " 'use',\n",
       " 'used',\n",
       " 'using',\n",
       " 've',\n",
       " 'veterans',\n",
       " 'video',\n",
       " 'violence',\n",
       " 'virginia',\n",
       " 'vote',\n",
       " 'voted',\n",
       " 'voter',\n",
       " 'voters',\n",
       " 'votes',\n",
       " 'voting',\n",
       " 'wage',\n",
       " 'walker',\n",
       " 'wall',\n",
       " 'want',\n",
       " 'wants',\n",
       " 'war',\n",
       " 'washington',\n",
       " 'water',\n",
       " 'way',\n",
       " 'weapons',\n",
       " 'week',\n",
       " 'went',\n",
       " 'white',\n",
       " 'wisconsin',\n",
       " 'woman',\n",
       " 'women',\n",
       " 'won',\n",
       " 'word',\n",
       " 'work',\n",
       " 'workers',\n",
       " 'working',\n",
       " 'world',\n",
       " 'year',\n",
       " 'years',\n",
       " 'york',\n",
       " 'young']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claim_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claim_array[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_vectorizer = TfidfVectorizer(max_features = 500)\n",
    "# tf_vec = tf_vectorizer.fit_transform(data['claim'].values.astype('U'))\n",
    "#print(tf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_claim_array = tf_vec.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '04/10/2016',\n",
       " 1: '06/11/2017',\n",
       " 2: '07/03/2019',\n",
       " 3: '07/07/2016',\n",
       " 4: '23/10/2016',\n",
       " 5: '25/02/2019',\n",
       " 6: '26/09/2018',\n",
       " 7: '30/01/2019',\n",
       " 8: 'Other'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label encoding\n",
    "data['date']=data['date'].astype('category')\n",
    "data['date_cat']=data['date'].cat.codes\n",
    "date_labels = dict(enumerate(data['date'].cat.categories))\n",
    "date_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.drop(['Unnamed: 0', 'claim', 'id', 'related_articles','claimant', 'date'], axis=1)\n",
    "claim_features = pd.DataFrame(data=claim_array, columns = vectorizer.get_feature_names())\n",
    "cleaned_features = pd.concat([features, claim_features], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### split the test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test and train data\n",
    "X = cleaned_features.drop(['label'], axis=1)\n",
    "y = cleaned_features['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaron/.virtualenvs/class/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/aaron/.virtualenvs/class/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model got an accuracy of 64.79% on the testing set\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "    \n",
    "TN = confusion_matrix(y_test, predictions)[0][0]\n",
    "FP = confusion_matrix(y_test, predictions)[0][1]\n",
    "FN = confusion_matrix(y_test, predictions)[1][0]\n",
    "TP = confusion_matrix(y_test, predictions)[1][1]\n",
    "total = TN + FP + FN + TP\n",
    "ACC = (TP + TN) / float(total)\n",
    "\n",
    "print (\"This model got an accuracy of {}% on the testing set\".format(round(ACC*100,2)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5746732376258838"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1574,  633,   21],\n",
       "       [ 821, 1101,   12],\n",
       "       [ 287,  211,    7]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "class",
   "language": "python",
   "name": "class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
